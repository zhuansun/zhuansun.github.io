<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张三的个人电脑</title>
  
  
  <link href="https://zhuansunpengcheng.gitee.io/atom.xml" rel="self"/>
  
  <link href="https://zhuansunpengcheng.gitee.io/"/>
  <updated>2022-11-04T03:52:28.486Z</updated>
  <id>https://zhuansunpengcheng.gitee.io/</id>
  
  <author>
    <name>zs</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>mysql的日志从入门到入土</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/mysql%E7%9A%84%E6%97%A5%E5%BF%97%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/mysql%E7%9A%84%E6%97%A5%E5%BF%97%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/</id>
    <published>2022-11-04T03:52:28.486Z</published>
    <updated>2022-11-04T03:52:28.486Z</updated>
    
    <content type="html"><![CDATA[<h1 id="mysql的日志从入门到入土"><a href="#mysql的日志从入门到入土" class="headerlink" title="mysql的日志从入门到入土"></a>mysql的日志从入门到入土</h1><hr><p>本文有xmind，配合xmind查看更加友好哦</p><blockquote><p>点击下载：<a href="mysql%E7%9A%84%E6%97%A5%E5%BF%97%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F.assets/mysql%E6%97%A5%E5%BF%97%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F.xmind">mysql日志从入门到入土.xmind</a></p></blockquote><h2 id="一条更新语句的执行流程是什么（引入redolog和binlog）"><a href="#一条更新语句的执行流程是什么（引入redolog和binlog）" class="headerlink" title="一条更新语句的执行流程是什么（引入redolog和binlog）"></a>一条更新语句的执行流程是什么（引入redolog和binlog）</h2><blockquote><p>update T set c&#x3D;c+1 where ID&#x3D;2;</p></blockquote><p>mysql的更新流程和SQL语句的基本执行链路是一样的：连接器-&gt;分析器-&gt;优化器-&gt;执行器-&gt;存储引擎</p><img src="mysql的日志从入门到入土.assets/image-20221012203451881.png" alt="image-20221012203451881" style="zoom: 80%;" /><p>通过连接器，先连接数据库。</p><p>清空查询缓存：在一个表上有更新的时候，跟这个表有关的查询缓存会失效。这也就是我们一般不建议使用查询缓存的原因（在mysql8.0中，已经把查询缓存整个模块都删掉了）。</p><p>分析器会通过词法和语法解析知道这是一条更新语句。</p><p>优化器决定要使用哪个索引。</p><p>执行器负责具体执行，找到这一行，然后更新。</p><h2 id="重要的日志模块（redolog和binlog）"><a href="#重要的日志模块（redolog和binlog）" class="headerlink" title="重要的日志模块（redolog和binlog）"></a>重要的日志模块（redolog和binlog）</h2><p>与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和 binlog（归档日志）</p><h3 id="什么是redolog"><a href="#什么是redolog" class="headerlink" title="什么是redolog"></a>什么是redolog</h3><p>作者举了《孔乙己》中一个赊账的例子，孔乙己来到酒店喝酒，老板会先把孔乙己的酒钱记录在一个粉板上，然后等不忙的时候，在累加到账本上。</p><p>这里的粉板就是redolog，账本就是磁盘；</p><p>考虑这个场景，当很多很多的人来喝酒的时候，老板一般有两种方式记账（当数据库更新一条记录的时候，一般有两种方式）：</p><ul><li>直接掏出账本，在账本上加加减减。（直接操作磁盘的数据，进行更新）</li><li>另一种做法是先在粉板上记下这次的账，等打烊以后再把账本翻出来核算（先记录到redolog，等mysql空闲的时候，刷到磁盘）。</li></ul><p>在酒店生意红火的时候，老板一定选择后者，因为前者操作实在是太麻烦了。</p><ul><li>首先，你得找到这个人的赊账总额那条记录。你想想，密密麻麻几十页，掌柜要找到那个名字，可能还得带上老花镜慢慢找。（磁盘的随机IO读）</li><li>找到之后再拿出算盘计算，最后再将结果写回到账本上（磁盘的随机写）</li></ul><p>粉板（redolog）就完美的解决了这两个问题：</p><ul><li>首先说找记录：mysql的所有记录都是从数据页中查的，如果要更新的数据所在的数据页在内存中，可以直接找到，如果不在内存中，会先从磁盘把这个数据库加载到内存中。（这个步骤没法省，redolog优化的地方并不在于这里，这里会有<strong>changebuffer</strong>优化（后面说））</li><li>找到记录之后，然后说更新记录：更新的结果是写到redolog中，而不是写到磁盘中，就避免了磁盘的随机IO，虽然redolog也是写到磁盘中的，但是由于组提交的存在，一次磁盘的写入是大量的顺序IO；（redolog是顺序写，并且可以组提交，还有别的一些优化，收益最大是是这两个因素；）</li></ul><h3 id="redo-log有什么用"><a href="#redo-log有什么用" class="headerlink" title="redo log有什么用"></a>redo log有什么用</h3><p>有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 <strong>crash-safe</strong>。</p><h3 id="为什么要引入redolog"><a href="#为什么要引入redolog" class="headerlink" title="为什么要引入redolog"></a>为什么要引入redolog</h3><p>因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，<strong>binlog</strong> 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。</p><p>mysql要保证数据的持久性，保证持久性就需要将数据写到磁盘，但是写到磁盘的话，涉及到</p><ul><li>刷页，因为mysql所有的操作是针对数据页操作的，而一个简单的更新，可能就要刷整整一个数据页</li><li>随机IO，一个事务所更新的涉及到的数据页可能不止一个，而且数据页可能不相连，就涉及到随机IO</li></ul><p>这俩问题，咋办呢？就引入了redolog，一个更新不刷页（先记录到redolog日志中），这样就避免了上面两个问题</p><ul><li>redolog将随机IO改成了顺序IO，而且避免了每次更新都刷盘（刷盘也是要刷的，但是是组提交）</li></ul><h3 id="redolog的结构"><a href="#redolog的结构" class="headerlink" title="redolog的结构"></a>redolog的结构</h3><p>redolog是循环写的文件，InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。</p><img src="mysql的日志从入门到入土.assets/image-20221027161923607.png" alt="image-20221027161923607" style="zoom: 50%;" /><p>write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。</p><p>write pos 和 checkpoint 之间的是还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。</p><h3 id="redolog一般设置多大"><a href="#redolog一般设置多大" class="headerlink" title="redolog一般设置多大"></a>redolog一般设置多大</h3><p>redo log 太小的话，会导致很快就被写满，然后不得不强行刷 redo log，这样 WAL 机制的能力就发挥不出来了。所以，如果是现在常见的几个 TB 的磁盘的话，就不要太小气了，直接将 redo log 设置为 4 个文件、每个文件 1GB 吧。</p><ul><li>innodb_log_file_size：该参数指定了每个redo日志文件的大小，在MySQL 5.7.21这个版本中的默认值为48MB，</li><li>innodb_log_files_in_group：该参数指定redo日志文件的个数，默认值为2，最大值为100。</li></ul><h3 id="binlog是什么"><a href="#binlog是什么" class="headerlink" title="binlog是什么"></a>binlog是什么</h3><p>我们知道mysql是由两部分组成，server层和引擎层，上面介绍的redolog就是innodb引擎独有的日志，而MySQL的server层也有自己的日志，叫做binlog；</p><p>为什么要有两个日志呢</p><p>因为mysql在5.5版本之前，默认的存储引擎是MyISAM，但是MyISAM并没有<strong>Crash-Safe</strong>的能力，而server层自带的binlog又只有归档的能力，也不具备<strong>Crash-Safe</strong>的能力，所以才会有后来的innodb以插件的形式引入mysql中，作为mysql的引擎，并使用了redo log，实现了Crash-Safe的能力</p><p>除了以上的原因，binlog和redolog还有其他的区别，解释了为什么存在两个日志</p><ul><li>这两个日志的使用方不一样：redolog是innodb引擎所特有的，只有innodb才能用；而binlog是mysql的server层有的，所有的引擎都可以使用；</li><li>这两个日志记录的内容不一样：redolog是物理日志，记录的是在某个数据页上做了什么修改；而binlog是逻辑日志，简单的说就是sql语句。</li><li>这两个日志的记录方式不一样：redolog是循环写，redolog文件写满了，会从头重新写；binlog是追加写；binlog文件写满了，会切割，在新文件中继续写</li></ul><h3 id="binlog有什么用"><a href="#binlog有什么用" class="headerlink" title="binlog有什么用"></a>binlog有什么用</h3><p>主要是归档（归档之后可以用于数据恢复）和主从同步</p><h3 id="binlog的结构"><a href="#binlog的结构" class="headerlink" title="binlog的结构"></a>binlog的结构</h3><h4 id="binlog的存储目录"><a href="#binlog的存储目录" class="headerlink" title="binlog的存储目录"></a>binlog的存储目录</h4><p>在磁盘的上的结构，binlog默认是存放在<strong>MySQL服务器的数据目录</strong>下，（可以修改binlog的存放路径和binlog的文件名），如果你不知道数据目录是哪个，可以通过这个命令查看</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">mysql<span class="token operator">></span> <span class="token keyword">show</span> variables <span class="token operator">like</span> <span class="token string">'%datadir%'</span><span class="token punctuation">;</span><span class="token operator">+</span><span class="token comment">---------------+---------------------------------------------+</span><span class="token operator">|</span> Variable_name <span class="token operator">|</span> <span class="token keyword">Value</span>                                       <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">---------------+---------------------------------------------+</span><span class="token operator">|</span> datadir       <span class="token operator">|</span> C:\ProgramData\MySQL\MySQL Server <span class="token number">8.0</span>\<span class="token keyword">Data</span>\ <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">---------------+---------------------------------------------+</span><span class="token number">1</span> <span class="token keyword">row</span> <span class="token operator">in</span> <span class="token keyword">set</span><span class="token punctuation">,</span> <span class="token number">1</span> warning <span class="token punctuation">(</span><span class="token number">0.00</span> sec<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在数据目录下，你就可以看到binlog的文件，就像是这样，binlog是二进制文件，就像它的全名一样：binary log，所以是不能直接打开的：</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token number">000001</span>xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token number">000002</span>xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token number">000003</span>xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token number">000004</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>除了真正存储binlog日志的文件外，MySQL服务器还会在相同的路径下生成一个关于binlog的索引文件，它的名称就是：</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token keyword">index</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个索引文件是一个文本文件，我们可以直接打开：</p><pre class="line-numbers language-mysql" data-language="mysql"><code class="language-mysql">shell&gt; cat xxx-bin.index.&#x2F;xxx-bin.000001.&#x2F;xxx-bin.000001.&#x2F;xxx-bin.000001.&#x2F;xxx-bin.000001<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到，这个索引文件只是简单的将各个binlog文件的路径存储了起来而已。</p><h4 id="怎么查看binlog的格式"><a href="#怎么查看binlog的格式" class="headerlink" title="怎么查看binlog的格式"></a>怎么查看binlog的格式</h4><p>下面的三种查看方式，前两个是一样的，都表示查看当前session的binlog格式；最后一个表示查看全局的binlog格式</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">mysql<span class="token operator">></span> <span class="token keyword">show</span> variables <span class="token operator">like</span> <span class="token string">'%binlog_format%'</span><span class="token punctuation">;</span><span class="token operator">+</span><span class="token comment">---------------+-------+</span><span class="token operator">|</span> Variable_name <span class="token operator">|</span> <span class="token keyword">Value</span> <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">---------------+-------+</span><span class="token operator">|</span> binlog_format <span class="token operator">|</span> <span class="token keyword">ROW</span>   <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">---------------+-------+</span><span class="token number">1</span> <span class="token keyword">row</span> <span class="token operator">in</span> <span class="token keyword">set</span><span class="token punctuation">,</span> <span class="token number">1</span> warning <span class="token punctuation">(</span><span class="token number">0.02</span> sec<span class="token punctuation">)</span>mysql<span class="token operator">></span> <span class="token keyword">show</span> <span class="token keyword">session</span> variables <span class="token operator">like</span> <span class="token string">'%binlog_format%'</span><span class="token punctuation">;</span><span class="token operator">+</span><span class="token comment">---------------+-------+</span><span class="token operator">|</span> Variable_name <span class="token operator">|</span> <span class="token keyword">Value</span> <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">---------------+-------+</span><span class="token operator">|</span> binlog_format <span class="token operator">|</span> <span class="token keyword">ROW</span>   <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">---------------+-------+</span><span class="token number">1</span> <span class="token keyword">row</span> <span class="token operator">in</span> <span class="token keyword">set</span><span class="token punctuation">,</span> <span class="token number">1</span> warning <span class="token punctuation">(</span><span class="token number">0.02</span> sec<span class="token punctuation">)</span>mysql<span class="token operator">></span> <span class="token keyword">show</span> <span class="token keyword">global</span> variables <span class="token operator">like</span> <span class="token string">'%binlog_format%'</span><span class="token punctuation">;</span><span class="token operator">+</span><span class="token comment">---------------+-------+</span><span class="token operator">|</span> Variable_name <span class="token operator">|</span> <span class="token keyword">Value</span> <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">---------------+-------+</span><span class="token operator">|</span> binlog_format <span class="token operator">|</span> <span class="token keyword">ROW</span>   <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">---------------+-------+</span><span class="token number">1</span> <span class="token keyword">row</span> <span class="token operator">in</span> <span class="token keyword">set</span><span class="token punctuation">,</span> <span class="token number">1</span> warning <span class="token punctuation">(</span><span class="token number">0.02</span> sec<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="怎么设置binlog的格式"><a href="#怎么设置binlog的格式" class="headerlink" title="怎么设置binlog的格式"></a>怎么设置binlog的格式</h4><p>下面展示三种设置binlog的方式，前两个是一样的，设置当前session的binlog格式，重启后就失效了。最后一个表示设置全局的binlog格式，需要重启后才生效。</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">mysql<span class="token operator">></span> <span class="token keyword">SET</span> binlog_format <span class="token operator">=</span> <span class="token string">'statement'</span><span class="token punctuation">;</span>Query OK<span class="token punctuation">,</span> <span class="token number">0</span> <span class="token keyword">rows</span> affected <span class="token punctuation">(</span><span class="token number">0.00</span> sec<span class="token punctuation">)</span>mysql<span class="token operator">></span> <span class="token keyword">SET</span> <span class="token keyword">session</span> binlog_format <span class="token operator">=</span> <span class="token string">'statement'</span><span class="token punctuation">;</span>Query OK<span class="token punctuation">,</span> <span class="token number">0</span> <span class="token keyword">rows</span> affected <span class="token punctuation">(</span><span class="token number">0.00</span> sec<span class="token punctuation">)</span>mysql<span class="token operator">></span> <span class="token keyword">SET</span> <span class="token keyword">global</span> binlog_format <span class="token operator">=</span> <span class="token string">'statement'</span><span class="token punctuation">;</span>Query OK<span class="token punctuation">,</span> <span class="token number">0</span> <span class="token keyword">rows</span> affected <span class="token punctuation">(</span><span class="token number">0.00</span> sec<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="binlog的三种格式"><a href="#binlog的三种格式" class="headerlink" title="binlog的三种格式"></a>binlog的三种格式</h4><p>binlog 有两种格式，一种是 statement，一种是 row。可能你在其他资料上还会看到有第三种格式，叫作 mixed，其实它就是前两种格式的混合。</p><p>下面看一下三种格式分别记录了什么？</p><p>准备以下数据：</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> <span class="token identifier"><span class="token punctuation">`</span>t<span class="token punctuation">`</span></span> <span class="token punctuation">(</span>  <span class="token identifier"><span class="token punctuation">`</span>id<span class="token punctuation">`</span></span> <span class="token keyword">int</span><span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">)</span> <span class="token operator">NOT</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span>  <span class="token identifier"><span class="token punctuation">`</span>a<span class="token punctuation">`</span></span> <span class="token keyword">int</span><span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span>  <span class="token identifier"><span class="token punctuation">`</span>t_modified<span class="token punctuation">`</span></span> <span class="token keyword">timestamp</span> <span class="token operator">NOT</span> <span class="token boolean">NULL</span> <span class="token keyword">DEFAULT</span> <span class="token keyword">CURRENT_TIMESTAMP</span><span class="token punctuation">,</span>  <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span> <span class="token punctuation">(</span><span class="token identifier"><span class="token punctuation">`</span>id<span class="token punctuation">`</span></span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token keyword">KEY</span> <span class="token identifier"><span class="token punctuation">`</span>a<span class="token punctuation">`</span></span> <span class="token punctuation">(</span><span class="token identifier"><span class="token punctuation">`</span>a<span class="token punctuation">`</span></span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token keyword">KEY</span> <span class="token identifier"><span class="token punctuation">`</span>t_modified<span class="token punctuation">`</span></span><span class="token punctuation">(</span><span class="token identifier"><span class="token punctuation">`</span>t_modified<span class="token punctuation">`</span></span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">ENGINE</span><span class="token operator">=</span><span class="token keyword">InnoDB</span><span class="token punctuation">;</span><span class="token keyword">insert</span> <span class="token keyword">into</span> t <span class="token keyword">values</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token string">'2018-11-13'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">insert</span> <span class="token keyword">into</span> t <span class="token keyword">values</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token string">'2018-11-12'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">insert</span> <span class="token keyword">into</span> t <span class="token keyword">values</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token string">'2018-11-11'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">insert</span> <span class="token keyword">into</span> t <span class="token keyword">values</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token string">'2018-11-10'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">insert</span> <span class="token keyword">into</span> t <span class="token keyword">values</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token string">'2018-11-09'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h6 id="statement"><a href="#statement" class="headerlink" title="statement"></a><strong>statement</strong></h6><p>按照上面的方式，查看当前binlog的格式，并将当前会话的binlog的格式设置为：statement</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">mysql<span class="token operator">></span> <span class="token keyword">show</span> variables <span class="token operator">like</span> <span class="token string">'%binlog_format%'</span><span class="token punctuation">;</span><span class="token operator">+</span><span class="token comment">---------------+-----------+</span><span class="token operator">|</span> Variable_name <span class="token operator">|</span> <span class="token keyword">Value</span>     <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">---------------+-----------+</span><span class="token operator">|</span> binlog_format <span class="token operator">|</span> STATEMENT <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">---------------+-----------+</span><span class="token number">1</span> <span class="token keyword">row</span> <span class="token operator">in</span> <span class="token keyword">set</span><span class="token punctuation">,</span> <span class="token number">1</span> warning <span class="token punctuation">(</span><span class="token number">0.00</span> sec<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>执行以下语句</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">mysql<span class="token operator">></span> <span class="token keyword">delete</span> <span class="token keyword">from</span> t <span class="token keyword">where</span> a<span class="token operator">>=</span><span class="token number">4</span> <span class="token operator">and</span> t_modified<span class="token operator">&lt;=</span><span class="token string">'2018-11-10'</span> <span class="token keyword">limit</span> <span class="token number">1</span><span class="token punctuation">;</span>Query OK<span class="token punctuation">,</span> <span class="token number">1</span> <span class="token keyword">row</span> affected <span class="token punctuation">(</span><span class="token number">0.01</span> sec<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>在查看binlog的内容之前，首先查看当前binlog写在了哪个文件上，因为binlog有很多个</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">mysql<span class="token operator">></span> <span class="token keyword">show</span> master <span class="token keyword">status</span><span class="token punctuation">;</span><span class="token operator">+</span><span class="token comment">----------------+----------+--------------+------------------+-------------------+</span><span class="token operator">|</span> <span class="token keyword">File</span>           <span class="token operator">|</span> Position <span class="token operator">|</span> Binlog_Do_DB <span class="token operator">|</span> Binlog_Ignore_DB <span class="token operator">|</span> Executed_Gtid_Set <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">----------------+----------+--------------+------------------+-------------------+</span><span class="token operator">|</span> xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token number">000002</span> <span class="token operator">|</span>     <span class="token number">7829</span> <span class="token operator">|</span>              <span class="token operator">|</span>                  <span class="token operator">|</span>                   <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">----------------+----------+--------------+------------------+-------------------+</span><span class="token number">1</span> <span class="token keyword">row</span> <span class="token operator">in</span> <span class="token keyword">set</span> <span class="token punctuation">(</span><span class="token number">0.00</span> sec<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后查看binlog的内容（binlog很大，这里只截取了一部分）</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">mysql<span class="token operator">></span> <span class="token keyword">show</span> binlog events <span class="token operator">in</span> <span class="token string">'xxx-bin.000002'</span><span class="token punctuation">;</span><span class="token operator">+</span><span class="token comment">----------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------+</span><span class="token operator">|</span> Log_name       <span class="token operator">|</span> Pos  <span class="token operator">|</span> Event_type     <span class="token operator">|</span> Server_id <span class="token operator">|</span> End_log_pos <span class="token operator">|</span> Info                                                                      <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">----------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------+</span><span class="token operator">|</span> xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token number">000002</span> <span class="token operator">|</span> <span class="token number">7489</span> <span class="token operator">|</span> Anonymous_Gtid <span class="token operator">|</span>         <span class="token number">1</span> <span class="token operator">|</span>        <span class="token number">7568</span> <span class="token operator">|</span> <span class="token keyword">SET</span> @<span class="token variable">@SESSION.GTID_NEXT</span><span class="token operator">=</span> <span class="token string">'ANONYMOUS'</span>                                      <span class="token operator">|</span><span class="token operator">|</span> xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token number">000002</span> <span class="token operator">|</span> <span class="token number">7568</span> <span class="token operator">|</span> Query          <span class="token operator">|</span>         <span class="token number">1</span> <span class="token operator">|</span>        <span class="token number">7654</span> <span class="token operator">|</span> <span class="token keyword">BEGIN</span>                                                                     <span class="token operator">|</span><span class="token operator">|</span> xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token number">000002</span> <span class="token operator">|</span> <span class="token number">7654</span> <span class="token operator">|</span> Query          <span class="token operator">|</span>         <span class="token number">1</span> <span class="token operator">|</span>        <span class="token number">7798</span> <span class="token operator">|</span> <span class="token keyword">use</span> <span class="token identifier"><span class="token punctuation">`</span>zs<span class="token punctuation">`</span></span><span class="token punctuation">;</span> <span class="token keyword">delete</span> <span class="token keyword">from</span> t   <span class="token keyword">where</span> a<span class="token operator">>=</span><span class="token number">4</span> <span class="token operator">and</span> t_modified<span class="token operator">&lt;=</span><span class="token string">'2018-11-10'</span> <span class="token keyword">limit</span> <span class="token number">1</span> <span class="token operator">|</span><span class="token operator">|</span> xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token number">000002</span> <span class="token operator">|</span> <span class="token number">7798</span> <span class="token operator">|</span> Xid            <span class="token operator">|</span>         <span class="token number">1</span> <span class="token operator">|</span>        <span class="token number">7829</span> <span class="token operator">|</span> <span class="token keyword">COMMIT</span> <span class="token comment">/* xid=1840 */</span>                                                     <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">----------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------+</span><span class="token number">56</span> <span class="token keyword">rows</span> <span class="token operator">in</span> <span class="token keyword">set</span> <span class="token punctuation">(</span><span class="token number">0.00</span> sec<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>内容解释</p><table><thead><tr><th>行</th><th>含义</th></tr></thead><tbody><tr><td>第一行：SET @@SESSION.GTID_NEXT&#x3D; ‘ANONYMOUS’</td><td>主备切换用的</td></tr><tr><td>第二行：BEGIN</td><td>跟第四行的 commit 对应，表示中间是一个事务</td></tr><tr><td>第三行</td><td>是真实执行的语句了。可以看到，在真实执行的 delete 命令之前，还有一个“use ‘zs’”命令。这条命令是 MySQL 根据当前要操作的表所在的数据库，自行添加的。这样做可以保证日志传到备库去执行的时候，不论当前的工作线程在哪个库里，都能够正确地更新到 test 库的表 t。use ‘zs’命令之后的 delete 语句，就是我们输入的 SQL 原文了。</td></tr><tr><td>第四行：COMMIT &#x2F;* xid&#x3D;1840 *&#x2F;</td><td>你可以看到里面写着 xid&#x3D;1840,xid是崩溃恢复的时候，和redolog关联，用来校验binlog完整性的</td></tr></tbody></table><h6 id="row"><a href="#row" class="headerlink" title="row"></a><strong>row</strong></h6><p>先将测试数据复原，重新导入</p><p>按照上面的方式，查看当前binlog的格式，并将当前会话的binlog的格式设置为：row</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">mysql<span class="token operator">></span> <span class="token keyword">show</span> variables <span class="token operator">like</span> <span class="token string">'%binlog_format%'</span><span class="token punctuation">;</span><span class="token operator">+</span><span class="token comment">---------------+-------+</span><span class="token operator">|</span> Variable_name <span class="token operator">|</span> <span class="token keyword">Value</span> <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">---------------+-------+</span><span class="token operator">|</span> binlog_format <span class="token operator">|</span> <span class="token keyword">ROW</span>   <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">---------------+-------+</span><span class="token number">1</span> <span class="token keyword">row</span> <span class="token operator">in</span> <span class="token keyword">set</span><span class="token punctuation">,</span> <span class="token number">1</span> warning <span class="token punctuation">(</span><span class="token number">0.00</span> sec<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>执行以下语句</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">mysql<span class="token operator">></span> <span class="token keyword">delete</span> <span class="token keyword">from</span> t <span class="token keyword">where</span> a<span class="token operator">>=</span><span class="token number">4</span> <span class="token operator">and</span> t_modified<span class="token operator">&lt;=</span><span class="token string">'2018-11-10'</span> <span class="token keyword">limit</span> <span class="token number">1</span><span class="token punctuation">;</span>Query OK<span class="token punctuation">,</span> <span class="token number">1</span> <span class="token keyword">row</span> affected <span class="token punctuation">(</span><span class="token number">0.01</span> sec<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>在查看binlog的内容之前，首先查看当前binlog写在了哪个文件上，因为binlog有很多个</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">mysql<span class="token operator">></span> <span class="token keyword">show</span> master <span class="token keyword">status</span><span class="token punctuation">;</span><span class="token operator">+</span><span class="token comment">----------------+----------+--------------+------------------+-------------------+</span><span class="token operator">|</span> <span class="token keyword">File</span>           <span class="token operator">|</span> Position <span class="token operator">|</span> Binlog_Do_DB <span class="token operator">|</span> Binlog_Ignore_DB <span class="token operator">|</span> Executed_Gtid_Set <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">----------------+----------+--------------+------------------+-------------------+</span><span class="token operator">|</span> xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token number">000002</span> <span class="token operator">|</span>     <span class="token number">7829</span> <span class="token operator">|</span>              <span class="token operator">|</span>                  <span class="token operator">|</span>                   <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">----------------+----------+--------------+------------------+-------------------+</span><span class="token number">1</span> <span class="token keyword">row</span> <span class="token operator">in</span> <span class="token keyword">set</span> <span class="token punctuation">(</span><span class="token number">0.00</span> sec<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后查看binlog的内容（binlog很大，这里只截取了一部分）</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">mysql<span class="token operator">></span> <span class="token keyword">show</span> binlog events <span class="token operator">in</span> <span class="token string">'xxx-bin.000002'</span><span class="token punctuation">;</span><span class="token operator">+</span><span class="token comment">----------------+------+----------------+-----------+-------------+--------------------------------------+</span><span class="token operator">|</span> Log_name       <span class="token operator">|</span> Pos  <span class="token operator">|</span> Event_type     <span class="token operator">|</span> Server_id <span class="token operator">|</span> End_log_pos <span class="token operator">|</span> Info                                 <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">----------------+------+----------------+-----------+-------------+--------------------------------------+</span><span class="token operator">|</span> xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token number">000002</span> <span class="token operator">|</span> <span class="token number">5043</span> <span class="token operator">|</span> Anonymous_Gtid <span class="token operator">|</span>         <span class="token number">1</span> <span class="token operator">|</span>        <span class="token number">5122</span> <span class="token operator">|</span> <span class="token keyword">SET</span> @<span class="token variable">@SESSION.GTID_NEXT</span><span class="token operator">=</span> <span class="token string">'ANONYMOUS'</span> <span class="token operator">|</span><span class="token operator">|</span> xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token number">000002</span> <span class="token operator">|</span> <span class="token number">5122</span> <span class="token operator">|</span> Query          <span class="token operator">|</span>         <span class="token number">1</span> <span class="token operator">|</span>        <span class="token number">5203</span> <span class="token operator">|</span> <span class="token keyword">BEGIN</span>                                <span class="token operator">|</span><span class="token operator">|</span> xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token number">000002</span> <span class="token operator">|</span> <span class="token number">5203</span> <span class="token operator">|</span> Table_map      <span class="token operator">|</span>         <span class="token number">1</span> <span class="token operator">|</span>        <span class="token number">5251</span> <span class="token operator">|</span> table_id: <span class="token number">169</span> <span class="token punctuation">(</span>zs<span class="token punctuation">.</span>t<span class="token punctuation">)</span>                 <span class="token operator">|</span><span class="token operator">|</span> xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token number">000002</span> <span class="token operator">|</span> <span class="token number">5251</span> <span class="token operator">|</span> Delete_rows    <span class="token operator">|</span>         <span class="token number">1</span> <span class="token operator">|</span>        <span class="token number">5299</span> <span class="token operator">|</span> table_id: <span class="token number">169</span> flags: STMT_END_F      <span class="token operator">|</span><span class="token operator">|</span> xxx<span class="token operator">-</span>bin<span class="token punctuation">.</span><span class="token number">000002</span> <span class="token operator">|</span> <span class="token number">5299</span> <span class="token operator">|</span> Xid            <span class="token operator">|</span>         <span class="token number">1</span> <span class="token operator">|</span>        <span class="token number">5330</span> <span class="token operator">|</span> <span class="token keyword">COMMIT</span> <span class="token comment">/* xid=1924 */</span>                <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">----------------+------+----------------+-----------+-------------+--------------------------------------+</span><span class="token number">74</span> <span class="token keyword">rows</span> <span class="token operator">in</span> <span class="token keyword">set</span> <span class="token punctuation">(</span><span class="token number">0.00</span> sec<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>内容解释</p><table><thead><tr><th>行</th><th>含义</th></tr></thead><tbody><tr><td>第一行：SET @@SESSION.GTID_NEXT&#x3D; ‘ANONYMOUS’</td><td>主备切换用的</td></tr><tr><td>第二行：BEGIN</td><td>跟第五行的 commit 对应，表示中间是一个事务</td></tr><tr><td>第三行：</td><td>在statement格式中，记录的是sql原文，在row格式下，记录的是两个event：Table_map和Delete_rows这两个动作</td></tr><tr><td>第四行：</td><td>Table_map表示要操作哪个数据库的那张表； Delete_rows表示删除一行，具体的内容，这里看不到，需要借助mysqlbinlog工具来看</td></tr><tr><td>第五行：COMMIT &#x2F;* xid&#x3D;1924 *&#x2F;</td><td>你可以看到里面写着 xid&#x3D;1924 ,xid是崩溃恢复的时候，和redolog关联，用来校验binlog完整性的</td></tr></tbody></table><p>在第四行中，我们看不到具体的内容，所以需要通过mysqlbinlog工具来看</p><p>mysqlbinlog是啥，就是一个可以执行的工具，在windows系统下，这个工具在mysql的安装目录下，叫：mysqlbinlog.exe</p><p>因为我现在用的windows，所以就用windows来展示了</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">C:<span class="token punctuation">\</span>Users<span class="token operator">></span>cd C:<span class="token punctuation">\</span>Program Files<span class="token punctuation">\</span>MySQL<span class="token punctuation">\</span>MySQL Server <span class="token number">8.0</span><span class="token punctuation">\</span>binC:<span class="token punctuation">\</span>Program Files<span class="token punctuation">\</span>MySQL<span class="token punctuation">\</span>MySQL Server <span class="token number">8.0</span><span class="token punctuation">\</span>bin<span class="token operator">></span>dir 驱动器 C 中的卷是 系统 卷的序列号是 0003-57E7 C:<span class="token punctuation">\</span>Program Files<span class="token punctuation">\</span>MySQL<span class="token punctuation">\</span>MySQL Server <span class="token number">8.0</span><span class="token punctuation">\</span>bin 的目录<span class="token number">2022</span>/11/01  <span class="token number">14</span>:10    <span class="token operator">&lt;</span>DIR<span class="token operator">></span>          <span class="token builtin class-name">.</span><span class="token number">2022</span>/11/01  <span class="token number">14</span>:10    <span class="token operator">&lt;</span>DIR<span class="token operator">></span>          <span class="token punctuation">..</span><span class="token number">2021</span>/07/01  02:12         xxxxxxxxx xxxxxxxx（因为文件太多了，所以我这里就是省略了）<span class="token number">2021</span>/07/01  02:12         <span class="token number">6,960</span>,408 mysql.exe<span class="token number">2021</span>/07/01  02:12         <span class="token number">6,854</span>,952 mysqladmin.exe<span class="token number">2021</span>/07/01  02:12         <span class="token number">7,168</span>,808 mysqlbinlog.exe              <span class="token number">48</span> 个文件    <span class="token number">270,259</span>,595 字节               <span class="token number">2</span> 个目录 <span class="token number">24,862</span>,003,200 可用字节<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到在bin目录下，有一个工具叫做：mysqlbinlog.exe</p><p>然后我们在<code>C:\Program Files\MySQL\MySQL Server 8.0\bin</code>这个目录下运行下面的命令</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">mysqlbinlog.exe  <span class="token parameter variable">-vv</span> <span class="token string">"C:\ProgramData\MySQL\MySQL Server 8.0\Data\SF0001408876LA-bin.000002"</span> --start-position<span class="token operator">=</span><span class="token number">5043</span> --stop-position<span class="token operator">=</span><span class="token number">5300</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><code>-vv</code>表示verbose，啰嗦模式，为了把内容都解析出来，所以从结果里面可以看到各个字段的值（比如，@1&#x3D;4、 @2&#x3D;4 这些值）。</li><li><code>--start-position</code>表示binlog的开始位置，值哪里来的，来自于<code>show binlog events in &#39;xxx-bin.000002&#39;</code>结果的pos字段</li><li><code>--stop-position</code>表示binlog的结束位置，值哪里来的，来自于<code>show binlog events in &#39;xxx-bin.000002&#39;</code>结果的pos字段，写大一点，要不然不包括进来（比如我的pos&#x3D;5299，但是我这里写的是5300）</li></ul><p>运行结果如下（结果还挺长的，删掉了一些，只列出比较重要的几个内容）</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">C:<span class="token punctuation">\</span>Program Files<span class="token punctuation">\</span>MySQL<span class="token punctuation">\</span>MySQL Server <span class="token number">8.0</span><span class="token punctuation">\</span>bin<span class="token operator">></span>mysqlbinlog.exe  <span class="token parameter variable">-vv</span> <span class="token string">"C:\ProgramData\MySQL\MySQL Server 8.0\Data\SF0001408876LA-bin.000002"</span> --start-position<span class="token operator">=</span><span class="token number">5043</span> --stop-position<span class="token operator">=</span><span class="token number">5300</span><span class="token comment"># at 5043</span><span class="token comment">#221101 15:21:11 server id 1  end_log_pos 5122 CRC32 0x72f668e7         Anonymous_GTID  last_committed=17       sequence_number=18      rbr_only=yes    original_committed_timestamp=1667287271257812   immediate_commit_timestamp=1667287271257812  transaction_length=287</span>SET @@<span class="token environment constant">SESSION</span>.GTID_NEXT<span class="token operator">=</span> <span class="token string">'ANONYMOUS'</span>/*<span class="token operator">!</span>*/<span class="token punctuation">;</span><span class="token comment"># at 5122</span><span class="token comment">#221101 15:21:11 server id 1  end_log_pos 5203 CRC32 0xe3288066         Query   thread_id=11    exec_time=0     error_code=0</span>SET <span class="token assign-left variable">TIMESTAMP</span><span class="token operator">=</span><span class="token number">1667287271</span>/*<span class="token operator">!</span>*/<span class="token punctuation">;</span>SET @@session.pseudo_thread_id<span class="token operator">=</span><span class="token number">11</span>/*<span class="token operator">!</span>*/<span class="token punctuation">;</span>SET @@session.sql_mode<span class="token operator">=</span><span class="token number">1075838976</span>/*<span class="token operator">!</span>*/<span class="token punctuation">;</span>BEGIN/*<span class="token operator">!</span>*/<span class="token punctuation">;</span><span class="token comment"># at 5203</span><span class="token comment">#221101 15:21:11 server id 1  end_log_pos 5251 CRC32 0x613f3131         Table_map: `zs`.`t` mapped to number 169</span><span class="token comment"># at 5251</span><span class="token comment">#221101 15:21:11 server id 1  end_log_pos 5299 CRC32 0x75141201         Delete_rows: table id 169 flags: STMT_END_F</span>BINLOG <span class="token string">'58hgYxMBAAAAMAAAAIMUAAAAAKkAAAAAAAEAAnpzAAF0AAMDAxEBAAIBAQAxMT9h58hgYyABAAAAMAAAALMUAAAAAKkAAAAAAAEAAgAD/wAEAAAABAAAAFvlrwABEhR1'</span>/*<span class="token operator">!</span>*/<span class="token punctuation">;</span><span class="token comment">### DELETE FROM `zs`.`t`</span><span class="token comment">### WHERE</span><span class="token comment">###   @1=4 /* INT meta=0 nullable=0 is_null=0 */</span><span class="token comment">###   @2=4 /* INT meta=0 nullable=1 is_null=0 */</span><span class="token comment">###   @3=1541779200 /* TIMESTAMP(0) meta=0 nullable=0 is_null=0 */</span><span class="token comment"># at 5299</span><span class="token comment">#221101 15:21:11 server id 1  end_log_pos 5330 CRC32 0xc30d2901         Xid = 1924</span>COMMIT/*<span class="token operator">!</span>*/<span class="token punctuation">;</span>SET @@<span class="token environment constant">SESSION</span>.GTID_NEXT<span class="token operator">=</span> <span class="token string">'AUTOMATIC'</span> /* added by mysqlbinlog */ /*<span class="token operator">!</span>*/<span class="token punctuation">;</span>DELIMITER <span class="token punctuation">;</span><span class="token comment"># End of log file</span>/*<span class="token operator">!</span><span class="token number">50003</span> SET <span class="token assign-left variable">COMPLETION_TYPE</span><span class="token operator">=</span>@OLD_COMPLETION_TYPE*/<span class="token punctuation">;</span>/*<span class="token operator">!</span><span class="token number">50530</span> SET @@<span class="token environment constant">SESSION</span>.PSEUDO_SLAVE_MODE<span class="token operator">=</span><span class="token number">0</span>*/<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p><code>server id 1</code>表示这个事务是在 server_id&#x3D;1 的这个库上执行的。</p></li><li><p><code>CRC32 0xc30d2901</code>每个 event 都有 CRC32 的值，这是因为我把参数 binlog_checksum 设置成了 CRC32。</p><ul><li>可以通过<code>show variables like &#39;%binlog_checksum%&#39;;</code> 查看binlog_checksum 的值</li></ul></li><li><p><code>SET TIMESTAMP=1667287271/*!*/;</code>当前sql执行的时间戳，在主备同步的时候，如果有延迟，而sql中又使用了日期函数的话，容易导致主备不一致，所以mysql在binlog中，保存了每个sql执行的时间，这样主备同步的时候，日期函数就不会出问题了。</p></li><li><p><code>@1=4 /* INT meta=0 nullable=0 is_null=0 */</code></p></li><li><p><code>@2=4 /* INT meta=0 nullable=1 is_null=0 */</code></p></li><li><p><code>@3=1541779200 /* TIMESTAMP(0) meta=0 nullable=0 is_null=0 */</code></p></li><li><p>上面这三行，表示被删掉的这条记录的原始的值。为什么会记录的这么详细？因为我们开启了记录全部信息</p></li></ul><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">mysql<span class="token operator">></span> <span class="token keyword">show</span> variables <span class="token operator">like</span> <span class="token string">'%binlog_row_image%'</span><span class="token punctuation">;</span><span class="token operator">+</span><span class="token comment">------------------+-------+</span><span class="token operator">|</span> Variable_name    <span class="token operator">|</span> <span class="token keyword">Value</span> <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">------------------+-------+</span><span class="token operator">|</span> binlog_row_image <span class="token operator">|</span> <span class="token keyword">FULL</span>  <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">------------------+-------+</span><span class="token number">1</span> <span class="token keyword">row</span> <span class="token operator">in</span> <span class="token keyword">set</span><span class="token punctuation">,</span> <span class="token number">1</span> warning <span class="token punctuation">(</span><span class="token number">0.02</span> sec<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>如果把<code>binlog_row_image</code>设置为：MINIMAL，就不会记录的这么详细的，只会记录一个id而已。</li><li>最后的 Xid event，用于表示事务被正确地提交了。</li></ul><h6 id="mix"><a href="#mix" class="headerlink" title="mix"></a><strong>mix</strong></h6><p>mix格式的binlog其实就是statement和row格式的结合。</p><p>对于statement来说，容易导致主备同步不一致的问题，比如主备上索引选择不一致的话，就会导致主备不一致；</p><p>对于row来说，如果删除的数据很多，row会把删掉的每一条记录都记下来，占用磁盘IO，浪费空间。</p><p>所以就诞生了mix格式</p><p>在binlog_format&#x3D;mix格式下，mysql会自己判断，如果当前语句存在数据不一致的风险，就会采用row格式，否则采用statement格式；</p><p>但是mix格式下，因为是mysql自己判断的，这些判断逻辑在实际环境中，可能会出现不可预知的问题。</p><p>所以：一般都是直接使用 ROW 格式</p><p>在公司的生产环境，使用的binlog格式是：ROW</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token comment">-- 查看binlog的格式</span><span class="token keyword">show</span> variables <span class="token operator">like</span> <span class="token string">'binlog_format'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="mysql的WAL机制"><a href="#mysql的WAL机制" class="headerlink" title="mysql的WAL机制"></a>mysql的WAL机制</h2><p>WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。</p><p>WAL 机制是减少磁盘写，可是每次提交事务都要写 redo log 和 binlog，这磁盘读写次数也没变少呀？</p><p>现在你就能理解了，WAL 机制主要得益于两个方面：</p><ul><li>redo log 和 binlog 都是顺序写，磁盘的顺序写比随机写速度要快；</li><li>组提交机制，可以大幅度降低磁盘的 IOPS 消耗。怎么降低的呢？<a href="#%E7%BB%84%E6%8F%90%E4%BA%A4%EF%BC%88%E5%8F%8C1%E9%85%8D%E7%BD%AE%EF%BC%8CLSN%EF%BC%89">点我去看：组提交（双1配置，LSN）</a></li></ul><h2 id="日志的写入流程（二阶段提交）"><a href="#日志的写入流程（二阶段提交）" class="headerlink" title="日志的写入流程（二阶段提交）"></a>日志的写入流程（二阶段提交）</h2><h3 id="redolog和binlog的整体写入流程（二阶段提交）"><a href="#redolog和binlog的整体写入流程（二阶段提交）" class="headerlink" title="redolog和binlog的整体写入流程（二阶段提交）"></a>redolog和binlog的整体写入流程（二阶段提交）</h3><pre class="mermaid">sequenceDiagramclient->>server:更新ID=2这一行server->>innodb:获取ID=2这一行innodb->>innodb:ID=2这一行是否在内存页中innodb->>server:在内存中，直接返回ID=2的行数据innodb->>disk:不在内存中\n从磁盘中加载ID=2这一行的数据页disk->>innodb:返回ID=2所在的数据页innodb->>server:返回ID=2的行数据server->>server:对ID=2的数据，进行更新操作server->>innodb:写入更新后的数据innodb->>innodb:更新内存innodb->>innodb:记录redolog，处于prepare状态innodb->>server:更新成功server->>server:记录binlogserver->>innodb:提交事务innodb->>innodb:redolog提交，处于commit状态innodb->>server:更新完成server->>client:更新完成</pre><h4 id="为什么要有两阶段提交（反证法）"><a href="#为什么要有两阶段提交（反证法）" class="headerlink" title="为什么要有两阶段提交（反证法）"></a>为什么要有两阶段提交（反证法）</h4><p>为什么必须要有两阶段提交呢？我们知道两阶段提交是为了保证分布式事务的数据一致性的， 那么mysql是要保证什么数据的一致性。</p><p>很显然：是为了保证redolog和binlog的数据一致性</p><p>那么为什么要保证redolog和binlog的数据一致性呢？这就涉及到redolog和binlog的作用是什么呢？对！是为了崩溃恢复。</p><p>那么我们看看没有两阶段提交会怎么样？</p><p>1、<strong>先写redolog后写binlog</strong>。假设redolog写完了，binlog没写完，mysql崩了。重启之后，因为redolog完整，数据恢复；但是binlog不完整， binlog 里面就没有记录这个语句。然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库恢复出来数据与原库的值不同。 另外主备一致也是通过binlog同步的，binlog不完整，备库的数据就不对了。</p><p>2、<strong>先写binlog后写redolog</strong>。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以数据不变。但是 binlog 里面已经记录了数据变更的日志。所以，在之后用 binlog 来恢复的时候数据与原库的值不同。</p><h3 id="redolog的写入流程"><a href="#redolog的写入流程" class="headerlink" title="redolog的写入流程"></a>redolog的写入流程</h3><pre class="mermaid">sequenceDiagraminnodb->>innodb:dml成功\n更新内存\n开始记录redologinnodb->>redologbuffer:redolog记录到内存中redologbuffer->>pagecache:写入到文件系统的pagecache中(write)pagecache->>disk:持久化到磁盘文件(fsync)</pre><p>我们看到redolog写盘是有三步的，先写redologbuffer，在write到pagecache中，在fsync到disk中；</p><p>那么问题来了，这三步，对于mysql来说，究竟哪一步才算做redolog写入成功了呢？</p><p>是写到redologbuffer就行了，还是必须要fsync之后才行呢？</p><p>其实这是通过一个参数配置的：innodb_flush_log_at_trx_commit</p><ul><li>当innodb_flush_log_at_trx_commit&#x3D;0的时候，表示每次事务提交都只留在redologbuffer中；</li><li>当innodb_flush_log_at_trx_commit&#x3D;1的时候，表示每次事务提交都会fsync持久化到磁盘中；</li><li>当innodb_flush_log_at_trx_commit&#x3D;2的时候，表示每次事务提交都只是write到文件系统的pagecache中；</li></ul><p>公司的生产环境，配置的是：innodb_flush_log_at_trx_commit &#x3D; 1</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token comment">-- redolog的刷盘配置</span><span class="token keyword">show</span> variables <span class="token operator">like</span> <span class="token string">'innodb_flush_log_at_trx_commit'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>redologbuffer有多大？超过了怎么办呢？</p><ul><li>redologbuffer有多大是通过innodb_log_buffer_size来控制的。show viriables like ‘innodb_log_buffer_size’</li><li>超过了怎么办？不会超过的，为什么呢？<ul><li>当redologbuffer中存的内容超过innodb_log_buffer_size一半的时候，就会触发write到文件系统的pagecache中</li><li>所以不会超过的。</li></ul></li></ul><h3 id="binlog的写入流程"><a href="#binlog的写入流程" class="headerlink" title="binlog的写入流程"></a>binlog的写入流程</h3><pre class="mermaid">sequenceDiagraminnodb->>server:当dml成功\n此时redolog处于prepare状态server->>server:开始记录binlogserver->>binlogcache:将binlog记录到binlogcache中binlogcache->>binlogcache:binlog cache\n是在内存中的\n每个线程私有的binlogcache->>pagecache:将binlog写入到操作系统的pagecache中（write）pagecache->>disk:数据持久化到磁盘（fsync）\n这一步占用IOPS</pre><p>我们看到binlog写盘是有三步的，先写binlogcache，在write到pagecache中，在fsync到disk中；</p><p>那么问题来了，这三步，对于mysql来说，究竟哪一步才算做binlog写入成功了呢？</p><p>是写到binlogcache就行了，还是必须要fsync之后才行呢？</p><p>其实这是通过一个参数配置的：sync_binlog</p><ul><li>sync_binlog&#x3D;0的时候，表示每次事务提交都只write，不fsync</li><li>sync_binlog&#x3D;1的时候，表示每次事务都会执行fsync</li><li>sync_binlog&#x3D;N（N&gt;1）的时候，表示累计到N个事务之后，才fsync</li></ul><p>公司的生产环境，配置的是：sync_binlog &#x3D; 1</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token comment">-- binlog的刷盘配置</span><span class="token keyword">show</span> variables <span class="token operator">like</span> <span class="token string">'sync_binlog'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>binlogcache有多大呢？超过了怎么办呢？</p><ul><li>show viriables like ‘binlog_cache_size’；表示单个线程内 binlog cache 所占内存的大小</li><li>超过了会刷盘，但是并不是写到binlog文件中，而是暂存到磁盘中，写在临时文件中；</li></ul><h3 id="redolog和binlog的刷盘时机"><a href="#redolog和binlog的刷盘时机" class="headerlink" title="redolog和binlog的刷盘时机"></a>redolog和binlog的刷盘时机</h3><p><strong>redolog的刷盘时机</strong></p><p>考虑一个极端的情况，当我们把innodb_flash_log_at_trx_commit设置为0的时候，此时redolog只会写到redologbuffer（redologbuffer是在mysql的内存中的），那么什么时候刷盘呢？</p><ul><li>刷盘时机1：InnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。</li><li>刷盘时机2：当redologbuffer中的内容占用超过redolog_buffer_size大小的一半时候，后台线程会主动写盘</li><li>刷盘时机3：当并行的其他事务提交的时候，会将redologbuffer中的所有内容都刷盘；<ul><li>对于时机3：不能将innodb_flash_log_at_trx_commit配置设置为0，因为这个时候，事务提交的时候不会刷盘</li><li>对于时机3：可能会把进行汇总的事务的redolog进行刷盘，会有问题吗？<ul><li>不会，这个和崩溃恢复的流程有关。此时redolog是prepare阶段的，要想恢复的话，还得去找binlog呢。</li></ul></li></ul></li></ul><p><strong>binlog的刷盘时机</strong></p><p>binlog会有刷盘时机吗？sync_binlog不管设置成多少，都至少保证了binlog会写到文件系统的pagecache中，接下来就是操作系统的范畴了。</p><p>详细的说，binlog的刷盘是在 “二阶段三步骤” 的第二步骤 sync state 中</p><h3 id="在两阶段提交的不同时刻，MySQL-异常重启会出现什么现象。"><a href="#在两阶段提交的不同时刻，MySQL-异常重启会出现什么现象。" class="headerlink" title="在两阶段提交的不同时刻，MySQL 异常重启会出现什么现象。"></a>在两阶段提交的不同时刻，MySQL 异常重启会出现什么现象。</h3><p>在讨论这个问题的时候，简化一下二阶段提交，只看最基本的情况。</p><pre class="mermaid">graph TD;id1([写入binlog处于prepare阶段])--时刻A-->id2([写binlog]);id2--时刻B-->id3([提交事务处于comit阶段])</pre><p><strong>时刻A</strong></p><p>就是写入 redo log 处于 prepare 阶段之后、写 binlog 之前，发生了崩溃（crash）由于此时 binlog 还没写，redo log 也还没提交，所以崩溃恢复的时候，这个事务会回滚。这时候，binlog 还没写，所以也不会传到备库。到这里，大家都可以理解。</p><p><strong>时刻B</strong></p><p>就是 binlog 写完，redo log 还没 commit 前发生 crash，那崩溃恢复的时候 MySQL 会怎么处理呢？</p><ul><li>如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交；</li><li>如果 redo log 里面的事务只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整：<ul><li>a. 如果是，则提交事务；</li><li>b. 否则，回滚事务。</li></ul></li></ul><h2 id="组提交（双1配置，LSN）"><a href="#组提交（双1配置，LSN）" class="headerlink" title="组提交（双1配置，LSN）"></a>组提交（双1配置，LSN）</h2><p>关于组提交，这篇文章是我见过讲的最好的一篇：<a href="https://zhuanlan.zhihu.com/p/567154450">https://zhuanlan.zhihu.com/p/567154450</a></p><p>上文中提到的组提交的代码在\mysql-5.7.19\sql\binlog.cc</p><p>这篇文章里面有一些图，可以帮助更好的理解：<a href="https://blog.51cto.com/u_15080021/2642167">https://blog.51cto.com/u_15080021/2642167</a></p><h3 id="双1配置"><a href="#双1配置" class="headerlink" title="双1配置"></a>双1配置</h3><p>我们看到mysql默认会把innodb_flash_log_at_trx_commit设置为1，sync_binlog设置为1，这就是通常我们所说的 MySQL 的<strong>“ 双 1 ”配置</strong>。</p><p>也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog。</p><p>那么问题来了，如果mysql的TPS是2万，也就意味着每秒要写4万次磁盘，但是单独测试磁盘的IOPS，也就在2万左右，怎么能承受住mysql的2万的TPS呢？</p><p>这个时候就用到了组提交（group commit）</p><h3 id="日志逻辑序列号（LSN）"><a href="#日志逻辑序列号（LSN）" class="headerlink" title="日志逻辑序列号（LSN）"></a>日志逻辑序列号（LSN）</h3><p>在介绍组提交之前，需要先了解日志逻辑序列号（log sequence number）LSN</p><h4 id="什么是LSN"><a href="#什么是LSN" class="headerlink" title="什么是LSN"></a>什么是LSN</h4><ul><li>每一个redolog的生成，都伴随着LSN的生成</li><li>LSN是单调递增的</li><li>LSN是用来对应redolog的一个一个的写入点（write pos）</li><li>每当写入长度为length的redolog，LSN的值就会加上length</li></ul><h4 id="flushed-to-disk-lsn"><a href="#flushed-to-disk-lsn" class="headerlink" title="flushed_to_disk_lsn"></a>flushed_to_disk_lsn</h4><p>已经刷到磁盘的LSN</p><p>系统第一次启动时，该变量的值和初始的<code>lsn</code>值是相同的。随着系统的运行，<code>redo</code>日志被不断写入<code>log buffer</code>，但是并不会立即刷新到磁盘，<code>lsn</code>的值就和<code>flushed_to_disk_lsn</code>的值拉开了差距。</p><p>如果两者的值相同时，说明log buffer中的所有redo日志都已经刷新到磁盘中了。</p><h4 id="current-flush-lsn"><a href="#current-flush-lsn" class="headerlink" title="current_flush_lsn"></a>current_flush_lsn</h4><p>当前正在刷的LSN</p><h4 id="LSN有什么用"><a href="#LSN有什么用" class="headerlink" title="LSN有什么用"></a>LSN有什么用</h4><ul><li>减少redolog组提交时候的刷盘次数，减少磁盘IO（看后面的组提交有详细的说明）</li></ul><h3 id="组提交"><a href="#组提交" class="headerlink" title="组提交"></a>组提交</h3><p>简单的记忆就是：两个阶段，三个步骤</p><p>经过 5.6&#x2F;5.7&#x2F;8.0 的逐步优化，两阶段提交的逻辑优化为：</p><ul><li>Prepare 阶段基本不变，只是将redolog写入pagecache（并不刷盘）。</li><li>Commit 阶段按步骤做流水线批处理，将锁粒度进一步拆细。Commit 阶段又拆为三个步骤：<ul><li>flush stage：redo log 刷盘（多个事务 redo 合并刷盘），按事务进入的顺序将 binlog  写入pagecache（并不刷盘）。</li><li>sync stage：对 binlog 刷盘（多个事务的 binlog 合并刷盘）。</li><li>commit stage：各个线程按顺序做 InnoDB commit 操作。</li></ul></li></ul><p>三个步骤（stage）中，每个 stage 一个队列，第一个进入该队列的线程成为 leader，后续进入的线程会作为follower，并且一直阻塞直至leader完成提交（sql语句会阻塞）。leader 线程会领导队列中的所有线程执行该 stage 的任务，并带领所有 follower 进入到下一个 stage 去执行，当遇到下一个 stage 队列不为空的时候，leader 会变成 follower 注册到此队列中。</p><pre class="mermaid">sequenceDiagramserver->>innodb:更新数据note left of innodb:第一阶段innodb->>innodb:记录redologinnodb->>pagecache:redolog write \n 事务处于preparepagecache->>innodb:write 成功innodb->>server:redolog prepare 成功note left of innodb:第二阶段第一步骤server->>server:记录binlogpagecache->>disk:redolog刷盘(【组提交】)server->>pagecache:binlog写到文件系统的cachepagecache->>server:binlog 成功note left of disk:第二阶段第二步骤pagecache->>disk:binlog刷盘（【组提交】）note left of innodb:第二阶段第三步骤server->>innodb:提交事务innodb->>innodb:事务处于commitinnodb->>server:更新完成</pre><h4 id="组提交“组”在了哪里"><a href="#组提交“组”在了哪里" class="headerlink" title="组提交“组”在了哪里"></a>组提交“组”在了哪里</h4><p>在第二阶段的第一步骤中，redolog进行了组提交刷盘</p><p>在第二阶段的第二步骤中，binlog进行组提交刷盘</p><p>那么具体是怎么“组”提交的呢？</p><p>1、在第一阶段，事务线程不停地，刷刷刷的进来，写redolog，此时只写到pagecache中；</p><p>2、在第二阶段的第一步骤中，有一个队列，假设叫【队列-1】，</p><ul><li>当前线程会先把【队列-1】清空，以便下一批有一个leader</li><li>当前线程中直接对redolog进行刷盘。ha_flush_logs(NULL, true);</li><li>第一个进来的线程作为leader，后续的线程作为follower，进入【队列-1】（也就是说队列-1和刷redolog并没有啥关系）</li></ul><p>3、redolog刷盘完成之后，leader对【队列-1】中的事务线程，进行循环，write binlog</p><p>4、write bin完成之后，【队列-1】的leader进入第二阶段的第二步骤，会进入到【队列-2】</p><p>5、在第二阶段的第二步骤中，【队列-2】的leader会等待（受<strong>binlog_group_commit_sync_delay</strong> 和 <strong>binlog_group_commit_sync_no_delay_count</strong>控制）</p><p>6、等待之后，【队列-2】的leader开始对binlog进行刷盘（因为等待了一段时间，所以binlog这里也是组提交）</p><p>7、fsync binlog之后，【队列-2】的leader进入第二阶段的第三步骤，会进入到【队列-3】</p><p>8、【队列-3】的leader会按照串行化的方式，循环，一个一个的对事务线程进行COMMIT</p><h4 id="组提交和LSN有啥关系"><a href="#组提交和LSN有啥关系" class="headerlink" title="组提交和LSN有啥关系"></a>组提交和LSN有啥关系</h4><p>首先明确：</p><ul><li>在innodb中，每条redolog都有自己的LSN，这是一个单调递增的值。</li><li>每个事务的更新操作都会包含一条或者<strong>多条</strong>redo log</li><li>各个事务在将redo log写入 redo log buffer (通过log_mutex保护)时，都会获取<strong>当前事务</strong>最大的LSN。</li></ul><p>在组提交的第二阶段的第一步骤中，redolog会被组提交刷盘，组提交刷盘的时候，会有下面的流程</p><p>那么假设三个事务 tx1, tx2, tx3的最大LSN分别为 100 , 200 , 300  时，他们同时进行提交，如果tx3获取到了 log_mutex 互斥锁, 那么他会将小于 300 之前的redo log一起落盘，同时记录  <strong>flushed_to_disk_lsn</strong>&#x3D;300， 这样 tx1, tx2不用再次请求磁盘io。</p><p>同时，如果存在 tx0 的 LSN0 &lt; 300，LSN0 也会落盘，即使tx0还没有提交。然后当tx0的事务开始提交的时候，发现redolog已经刷盘了（ flushed_to_disk_lsn &gt;&#x3D; lsn），就直接返回了，节省了时间。</p><ol><li>获取 log mutex互斥锁</li><li>如果 flushed_to_disk_lsn &gt;&#x3D; lsn, 表示日志已经被刷盘，跳转 5 后进入等待状态</li><li>如果 current_flush_lsn &gt;&#x3D; lsn, 表示日志正在刷盘中，跳转 5 后进入等待状态</li><li>将小于 lsn 的日志刷盘 (sync)</li><li>释放 log_mutex互斥锁</li></ol><h2 id="mysql的崩溃恢复Crash-Safe能力（重要作用）"><a href="#mysql的崩溃恢复Crash-Safe能力（重要作用）" class="headerlink" title="mysql的崩溃恢复Crash-Safe能力（重要作用）"></a>mysql的崩溃恢复Crash-Safe能力（重要作用）</h2><h3 id="崩溃恢复的具体步骤"><a href="#崩溃恢复的具体步骤" class="headerlink" title="崩溃恢复的具体步骤"></a>崩溃恢复的具体步骤</h3><p>1、mysql崩溃重启后，进行恢复</p><p>2、判断redolog的状态，如果redolog&#x3D;commit，直接提交事务</p><p>3、如果redolog&#x3D;prepare，则通过xid去找binlog</p><p>4、binlog存在，并且binlog是完整的，提交事务</p><p>5、binlog不存在，或者binlog存在，但是不完整，回滚事务</p><h3 id="MySQL-怎么知道-binlog-是完整的"><a href="#MySQL-怎么知道-binlog-是完整的" class="headerlink" title="MySQL 怎么知道 binlog 是完整的"></a>MySQL 怎么知道 binlog 是完整的</h3><p>回答：一个事务的 binlog 是有完整格式的：</p><ul><li>statement 格式的 binlog，最后会有 COMMIT；</li><li>row 格式的 binlog，最后会有一个 XID event。</li></ul><p>另外，在 MySQL 5.6.2 版本以后，还引入了 binlog-checksum 参数，用来验证 binlog 内容的正确性。对于 binlog 日志由于磁盘原因，可能会在日志中间出错的情况，MySQL 可以通过校验 checksum 的结果来发现。所以，MySQL 还是有办法验证事务 binlog 的完整性的。</p><p>可以通过下面的命令查看<code>binlog-checksum</code>的值</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">mysql<span class="token operator">></span> <span class="token keyword">show</span> variables <span class="token operator">like</span> <span class="token string">'%binlog_checksum%'</span><span class="token punctuation">;</span><span class="token operator">+</span><span class="token comment">-----------------+-------+</span><span class="token operator">|</span> Variable_name   <span class="token operator">|</span> <span class="token keyword">Value</span> <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">-----------------+-------+</span><span class="token operator">|</span> binlog_checksum <span class="token operator">|</span> CRC32 <span class="token operator">|</span><span class="token operator">+</span><span class="token comment">-----------------+-------+</span><span class="token number">1</span> <span class="token keyword">row</span> <span class="token operator">in</span> <span class="token keyword">set</span><span class="token punctuation">,</span> <span class="token number">1</span> warning <span class="token punctuation">(</span><span class="token number">0.00</span> sec<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这是<code>mysql 8.0</code>版本的默认值，就是CRC32，它有什么用呢，在ROW格式下的binlog，通过<code>mysqlbinlog</code>工具可以看到具体的内容</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>省略<span class="token comment"># at 5299</span><span class="token comment">#221101 15:21:11 server id 1  end_log_pos 5330 CRC32 0xc30d2901         Xid = 1824</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>省略<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>这个<code>CRC32 0xc30d2901</code>内容就是CRC32的值，用来校验binlog的完整性。</p><h3 id="redo-log-和-binlog-是怎么关联起来的"><a href="#redo-log-和-binlog-是怎么关联起来的" class="headerlink" title="redo log 和 binlog 是怎么关联起来的"></a>redo log 和 binlog 是怎么关联起来的</h3><p>它们有一个共同的数据字段，叫 XID。崩溃恢复的时候，会按顺序扫描 redo log：</p><ul><li>如果碰到既有 prepare、又有 commit 的 redo log，就直接提交；</li><li>如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。</li></ul><h3 id="如果只有binlog可以Crash-Safe吗？"><a href="#如果只有binlog可以Crash-Safe吗？" class="headerlink" title="如果只有binlog可以Crash-Safe吗？"></a>如果只有binlog可以Crash-Safe吗？</h3><p>不行，因为binlog是逻辑日志，binlog写完了，并不保证数据一定在磁盘中。</p><p>在崩溃恢复的时候，顺序扫描binlog，发现日志是完整的，那么此时要执行这个binlog吗？</p><ul><li>执行：如果数据已经写到了磁盘，又执行一遍，那数据就不对了呀。</li><li>不执行：如果数据没写到磁盘，不执行，数据就少了呀。</li></ul><p>综上，只用binlog是不可以的。</p><h3 id="如果只有redolog可以Crash-Safe吗？"><a href="#如果只有redolog可以Crash-Safe吗？" class="headerlink" title="如果只有redolog可以Crash-Safe吗？"></a>如果只有redolog可以Crash-Safe吗？</h3><p>如果只从崩溃恢复的角度来讲是可以的。你可以把 binlog 关掉，这样就没有两阶段提交了，但系统依然是 crash-safe 的。</p><p>但是呢，redolog是循环写的，所以只用redolog，mysql虽然可以crash-safe，但是不具备归档的能力了。</p><h2 id="刷脏页"><a href="#刷脏页" class="headerlink" title="刷脏页"></a>刷脏页</h2><h3 id="正常运行中的实例，数据写入后的最终落盘，是从-redo-log-更新过来的还是从-buffer-pool-更新过来的呢？"><a href="#正常运行中的实例，数据写入后的最终落盘，是从-redo-log-更新过来的还是从-buffer-pool-更新过来的呢？" class="headerlink" title="正常运行中的实例，数据写入后的最终落盘，是从 redo log 更新过来的还是从 buffer pool 更新过来的呢？"></a>正常运行中的实例，数据写入后的最终落盘，是从 redo log 更新过来的还是从 buffer pool 更新过来的呢？</h3><p>这里涉及到了，“redo log 里面到底是什么”的问题。</p><p>实际上，redo log 并没有记录数据页的完整数据，所以它并没有能力自己去更新磁盘数据页，也就不存在“数据最终落盘，是由 redo log 更新过去”的情况。</p><ul><li>如果是正常运行的实例的话，数据页被修改以后，跟磁盘的数据页不一致，称为脏页。最终数据落盘，就是把内存中的数据页写盘。这个过程，甚至与 redo log 毫无关系。</li><li>在崩溃恢复场景中，InnoDB 如果判断到一个数据页可能在崩溃恢复的时候丢失了更新，就会将这个数据页加载到内存中，然后让 redo log 更新内存中的数据页内容。更新完成后，内存页变成脏页，就回到了第一种情况的状态。</li></ul><h2 id="changebuffer（对更新的优化，对比redolog）"><a href="#changebuffer（对更新的优化，对比redolog）" class="headerlink" title="changebuffer（对更新的优化，对比redolog）"></a>changebuffer（对更新的优化，对比redolog）</h2><h3 id="changebuffer的更新流程"><a href="#changebuffer的更新流程" class="headerlink" title="changebuffer的更新流程"></a>changebuffer的更新流程</h3><p>每一次更新都必须从内存中（不在内存中，就要从磁盘中load）获取到要更新的这一行吗？</p><ul><li>不是的；</li><li>在mysql5.5之前，changebuffer叫做insert buffer，仅支持插入，在5.5之后，叫change buffer，支持了更新和删除；</li><li>changebuffer只有普通索引才能用到，因为唯一索引要判定记录是否存在，所以查询一定要的</li></ul><pre class="mermaid">sequenceDiagramclient->>server:执行dml语句server->>innodb:要执行dml语句innodb->>innodb:判断要执行的语句\n是否在内存中innodb->>cache:在内存中，直接操作内存cache->>innodb:操作完成，返回innodb->>server:操作完成，返回server->>client:操作完成innodb->>changebuffer:不在内存中，记录到change bufferchangebuffer->>innodb:操作完成，返回innodb->>server:操作完成，返回server->>client:操作完成note over client,disk:记录到缓存中，什么时候刷到磁盘呢？changebuffer->>disk:后台线程定时刷changebuffer->>disk:changebuffer不足时changebuffer->>disk:数据库正常关闭时changebuffer->>disk:redolog写满时\n（此时数据库不可用）note over client,disk:记录在缓存中，查询的时候怎么办client->>server:请求查询某一条记录server->>innodb:查询某一条记录innodb->>innodb:判断要查询的语句\n是否在内存中cache->>innodb:在内存中，直接返回innodb->>server:操作完成，返回server->>client:操作完成disk->>cache:不在内存中，从disk加载到内存中cache->>innodb:获取内存中的这一条记录innodb->>innodb:判断这一条记录是否有更新innodb->>server:没有更新\n操作完成，返回changebuffer->>innodb:有更新，应用changebuffer的更新\n这一步叫mergeinnodb->>cache:将更新后的记录先记录到内存页中innodb->>server:操作完成，返回server->>client:操作完成</pre><h3 id="普通索引和唯一索引的更新流程"><a href="#普通索引和唯一索引的更新流程" class="headerlink" title="普通索引和唯一索引的更新流程"></a>普通索引和唯一索引的更新流程</h3><p>在普通索引的更新流程（可以用changebuffer）</p><pre class="mermaid">sequenceDiagramclient->server:执行dml语句server->>innodb:要执行dml语句innodb->>innodb:判断要执行的语句\n是否在内存中innodb->>cache:在内存中，直接操作内存cache->>innodb:操作完成，返回innodb->>server:操作完成，返回server->>client:操作完成innodb->>changebuffer:不在内存中，记录到change bufferchangebuffer->>innodb:操作完成，返回innodb->>server:操作完成，返回server->>client:操作完成</pre><p>在唯一索引的更新流程（不可以用changebuffer）</p><pre class="mermaid">sequenceDiagramclient->server:执行dml语句server->>innodb:要执行dml语句innodb->>innodb:判断要执行的语句\n是否在内存中innodb->>cache:在内存中，直接操作内存cache->>innodb:操作完成，返回innodb->>server:操作完成，返回server->>client:操作完成innodb->>disk:不在内存中，从磁盘中加载到内存disk->>cache:从磁盘中加载到内存cache->>innodb:操作完成，返回innodb->>server:操作完成，返回server->>client:操作完成</pre><h3 id="change-buffer-和-redo-log"><a href="#change-buffer-和-redo-log" class="headerlink" title="change buffer 和 redo log"></a>change buffer 和 redo log</h3><p>redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。</p><h2 id="一些简单的问题"><a href="#一些简单的问题" class="headerlink" title="一些简单的问题"></a>一些简单的问题</h2><p>执行一个 update 语句以后，我再去执行 hexdump 命令直接查看 ibd 文件内容，为什么没有看到数据有改变呢？</p><p>为什么 binlog cache 是每个线程自己维护的，而 redo log buffer 是全局共用的？</p><p>事务执行期间，还没到提交阶段，如果发生 crash 的话，redo log 肯定丢了，这会不会导致主备不一致呢？</p><p>如果 binlog 写完盘以后发生 crash，这时候还没给客户端答复就重启了。等客户端再重连进来，发现事务已经提交成功了，这是不是 bug？</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;mysql的日志从入门到入土&quot;&gt;&lt;a href=&quot;#mysql的日志从入门到入土&quot; class=&quot;headerlink&quot; title=&quot;mysql的日志从入门到入土&quot;&gt;&lt;/a&gt;mysql的日志从入门到入土&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;本文有xmind，配合xmind查</summary>
      
    
    
    
    <category term="JAVA" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/"/>
    
    <category term="数据库" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="MYSQL" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/"/>
    
    
    <category term="mysql" scheme="https://zhuansunpengcheng.gitee.io/tags/mysql/"/>
    
    <category term="binlog" scheme="https://zhuansunpengcheng.gitee.io/tags/binlog/"/>
    
    <category term="redolog" scheme="https://zhuansunpengcheng.gitee.io/tags/redolog/"/>
    
  </entry>
  
  <entry>
    <title>mysql的主备从入门到入土</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/mysql%E7%9A%84%E4%B8%BB%E5%A4%87%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/mysql%E7%9A%84%E4%B8%BB%E5%A4%87%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/</id>
    <published>2022-11-04T03:52:28.485Z</published>
    <updated>2022-11-04T03:52:28.485Z</updated>
    
    <content type="html"><![CDATA[<h1 id="mysql的主备从入门到入土"><a href="#mysql的主备从入门到入土" class="headerlink" title="mysql的主备从入门到入土"></a>mysql的主备从入门到入土</h1><hr><p>主备一致（主备同步）</p><pre class="mermaid">flowchart LR开始((开始))-->undologmem[undolog<br/>mem]subgraph masterAundologmem-->datamem[data<br/>mem]datamem-->redologprepare[redolog<br/>prepare]redologprepare-->binlog[binlog]binlog-->redologcommit[redolog<br/>commit]binlog-->dump_thread[dump_thread]bg_thread[bg_thread]-->undologdisk[undolog<br/>disk]undologdisk-->datadisk[data<br/>disk]endredologcommit-->ack((ack))subgraph masterBdump_thread-->io_thread[io_thread]io_thread-->readylog[readylog]readylog-->sql_thread[sql_thread]sql_thread-->data[data]end</pre><p>高可用（主备切换）</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;mysql的主备从入门到入土&quot;&gt;&lt;a href=&quot;#mysql的主备从入门到入土&quot; class=&quot;headerlink&quot; title=&quot;mysql的主备从入门到入土&quot;&gt;&lt;/a&gt;mysql的主备从入门到入土&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;主备一致（主备同步）&lt;/p&gt;
&lt;p</summary>
      
    
    
    
    <category term="JAVA" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/"/>
    
    <category term="数据库" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="MYSQL" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/"/>
    
    
    <category term="mysql" scheme="https://zhuansunpengcheng.gitee.io/tags/mysql/"/>
    
    <category term="主备一致" scheme="https://zhuansunpengcheng.gitee.io/tags/%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4/"/>
    
    <category term="高可用" scheme="https://zhuansunpengcheng.gitee.io/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    
  </entry>
  
  <entry>
    <title>idea激活</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/TOOLS/idea/idea%E6%BF%80%E6%B4%BB/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/TOOLS/idea/idea%E6%BF%80%E6%B4%BB/</id>
    <published>2022-11-02T02:15:54.618Z</published>
    <updated>2022-11-02T02:15:54.618Z</updated>
    
    <content type="html"><![CDATA[<h1 id="idea激活"><a href="#idea激活" class="headerlink" title="idea激活"></a>idea激活</h1><p>激活使用的是无限试用的方法</p><p>下载插件包：<a href="idea%E6%BF%80%E6%B4%BB.assets/ide-eval-resetter-2.1.6.zip">ide-eval-resetter-2.1.6.zip</a></p><p>直接拖到IDEA或者DataGrip中安装</p><p>安装之后，在Help中就可以看到，点击就可以重置</p><img src="idea激活.assets/image-20221011163201374.png" alt="image-20221011163201374" style="zoom:80%;" />]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;idea激活&quot;&gt;&lt;a href=&quot;#idea激活&quot; class=&quot;headerlink&quot; title=&quot;idea激活&quot;&gt;&lt;/a&gt;idea激活&lt;/h1&gt;&lt;p&gt;激活使用的是无限试用的方法&lt;/p&gt;
&lt;p&gt;下载插件包：&lt;a href=&quot;idea%E6%BF%80%E6%B</summary>
      
    
    
    
    <category term="TOOLS" scheme="https://zhuansunpengcheng.gitee.io/categories/TOOLS/"/>
    
    <category term="idea" scheme="https://zhuansunpengcheng.gitee.io/categories/TOOLS/idea/"/>
    
    
    <category term="idea破解" scheme="https://zhuansunpengcheng.gitee.io/tags/idea%E7%A0%B4%E8%A7%A3/"/>
    
  </entry>
  
  <entry>
    <title>我的面试问题</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/WORKER/%E6%88%91%E7%9A%84%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/WORKER/%E6%88%91%E7%9A%84%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98/</id>
    <published>2022-10-31T10:57:46.524Z</published>
    <updated>2022-10-31T10:57:46.524Z</updated>
    
    <content type="html"><![CDATA[<p>redis的基本类型有哪些？<br>redis的有哪些使用场景？<br>String类型里面有bitmap，了解过吗？<br>sorted_set的底层原理是什么？<br>    跳表<br>redis的事务<br>    如果A，B两个事务，A事务修改了k1的值，然后查询k1，B事务删了k1，如果B的exec先发送完，A的exec后发送，会出现什么现象<br>    怎么解决这个现象呢？<br>缓存穿透？布隆过滤器？<br>Redis的持久化？RDB和AOF的区别<br>    RDB的原理：父子进程fork 和 copy on write； 指针指向同一个key，如果这个key修改了怎么办？<br>Redis主从中怎么保证数据的一致性（主从复制怎么实现的）<br>    - 通过RDB文件进行同步的，同步有两种方式网络和磁盘<br>    redis的管道pipline用过吗<br>    redis的内存如果用完了，会怎么样？<br>    - Redis有哪几种数据淘汰策略？<br>    - LRU<br>    - 怎么保证都是热点数据<br>    redis的集群有了解吗?<br>    普通哈希，一致性哈希（哈希环），哈希槽</p><p>一条查询语句是怎么执行的？</p><p>事务的四个特性，Mysql是怎么实现这四个特性的<br>事务有哪些隔离级别？<br>RR是怎么解决脏读的？<br>生产使用的隔离级别是哪种？<br>MySQL日志有哪几种？区别呢？</p><p>如何强制使用某个索引？<br>如果有一个很长的url存到了库中，我要利用这个字段去精确查询某行记录，怎么创建索引更好？<br>前缀索引怎么确定长度？</p><p>mysql有哪些调优方案？<br>    - 索引，使用覆盖索引，索引下推；<br>mysql会不会选错索引？选错了怎么办？<br>    - sql中手动指定索引<br>    - 删除掉走错的索引<br>mysql为什么会选错索引？<br>    - 统计不准，可以anlize table；<br>analize table准确吗？<br>    - 不准确，抽样采集的；</p><p>涉及到财务的系统设计</p><ul><li><a href="http://confluence.sf-express.com/pages/viewpage.action?pageId=189135181">http://confluence.sf-express.com/pages/viewpage.action?pageId=189135181</a></li><li>快递  银行  商家</li><li>快递根据快递单生成账单，根据账单的金额，调用银行进行打款申请，打款给商家；</li><li>快递单号，发件人，发件人银行账号，收件人，快递金额</li><li>用户可能涉及的状态：</li></ul><p>DDD的一些基本概念：</p><ul><li>实体和值对象的区别是什么？</li><li>一个查询请求过来，打到DDD后台之后，你们的分层是怎么样的，这个请求的链路是什么样子的</li><li>防腐层是什么？</li></ul><p>mysql</p><ul><li>对一个字段创建索引，是选择普通索引，还是唯一索引（buffer）？</li><li>对一个很长的字段，想精确查询，怎么建立索引？</li><li>回表？ 怎么减少回表<ul><li>有没有可能经过索引优化，避免回表过程呢？（覆盖索引）</li></ul></li><li><h2 id="索引的B-树"><a href="#索引的B-树" class="headerlink" title="索引的B+树"></a>索引的B+树</h2></li><li>mysql的事务特性<ul><li>日志</li></ul></li></ul><p>分布式的基本概念：<br>    - 二阶段提交流程是什么？有什么问题？</p><p>JVM垃圾回收</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;redis的基本类型有哪些？&lt;br&gt;redis的有哪些使用场景？&lt;br&gt;String类型里面有bitmap，了解过吗？&lt;br&gt;sorted_set的底层原理是什么？&lt;br&gt;    跳表&lt;br&gt;redis的事务&lt;br&gt;    如果A，B两个事务，A事务修改了k1的值，然后查询</summary>
      
    
    
    
    <category term="WORKER" scheme="https://zhuansunpengcheng.gitee.io/categories/WORKER/"/>
    
    
    <category term="面试" scheme="https://zhuansunpengcheng.gitee.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>docker+hexo+gitee部署完美个人博客</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/TOOLS/hexo/docker+hexo+gitee%E9%83%A8%E7%BD%B2%E5%AE%8C%E7%BE%8E%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/TOOLS/hexo/docker+hexo+gitee%E9%83%A8%E7%BD%B2%E5%AE%8C%E7%BE%8E%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</id>
    <published>2022-10-31T10:57:46.522Z</published>
    <updated>2022-10-31T10:57:46.522Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>因为引用的文章在下面都说到了，可以先看看</p><p><a href="https://zhuanlan.zhihu.com/p/372398281">Docker + Git 部署Hexo发布</a></p><p><a href="https://www.cnblogs.com/moshuying/p/15801437.html">【hexo指南】hexo配置ER图流程图时序图插件</a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>因为平时使用typora写文章，除了基本的MD语法之外，还使用了</p><ul><li>本地图片</li><li>mermaid</li></ul><p>就是上面两点，导致部署hexo比较麻烦，特别是第一点。</p><p>为什么呢？</p><p>hexo新版本不是支持了本地图片吗？</p><p>但是支持的前提是：图片的文件夹和MD文件名一致，像下面这样</p><ul><li>xxxxx.md</li><li>xxxxx</li></ul><p>而我的图片存储方式是：</p><ul><li>xxxxx.md</li><li>xxxxx.assets</li></ul><p>对应typora的设置就是：</p><img src="docker+hexo+gitee部署完美个人博客.assets/image-20221030220722156.png" alt="image-20221030220722156" style="zoom:50%;" /><p>所以hexo就不支持了，我就很难受</p><h2 id="第一步：准备hexo文件"><a href="#第一步：准备hexo文件" class="headerlink" title="第一步：准备hexo文件"></a>第一步：准备hexo文件</h2><p>你需要准备你的所有笔记文件：比如我的是放在gitee的，大致有下面这些笔记</p><img src="docker+hexo+gitee部署完美个人博客.assets/image-20221030220831563.png" alt="image-20221030220831563" style="zoom:50%;" /><p>然后需要准备一个hexo的主题，我推荐是 butterfly，把主题下载下来</p><p>然后需要准备hexo的配置文件_config.yml，配置好你所需要的所有内容</p><p>然后准备下面的脚本，命名为：hexo-img-move.js</p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">const</span> fs <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">'fs-extra'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//note/xxx/xx.assets  2020/10/30/xxx/xx.assets</span><span class="token keyword">function</span> <span class="token function">copy</span><span class="token punctuation">(</span><span class="token parameter">dir<span class="token punctuation">,</span>dest</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>fs<span class="token punctuation">.</span><span class="token function">pathExists</span><span class="token punctuation">(</span>dir<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token parameter">err<span class="token punctuation">,</span> exists</span><span class="token punctuation">)</span> <span class="token operator">=></span> <span class="token punctuation">&#123;</span>  <span class="token keyword">if</span> <span class="token punctuation">(</span>exists<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>fs<span class="token punctuation">.</span><span class="token function">copy</span><span class="token punctuation">(</span>dir<span class="token punctuation">,</span> dest<span class="token punctuation">,</span> <span class="token parameter">err</span> <span class="token operator">=></span> <span class="token punctuation">&#123;</span>  <span class="token keyword">if</span><span class="token punctuation">(</span>err<span class="token punctuation">)</span>   <span class="token keyword">return</span> console<span class="token punctuation">.</span><span class="token function">error</span><span class="token punctuation">(</span>err<span class="token punctuation">)</span><span class="token punctuation">;</span>  console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token string">'success!'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token comment">//遍历递归public文件夹，将</span><span class="token keyword">function</span> <span class="token function">readFileList</span><span class="token punctuation">(</span><span class="token parameter">path<span class="token punctuation">,</span> filesList</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">var</span> files <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">readdirSync</span><span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token punctuation">;</span>    files<span class="token punctuation">.</span><span class="token function">forEach</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token parameter">itm<span class="token punctuation">,</span> index</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token keyword">var</span> stat <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">statSync</span><span class="token punctuation">(</span>path <span class="token operator">+</span> itm<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>stat<span class="token punctuation">.</span><span class="token function">isDirectory</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span><span class="token comment">//递归读取文件</span>            <span class="token function">readFileList</span><span class="token punctuation">(</span>path <span class="token operator">+</span> itm <span class="token operator">+</span> <span class="token string">"/"</span><span class="token punctuation">,</span> filesList<span class="token punctuation">)</span>        <span class="token punctuation">&#125;</span> <span class="token keyword">else</span> <span class="token punctuation">&#123;</span>            <span class="token keyword">var</span> obj <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span><span class="token comment">//定义一个对象存放文件的路径和名字</span>            obj<span class="token punctuation">.</span>path <span class="token operator">=</span> path<span class="token punctuation">;</span><span class="token comment">//路径</span>            obj<span class="token punctuation">.</span>filename <span class="token operator">=</span> itm<span class="token comment">//名字</span>            filesList<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span>obj<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token keyword">var</span> getFiles <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token comment">//获取文件夹下的所有文件</span>    <span class="token function-variable function">getFileList</span><span class="token operator">:</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token parameter">path</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token keyword">var</span> filesList <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token function">readFileList</span><span class="token punctuation">(</span>path<span class="token punctuation">,</span> filesList<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> filesList<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span>getFiles<span class="token punctuation">.</span><span class="token function">getFileList</span><span class="token punctuation">(</span><span class="token string">'/var/www/hexo/public/'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">forEach</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token parameter">obj</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span><span class="token comment">// console.log(obj.path);</span><span class="token comment">// var paths = obj.path.split('public/');</span><span class="token comment">// var dir_part = paths[1];</span><span class="token comment">// var dir_full = '/var/www/hexo/source/_posts/'+dir_part;</span><span class="token comment">// var dir = dir_full.substring(0,dir_full.length-1)+'.assets';</span><span class="token keyword">var</span> dir_part <span class="token operator">=</span> obj<span class="token punctuation">.</span>path<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">'note'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">var</span> dir_full <span class="token operator">=</span> <span class="token string">'/var/www/hexo/source/_posts/note'</span><span class="token operator">+</span>dir_part<span class="token punctuation">;</span><span class="token keyword">var</span> dir <span class="token operator">=</span> dir_full<span class="token punctuation">.</span><span class="token function">substring</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span>dir_full<span class="token punctuation">.</span>length<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">'.assets'</span><span class="token punctuation">;</span><span class="token comment">//第一种：复制到 dir 的下一级目录下</span><span class="token keyword">var</span> dirs <span class="token operator">=</span> dir<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> aessets_name <span class="token operator">=</span> dirs<span class="token punctuation">[</span>dirs<span class="token punctuation">.</span>length<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token keyword">var</span> dest1 <span class="token operator">=</span> obj<span class="token punctuation">.</span>path<span class="token operator">+</span>aessets_name<span class="token punctuation">;</span><span class="token comment">// 第二种：复制到 dir 同级目录下</span><span class="token comment">// var dest2 = obj.path.substring(0,obj.path.length-1)+'.assets';</span><span class="token comment">//第三种：复制到 public 下</span><span class="token keyword">var</span> dirs <span class="token operator">=</span> dir<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> forder_name <span class="token operator">=</span> dirs<span class="token punctuation">[</span>dirs<span class="token punctuation">.</span>length<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">'.assets'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">var</span> dest_full <span class="token operator">=</span> obj<span class="token punctuation">.</span>path<span class="token punctuation">.</span><span class="token function">substring</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span>obj<span class="token punctuation">.</span>path<span class="token punctuation">.</span>length<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">'.assets'</span><span class="token punctuation">;</span><span class="token keyword">var</span> dests <span class="token operator">=</span> dest_full<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> dest3 <span class="token operator">=</span> <span class="token string">'/var/www/hexo/public/'</span><span class="token operator">+</span>dests<span class="token punctuation">[</span>dests<span class="token punctuation">.</span>length<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token comment">//</span><span class="token comment">///var/www/hexo/public/2022/10/30/note/JAVA/数据库/MYSQL/mysql的日志从入门到入土/</span><span class="token comment">// console.log(dir)</span><span class="token function">copy</span><span class="token punctuation">(</span>dir<span class="token punctuation">,</span>dest1<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">copy</span><span class="token punctuation">(</span>dir<span class="token punctuation">,</span>dest3<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="docker+hexo+gitee部署完美个人博客.assets/image-20221030221220970.png" alt="image-20221030221220970" style="zoom:80%;" /><h2 id="第二步：准备dockerfile"><a href="#第二步：准备dockerfile" class="headerlink" title="第二步：准备dockerfile"></a>第二步：准备dockerfile</h2><p>将下面的文件 命名为：Dockerfile  没有后缀名</p><pre class="line-numbers language-docker" data-language="docker"><code class="language-docker"><span class="token instruction"><span class="token keyword">FROM</span> node:14-alpine</span><span class="token instruction"><span class="token keyword">WORKDIR</span> /var/www/hexo</span><span class="token instruction"><span class="token keyword">RUN</span> echo <span class="token string">"Asia/Shanghai"</span> > /etc/timezone <span class="token operator">\</span>    &amp;&amp; echo <span class="token string">"https://mirrors.aliyun.com/alpine/v3.9/main/"</span> > /etc/apk/repositories  <span class="token operator">\</span>    &amp;&amp; npm config set registry https://registry.npm.taobao.org <span class="token operator">\</span>    &amp;&amp; apk add --no-cache git <span class="token operator">\</span>    &amp;&amp; apk add --no-cache openssh-client <span class="token operator">\</span>    &amp;&amp; npm install hexo-cli -g <span class="token operator">\</span>    &amp;&amp; hexo init <span class="token operator">\</span>    &amp;&amp; npm install hexo-renderer-swig <span class="token operator">\</span>    &amp;&amp; npm install <span class="token operator">\</span>    &amp;&amp; npm install fs-extra --save </span>    &amp;&amp; npm install hexo-deployer-git --save \    &amp;&amp; npm install hexo-renderer-jade hexo-generator-feed hexo-generator-sitemap hexo-browsersync hexo-generator-archive --save \    &amp;&amp; npm install hexo-filter-mermaid-diagrams --save \    &amp;&amp; npm install hexo-blog-encrypt --save \    &amp;&amp; git config --global user.email "zhuansunpengcheng@qq.com" \    &amp;&amp; git config --global user.name "zhuansun" \    &amp;&amp; ssh-keygen -t RSA -C "zhuansunpengcheng@qq.com" -P "" -N "" -f /root/.ssh/id_rsa \    &amp;&amp; echo "StrictHostKeyChecking no" >> /etc/ssh/ssh_config \    &amp;&amp; cat /root/.ssh/id_rsa.pub<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>我们使用alpine作为基本镜像，因为够小，对于hexo来说，足够了</li><li>设置工作目录是 &#x2F;var&#x2F;www&#x2F;hexo</li><li>下面就是安装git，ssh，hexo</li><li>npm install fs-extra –save ：安装fs-extra的依赖，是为了我们的 hexo-img-move.js能正常运行</li><li>安装了一些hexo的插件<ul><li>hexo-deployer-git：让hexo支持直接部署到git上</li><li>hexo-renderer-jade hexo-generator-feed hexo-generator-sitemap hexo-browsersync hexo-generator-archive：这些是为了butterfly主题支持用的</li><li>hexo-filter-mermaid-diagrams：让hexo支持mermaid流程图（解决了我的第二个痛点）：<a href="https://www.cnblogs.com/moshuying/p/15801437.html">https://www.cnblogs.com/moshuying/p/15801437.html</a></li><li>hexo-blog-encrypt：加密文章，使用方法见github：<a href="https://github.com/rdou/hexo-blog-encrypt">https://github.com/rdou/hexo-blog-encrypt</a><ul><li>说明：在http环境下不支持加密，这是因为脚本中的使用的crypto对象，有一个subtle属性只能在https环境下获取到。</li><li><img src="docker+hexo+gitee部署完美个人博客.assets/image-20221031001047545.png" alt="image-20221031001047545" style="zoom:80%;" /></li><li><img src="/docker+hexo+gitee%E9%83%A8%E7%BD%B2%E5%AE%8C%E7%BE%8E%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2.assets/image-20221031001405503.png" alt="image-20221031001405503"></li></ul></li></ul></li><li>设置git的邮箱和用户名，填自己的就行</li><li>设置ssh公钥，是为了让git提交的时候不用每次都输入密码，直接用公钥提交</li><li>echo “StrictHostKeyChecking no” &gt;&gt; &#x2F;etc&#x2F;ssh&#x2F;ssh_config \： 这一步很重要，是为了容器启动的时候，跳过ssh公钥的检查，避免手动输入yes</li><li>cat &#x2F;root&#x2F;.ssh&#x2F;id_rsa.pub：打印公钥</li></ul><h2 id="第三步：构建docker镜像"><a href="#第三步：构建docker镜像" class="headerlink" title="第三步：构建docker镜像"></a>第三步：构建docker镜像</h2><p>在Dockerfile的目录下，运行下面的命令</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">docker</span> build <span class="token parameter variable">-t</span> zhuansun/hexo:v1.0 <span class="token builtin class-name">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>-t：表示指定镜像的名字和标签</li><li>zhuansun&#x2F;hexo:v1.0 : 表示镜像的名字，v1.0是标签</li><li>. ：这个点，不知道啥意思，写上</li></ul><p>查看构建的镜像</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">ash-4.3<span class="token comment"># docker image ls</span>REPOSITORY                                          TAG                       IMAGE ID       CREATED         SIZEzhuansun/hexo                                       v1.0                      71b6e1b7ea19   <span class="token number">4</span> minutes ago   249MB<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="第四步：运行容器"><a href="#第四步：运行容器" class="headerlink" title="第四步：运行容器"></a>第四步：运行容器</h2><p>我用的是群辉的docker，挂载了四个目录，和一个端口</p><img src="docker+hexo+gitee部署完美个人博客.assets/image-20221030222106418.png" alt="image-20221030222106418" style="zoom: 80%;" /><img src="docker+hexo+gitee部署完美个人博客.assets/image-20221030222129007.png" alt="image-20221030222129007" style="zoom:50%;" /><p>其他的都是默认。然后启动</p><p>注意：</p><p>挂载的文件夹里面，要有内容哦。按照第一步，该放的都放好。</p><h2 id="第五步：启动容器"><a href="#第五步：启动容器" class="headerlink" title="第五步：启动容器"></a>第五步：启动容器</h2><img src="docker+hexo+gitee部署完美个人博客.assets/image-20221030222250742.png" alt="image-20221030222250742" style="zoom:80%;" /><h2 id="第六步：使用容器"><a href="#第六步：使用容器" class="headerlink" title="第六步：使用容器"></a>第六步：使用容器</h2><p>先开启群辉的ssh，然后进入到容器里面</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">docker</span> <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> xxxxxxx <span class="token function">sh</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>进来之后，默认就是我们设置的：&#x2F;var&#x2F;www&#x2F;hexo</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">ash-4.3<span class="token comment"># docker exec -it fb29ef2559ef sh</span>/var/www/hexo <span class="token comment"># ls</span>_config.landscape.yml  db.json                package-lock.json      public                 shell                  themes_config.yml            node_modules           package.json           scaffolds              <span class="token builtin class-name">source</span>                 yarn.lock/var/www/hexo <span class="token comment"># hexo clean</span>INFO  Validating configINFO  Deleted database.INFO  Deleted public folder./var/www/hexo <span class="token comment"># hexo g</span>INFO  Validating configINFO  Start processingINFO  Generated: note/JAVA/数据库/MYSQL/mysql中的交集差集并集/index.htmlINFO  Generated: note/JAVA/GIT/git批量删除分支/index.htmlINFO  Generated: note/PROJECT/时效项目/index.htmlINFO  <span class="token number">73</span> files generated <span class="token keyword">in</span> <span class="token number">7.03</span> s/var/www/hexo <span class="token comment"># node shell/hexo-img-move.js </span>success<span class="token operator">!</span>success<span class="token operator">!</span>success<span class="token operator">!</span>/var/www/hexo <span class="token comment"># hexo server -d</span>INFO  Validating configINFO  <span class="token punctuation">[</span>Browsersync<span class="token punctuation">]</span> Access URLs: ----------------------------------          UI: http://localhost:3001 ---------------------------------- UI External: http://localhost:3001 ----------------------------------INFO  Start processingINFO  Hexo is running at http://localhost:4000/ <span class="token builtin class-name">.</span> Press Ctrl+C to stop.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>说一下 node shell&#x2F;hexo-img-move.js  这一步，hexo生成静态文件之后，只有html，并没有图片的</li><li>如果你用的是图床，那么完全啥问题都没有</li><li>如果是本地图片，我们就需要把本地图片移动到指定的文件夹中</li><li>然后hexo server启动后，html才可以找到图片（具体怎么移动的可以看上面的代码）</li></ul><h2 id="第七步：本地验证hexo服务"><a href="#第七步：本地验证hexo服务" class="headerlink" title="第七步：本地验证hexo服务"></a>第七步：本地验证hexo服务</h2><p>图片可以正常显示</p><img src="docker+hexo+gitee部署完美个人博客.assets/image-20221030222950902.png" alt="image-20221030222950902" style="zoom:80%;" /><p>代码可以正常显示</p><img src="docker+hexo+gitee部署完美个人博客.assets/image-20221030223012143.png" alt="image-20221030223012143" style="zoom:80%;" /><p>时序图可以正常显示</p><img src="docker+hexo+gitee部署完美个人博客.assets/image-20221030223026695.png" alt="image-20221030223026695" style="zoom:80%;" /><h2 id="第八步：部署到gitee"><a href="#第八步：部署到gitee" class="headerlink" title="第八步：部署到gitee"></a>第八步：部署到gitee</h2><p>首先需要配置上面的公钥到gitee上，百度很简单</p><p>然后再hexo中：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">/var/www/hexo <span class="token comment"># hexo d</span>INFO  Validating configINFO  Deploying: <span class="token function">git</span>INFO  Clearing .deploy_git folder<span class="token punctuation">..</span>.INFO  Copying files from public folder<span class="token punctuation">..</span>.INFO  Copying files from extend dirs<span class="token punctuation">..</span>.<span class="token punctuation">[</span>master c3fe339<span class="token punctuation">]</span> Site updated: <span class="token number">2022</span>-10-30 <span class="token number">14</span>:31:27Enumerating objects: <span class="token number">308</span>, done.Counting objects: <span class="token number">100</span>% <span class="token punctuation">(</span><span class="token number">308</span>/308<span class="token punctuation">)</span>, done.Delta compression using up to <span class="token number">4</span> threadsCompressing objects: <span class="token number">100</span>% <span class="token punctuation">(</span><span class="token number">247</span>/247<span class="token punctuation">)</span>, done.Writing objects: <span class="token number">100</span>% <span class="token punctuation">(</span><span class="token number">284</span>/284<span class="token punctuation">)</span>, <span class="token number">22.02</span> MiB <span class="token operator">|</span> <span class="token number">1.09</span> MiB/s, done.Total <span class="token number">284</span> <span class="token punctuation">(</span>delta <span class="token number">51</span><span class="token punctuation">)</span>, reused <span class="token number">0</span> <span class="token punctuation">(</span>delta <span class="token number">0</span><span class="token punctuation">)</span>remote: Resolving deltas: <span class="token number">100</span>% <span class="token punctuation">(</span><span class="token number">51</span>/51<span class="token punctuation">)</span>, completed with <span class="token number">3</span> <span class="token builtin class-name">local</span> objects.remote: Powered by GITEE.COM <span class="token punctuation">[</span>GNK-6.4<span class="token punctuation">]</span>To gitee.com:zhuansunpengcheng/zhuansunpengcheng.git   e4984bb<span class="token punctuation">..</span>c3fe339  HEAD -<span class="token operator">></span> masterBranch <span class="token string">'master'</span> <span class="token builtin class-name">set</span> up to track remote branch <span class="token string">'master'</span> from <span class="token string">'git@gitee.com:zhuansunpengcheng/zhuansunpengcheng.git'</span><span class="token builtin class-name">.</span>INFO  Deploy done: <span class="token function">git</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>提交成功之后，到gitee上，开始gitee page服务</p><img src="docker+hexo+gitee部署完美个人博客.assets/image-20221030223331923.png" alt="image-20221030223331923" style="zoom:50%;" /><p>然后就可以使用了</p><img src="docker+hexo+gitee部署完美个人博客.assets/image-20221030223426237.png" alt="image-20221030223426237" style="zoom:80%;" />]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;引用&quot;&gt;&lt;a href=&quot;#引用&quot; class=&quot;headerlink&quot; title=&quot;引用&quot;&gt;&lt;/a&gt;引用&lt;/h2&gt;&lt;p&gt;因为引用的文章在下面都说到了，可以先看看&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/3723</summary>
      
    
    
    
    <category term="TOOLS" scheme="https://zhuansunpengcheng.gitee.io/categories/TOOLS/"/>
    
    <category term="hexo" scheme="https://zhuansunpengcheng.gitee.io/categories/TOOLS/hexo/"/>
    
    
    <category term="docker" scheme="https://zhuansunpengcheng.gitee.io/tags/docker/"/>
    
    <category term="hexo" scheme="https://zhuansunpengcheng.gitee.io/tags/hexo/"/>
    
    <category term="gitee" scheme="https://zhuansunpengcheng.gitee.io/tags/gitee/"/>
    
  </entry>
  
  <entry>
    <title>群辉账号记录</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/NAS/%E7%BE%A4%E8%BE%89%E8%B4%A6%E5%8F%B7%E8%AE%B0%E5%BD%95/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/NAS/%E7%BE%A4%E8%BE%89%E8%B4%A6%E5%8F%B7%E8%AE%B0%E5%BD%95/</id>
    <published>2022-10-31T10:57:46.473Z</published>
    <updated>2022-10-31T10:57:46.473Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">  <script id="hbeData" type="hbeData" data-hmacdigest="98a8a0fa7bf7be52de2f404598a9d98edf936bdabea02648f86c370a258480a3">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">有东西被加密了, 请输入密码查看.</summary>
    
    
    
    <category term="NAS" scheme="https://zhuansunpengcheng.gitee.io/categories/NAS/"/>
    
    
    <category term="个人账号" scheme="https://zhuansunpengcheng.gitee.io/tags/%E4%B8%AA%E4%BA%BA%E8%B4%A6%E5%8F%B7/"/>
    
  </entry>
  
  <entry>
    <title>时效项目</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/PROJECT/%E6%97%B6%E6%95%88%E9%A1%B9%E7%9B%AE/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/PROJECT/%E6%97%B6%E6%95%88%E9%A1%B9%E7%9B%AE/</id>
    <published>2022-10-31T10:57:46.473Z</published>
    <updated>2022-10-31T10:57:46.473Z</updated>
    
    <content type="html"><![CDATA[<h1 id="时效项目"><a href="#时效项目" class="headerlink" title="时效项目"></a>时效项目</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>再快递的行业中，一个快递单号（称之为运单），在整个生命流程中，可能会产生各种各样的异常：比如丢失，退件，滞留等等各种；在这些异常中，有一种异常叫做时效异常；</p><p><strong>什么是时效异常？</strong>当我们在网上购买商品之后，都希望自己的商品能尽快送到自己手中，但是快递的流转是有自己的规则的，比如必须揽收，必须过分拨，派送，签收等等。作为快递公司，也希望能够尽快把货物送到用户手中，所以快递公司会对站点有时效性的要求：比如12H内揽收，发运在24H内需要到分拨，需要在24H内派送完成等等；如果站点完成不了这些要求，快递公司会对站点罚款。那么当这些单子不满足要求的时候，就会产生异常。我们叫做时效异常；</p><p><strong>需要做什么？</strong>时效项目做得是监听丰网所有的运单，从下单开始监控，监控运单的整个生命周期中的所有扫描信息，会不会产生时效异常。目前我们定义的时效异常有12种；每一个运单在下单之后，到签收的过程中，可能会产生0个或者多个异常；</p><table><thead><tr><th><strong>异常类型</strong></th><th><strong>判定规则</strong></th></tr></thead><tbody><tr><td>即将超时待揽收</td><td>下单后12h没有揽收</td></tr><tr><td>超时待揽收</td><td>下单后24h没有揽收</td></tr><tr><td>即将发运超时</td><td>揽收后12h没有到分拨</td></tr><tr><td>发运超时</td><td>揽收后24h没有到分拨</td></tr><tr><td>末端网点未到件</td><td>末端分拨发件后12h没有到末端网点</td></tr><tr><td>即将派签超时</td><td>派件后，12h没有签收</td></tr><tr><td>派签超时(24H-48H)</td><td>派件后，24h没有签收</td></tr><tr><td>派签超时(48H-72H)</td><td>派件后，48h没有签收</td></tr><tr><td>派签超时(72H以上)</td><td>派件后，72h没有签收</td></tr><tr><td>入库24H-48H未取</td><td>入库后，12h没有取出</td></tr><tr><td>入库48H-72H未取</td><td>入库后，48h没有取出</td></tr><tr><td>入库超72H未取</td><td>入库后，72h没有取出</td></tr></tbody></table><h2 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h2><p>请求量大：目前丰网的订单量在500万，扫描量每天亿级别；平均每分钟的吞吐量在7万，高峰期吞吐量在15万；</p><p>数据量大：订单量每天500万左右，需要存储3个月的数据：500万 * 90天 &#x3D; 45亿的数据</p><p>实时性高：要求产生时效异常之后，网点立马跟进处理，所以实时性要求很高；出现异常，立马感知到；</p><p>变化频繁：由于政策的变化，时效异常的考核可能不同，比如暴雨，疫情，政策等原因，部分时效异常的判定规则会有变化；</p><h2 id="数据量"><a href="#数据量" class="headerlink" title="数据量"></a>数据量</h2><h4 id="订单量"><a href="#订单量" class="headerlink" title="订单量"></a>订单量</h4><ul><li>高峰期每分钟70K左右</li></ul><p><img src="/%E6%97%B6%E6%95%88%E9%A1%B9%E7%9B%AE.assets/image-20220822170909082.png" alt="image-20220822170909082"></p><h3 id="扫描量"><a href="#扫描量" class="headerlink" title="扫描量"></a>扫描量</h3><ul><li>高峰期每分钟180K左右</li></ul><p><img src="/%E6%97%B6%E6%95%88%E9%A1%B9%E7%9B%AE.assets/image-20220822171224234.png" alt="image-20220822171224234"></p><h2 id="技术栈"><a href="#技术栈" class="headerlink" title="技术栈"></a>技术栈</h2><ol><li><p>sfboot（基础springboot 2.2.2.RELEASE）</p></li><li><p>kafka</p></li><li><p>本地缓存：caffeine</p></li><li><p>中间件缓存：redis</p></li><li><p>JexlEngine 执行表达式（hutool 工具包）</p></li><li><p>mycat</p></li><li><p>延迟队列：redission</p></li></ol><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>开始是4C8G（最大堆6G）；后面升级为6C8G（最大堆6G）</p><p><img src="/%E6%97%B6%E6%95%88%E9%A1%B9%E7%9B%AE.assets/image-20220906143903876.png" alt="image-20220906143903876"></p><p>数据库</p><p>初始数据库【已废弃】</p><p><img src="/%E6%97%B6%E6%95%88%E9%A1%B9%E7%9B%AE.assets/image-20220906143740394.png" alt="image-20220906143740394"></p><p>最新的数据库</p><p><img src="/%E6%97%B6%E6%95%88%E9%A1%B9%E7%9B%AE.assets/image-20220906143812882.png" alt="image-20220906143812882"></p><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><h4 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h4><h5 id="整体UML图"><a href="#整体UML图" class="headerlink" title="整体UML图"></a>整体UML图</h5><h5 id="表概念说明"><a href="#表概念说明" class="headerlink" title="表概念说明"></a>表概念说明</h5><p>描述每张表的作用是什么</p><table><thead><tr><th>表名</th><th>含义</th><th>描述</th></tr></thead><tbody><tr><td>ag_aging_object</td><td>引擎模块-时效对象定义表</td><td>时效对象的定义</td></tr><tr><td>ag_aging_status</td><td>引擎模块-运单状态定义表</td><td>表示当前时效对象关注的运单状态，不同的时效对象关注的状态可能不一样</td></tr><tr><td>ag_operate_action</td><td>引擎模块-操作动作定义表</td><td>消费运单和扫描，映射成引擎关注的动作：下单，网点类型扫描等</td></tr><tr><td>ag_aging_factor</td><td>引擎模块-时效因子定义表</td><td>定义因子，一般是基础表中的字段</td></tr><tr><td>ag_operate_element</td><td>引擎模块-操作要素定义表</td><td>加减乘除比较符的定义</td></tr><tr><td>ag_status_action</td><td>引擎模块-操作动作状态表</td><td>定义哪些动作可能产生哪些状态：比如下单产生待揽收；</td></tr><tr><td>ag_aging_rule</td><td>引擎模块-运单状态规则表</td><td>运单状态的判定规则 组合</td></tr><tr><td>ag_aging_rule_detail</td><td>引擎模块-运单状态规则详情表</td><td>每一个规则的详细判定逻辑</td></tr><tr><td>ag_aging_warn_rule</td><td>引擎模块-时效异常规则表</td><td>异常的判定规则</td></tr></tbody></table><h4 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h4><h5 id="kafka流转图"><a href="#kafka流转图" class="headerlink" title="kafka流转图"></a>kafka流转图</h5><h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4><h5 id="一个运单的状态判断流程"><a href="#一个运单的状态判断流程" class="headerlink" title="一个运单的状态判断流程"></a>一个运单的状态判断流程</h5><h5 id="一个异常的产生流程"><a href="#一个异常的产生流程" class="headerlink" title="一个异常的产生流程"></a>一个异常的产生流程</h5><h4 id="引擎的配置"><a href="#引擎的配置" class="headerlink" title="引擎的配置"></a>引擎的配置</h4><h5 id="引擎-运单状态的判定规则"><a href="#引擎-运单状态的判定规则" class="headerlink" title="引擎-运单状态的判定规则"></a>引擎-运单状态的判定规则</h5><table><thead><tr><th><strong>运单状态</strong></th><th><strong>规则判定</strong></th><th><strong>运单状态的产生时间</strong></th><th><strong>运单状态的产生站点</strong></th></tr></thead><tbody><tr><td>待揽收</td><td></td><td>ORDER_TIME</td><td>SEND_SITE</td></tr><tr><td>已揽收</td><td></td><td>REC_TIME</td><td>REC_SITE</td></tr><tr><td>始发分拨到件</td><td></td><td>FIRST_CENTER_ARR_TIME</td><td>FIRST_CENTER</td></tr><tr><td>始发分拨发件</td><td></td><td>FIRST_CENTER_SEND_TIME</td><td>FIRST_CENTER</td></tr><tr><td>末端分拨到件</td><td></td><td>END_CENTER_ARR_TIME</td><td>END_CENTER</td></tr><tr><td>末端分拨发件</td><td></td><td>END_CENTER_SEND_TIME</td><td>END_CENTER</td></tr><tr><td>末端网点到件</td><td></td><td>END_SITE_TIME</td><td>END_SITE</td></tr><tr><td>派件中</td><td></td><td>DISP_TIME</td><td>DISP_SITE</td></tr><tr><td>已入库</td><td></td><td>BOX_IN_TIME</td><td>BOX_IN_SITE</td></tr><tr><td>已取消</td><td></td><td>CANCEL_TIME</td><td></td></tr><tr><td>已签收</td><td></td><td>SIGN_TIME</td><td>SIGN_SITE</td></tr><tr><td>转寄</td><td></td><td>TRANSFER_TIME</td><td></td></tr><tr><td>退回</td><td></td><td>RETURN_TIME</td><td></td></tr><tr><td>换单&#x2F;作废</td><td></td><td>CHANGE_INVALID_TIME</td><td></td></tr></tbody></table><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="kafka扫描消费积压"><a href="#kafka扫描消费积压" class="headerlink" title="kafka扫描消费积压"></a>kafka扫描消费积压</h3><p>现象：</p><p>原因：扫描量太大，应用处理不完</p><p>解决办法：多线程（线程不能开太多，会导致cpu升高，同时线程太多，也会有部分线程堵塞，因为查库慢，只能等着），分TOPIC（本质上还是多个线程处理），升级CPU</p><h3 id="应用CPU升高"><a href="#应用CPU升高" class="headerlink" title="应用CPU升高"></a>应用CPU升高</h3><p>现象：</p><p>原因：线程数加的太多了</p><p>解决办法：</p><h3 id="数据库CPU升高"><a href="#数据库CPU升高" class="headerlink" title="数据库CPU升高"></a>数据库CPU升高</h3><p>现象：有job再跑，十分钟升高一次； 数据量大，读库不稳定，快的时候5ms，慢的时候200ms；</p><p>解决办法：</p><h3 id="数据延迟"><a href="#数据延迟" class="headerlink" title="数据延迟"></a>数据延迟</h3><p>初版的时候，因为kafka积压，会导致运单状态变更不及时，即使运单已经签收了， 在kafka中堵了4小时，此时产生了原本不应该产生的异；</p><p>原因：kafka堵了。kafka堵的原因是因为数据库层面性能瓶颈；暂未解决</p><p>解决办法：将延迟队列中的数据，扔到扫描的kafka后面去；</p><h3 id="运单基础信息表合并"><a href="#运单基础信息表合并" class="headerlink" title="运单基础信息表合并"></a>运单基础信息表合并</h3><p>现象：经过上面的多次优化，对一个扫描的处理，最终只能稳定在20-30ms左右，通过arthas查看，基本是卡在对库的操作上；</p><p>解决办法：升级mycat层的cpu（升级后8C16G 16个分片8C16G），但是效果不好；最终新购买了一个mycat集群（16C32G，32个分片4C8G）</p><ul><li>将原来的5张表缩减为1张表（非必要字段由下游业务自己填充）</li><li>运单信息由上游通过kafka扔给下游，避免下游在多一次查库</li><li>更新数据之前，判断所有的字段是否相同，如果相同，就不在操作数据库（减少一次数据库的更新）</li></ul><p>数据量大的时候，cpu会升高</p><p><img src="/%E6%97%B6%E6%95%88%E9%A1%B9%E7%9B%AE.assets/image-20221024153613762.png" alt="image-20221024153613762"></p><p>这个时候，去看应用的那个线程占用的cpu比较高？</p><p><img src="/%E6%97%B6%E6%95%88%E9%A1%B9%E7%9B%AE.assets/image-20221024153657174.png" alt="image-20221024153657174"></p><p>可以看到是消费运单状态的消费者，占用cpu比较多。用的是arthas的thread命令。</p><p>优化办法：在当前的配置下，这个部署单元，一秒钟消费的能力是有限的，如果上游给100W的数据，我们1分钟内可以消费100W的数据，但是代价是cpu升到90%，为了保证我们的CPU不超过80%，就不要再1分钟内消费这么多的数据，可以通过限制入口的扫描量，来控制。</p><p>可以通过滑动时间窗口的算法来限制。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;时效项目&quot;&gt;&lt;a href=&quot;#时效项目&quot; class=&quot;headerlink&quot; title=&quot;时效项目&quot;&gt;&lt;/a&gt;时效项目&lt;/h1&gt;&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h</summary>
      
    
    
    
    <category term="PROJECT" scheme="https://zhuansunpengcheng.gitee.io/categories/PROJECT/"/>
    
    
    <category term="项目" scheme="https://zhuansunpengcheng.gitee.io/tags/%E9%A1%B9%E7%9B%AE/"/>
    
    <category term="时效" scheme="https://zhuansunpengcheng.gitee.io/tags/%E6%97%B6%E6%95%88/"/>
    
  </entry>
  
  <entry>
    <title>看电影</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/NAS/%E7%9C%8B%E7%94%B5%E5%BD%B1/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/NAS/%E7%9C%8B%E7%94%B5%E5%BD%B1/</id>
    <published>2022-10-31T10:57:46.472Z</published>
    <updated>2022-10-31T10:57:46.472Z</updated>
    
    <content type="html"><![CDATA[<img src="看电影.assets/image-20220929172042541.png" alt="image-20220929172042541" style="zoom: 67%;" />]]></content>
    
    
      
      
    <summary type="html">&lt;img src=&quot;看电影.assets/image-20220929172042541.png&quot; alt=&quot;image-20220929172042541&quot; style=&quot;zoom: 67%;&quot; /&gt;</summary>
      
    
    
    
    <category term="NAS" scheme="https://zhuansunpengcheng.gitee.io/categories/NAS/"/>
    
    
    <category term="电影" scheme="https://zhuansunpengcheng.gitee.io/tags/%E7%94%B5%E5%BD%B1/"/>
    
  </entry>
  
  <entry>
    <title>群辉的SSH</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/NAS/%E7%BE%A4%E8%BE%89%E7%9A%84SSH/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/NAS/%E7%BE%A4%E8%BE%89%E7%9A%84SSH/</id>
    <published>2022-10-31T10:57:46.472Z</published>
    <updated>2022-10-31T10:57:46.472Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">  <script id="hbeData" type="hbeData" data-hmacdigest="89a74af18b3fadbdbf48384e172af6def4904445c666bdcd926d69ec131d0dc8">8c00d3d27fb3c22fd589b44480636208283bc4f252b9d9a315fe285b7e6fdd7f930a20fa3f69a5692dc74a7bfb3323d7e419308d04e4d1b2c69d6ce874d15309d9b162fc742adecea1031d6df21b4970298cac3e5f2c75a9773931a94ddbd44d82b561c49a1b41237030cec8a279f390d19b6c0e8e9445c6208e1dd88fdd5bd073919cb914a7302022c8b7134022ff8d8ffd9e17841462ef970cc3a330c9ed4b9e221ce8a1c472a795238e964e6a9bd39568d3ffb023b9bc150a23e5b94b3e472d35e981d51e9b1572cc8b2fab654670b4de33ec8ac492d501ec51ea8df2ef5b5d2b46cd2510351b4efc93ef1de3567119a316f11dcb488fa3435ec0ef7e5640f22a0c234b5d23a03246e045960a49c8405be82177305068354452b4a0d6f757f0ab3f5293a708f2683055c48382288dc61763ed5116eeba89374b81210b6b7ae891fd510f624f99bdd10a28537060f6125b9435738e1ee43ce46a3dd608f1399e987f29eaef971c0369b9a0e2d671b2f141f106b2859fe1ba00bd3c55b9a7105aa9e57910b314839c2cf1948d54e7fe780e0d241387ea446e6fd6d2b726e91ed597fecdbb6f1baf9a80192119a71c0dfe57c2df5c97bbe8ee2c1571eab5d01a959c014a389864b18142644f8b4001184c72a99a440e4999187b84c10abb308fe1ad28847c8ae2ab26b8f5223d5a0737b643702de328933a40909629e7f5ef4003634de8a5796156940b8482e36827fe015f2e4e18edc63feb5fa45f0da7061b528d9aac93f910d48642a7d44df5be7c5ea847f539e60d63bd94d4b326c6d4aaad071d97c9b7ae3d6e75327c0833b6bc65b3f3439d2028c282d965576dc54a920f12f65c925b620d711d6442656fbbc1c8b0aa800d6cce48c90fbe2f9146bfcdb7fe2522b1d649ee7541233c45b620be3122aa7b80e15989b0887cc138e48b554263b16418bc28fbaf7aee0f8bb2a099472d1ccbeda82a0cb9b9f952543b80ced08298fcf4ff688a50500d65041b9402</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">有东西被加密了, 请输入密码查看.</summary>
    
    
    
    <category term="NAS" scheme="https://zhuansunpengcheng.gitee.io/categories/NAS/"/>
    
    
    <category term="个人账号" scheme="https://zhuansunpengcheng.gitee.io/tags/%E4%B8%AA%E4%BA%BA%E8%B4%A6%E5%8F%B7/"/>
    
  </entry>
  
  <entry>
    <title>通过docker安装mysql5.7.19并解决中文乱码问题的方法</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/JAVA/%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%AE%B9%E5%99%A8/DOCKER/docker%E5%AE%89%E8%A3%85mysql5719%E5%B9%B6%E8%A7%A3%E5%86%B3%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/JAVA/%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%AE%B9%E5%99%A8/DOCKER/docker%E5%AE%89%E8%A3%85mysql5719%E5%B9%B6%E8%A7%A3%E5%86%B3%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/</id>
    <published>2022-10-31T10:57:46.471Z</published>
    <updated>2022-10-31T10:57:46.471Z</updated>
    
    <content type="html"><![CDATA[<h1 id="通过docker安装mysql5-7-19并解决中文乱码问题的方法"><a href="#通过docker安装mysql5-7-19并解决中文乱码问题的方法" class="headerlink" title="通过docker安装mysql5.7.19并解决中文乱码问题的方法"></a>通过docker安装mysql5.7.19并解决中文乱码问题的方法</h1><h2 id="安装Docker"><a href="#安装Docker" class="headerlink" title="安装Docker"></a>安装Docker</h2><p>首先安装docker；</p><h2 id="安装mysql5-7-19"><a href="#安装mysql5-7-19" class="headerlink" title="安装mysql5.7.19"></a>安装mysql5.7.19</h2><p>从dokcer仓库下载5.7.19版本的数据，如果不指定版本的话，下载的就是最新的版本，为什么选择5.7.19，因为公司使用的数据库是这个版本，所以就选择这个了。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">docker</span> pull mysql:5.7.19<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>下载好之后，如果有docker desktop的话，就可以看到了，没有desktop也可以使用命令查询</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">zhuansun@MacBook-Pro ~ % <span class="token function">docker</span> image listREPOSITORY  TAG    IMAGE ID    CREATED    SIZEmysql     <span class="token number">5.7</span>.19   3e3878acd190  <span class="token number">3</span> years ago  412MB<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>或者docker desktop</p><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901193908153.png" alt="image-20220901193908153" style="zoom: 50%;" /><h2 id="启动mysql5-7-19"><a href="#启动mysql5-7-19" class="headerlink" title="启动mysql5.7.19"></a>启动mysql5.7.19</h2><p>然后就是启动docker容器了：</p><p>同样的，有两种方式：一种是通过图形化工具docker-desktop；一种是通过命令；</p><h3 id="命令行简单启动"><a href="#命令行简单启动" class="headerlink" title="命令行简单启动"></a>命令行简单启动</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">docker</span> run <span class="token parameter variable">-p</span> <span class="token number">3306</span>:3306 <span class="token parameter variable">--name</span> mysql5.7.19 <span class="token punctuation">\</span><span class="token parameter variable">-e</span> <span class="token assign-left variable">MYSQL_ROOT_PASSWORD</span><span class="token operator">=</span><span class="token number">123456</span> <span class="token punctuation">\</span><span class="token parameter variable">-d</span> mysql:5.7.19<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>解释一下：</p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>docker run</td><td>没什么说的</td></tr><tr><td>-p 3306:3306</td><td>指定mysql的端口，前面是本机端口，后面是容器端口；如果容器已经起了一个3306的mysql，你想在启动一个mysql，就得把后面的端口改了。</td></tr><tr><td>–name mysql5.7.19</td><td>容器启动后的名字，只是个名字</td></tr><tr><td>-e MYSQL_ROOT_PASSWORD&#x3D;123456</td><td>docker启动mysql必须要设置密码的，这里是直接设置了root密码，也可以通过其他命令指定新的用户名和密码。网上搜就有</td></tr><tr><td>-d mysql:5.7.19</td><td>使用mysql:5.7.19这个版本的镜像</td></tr></tbody></table><h3 id="docker-desktop简单启动"><a href="#docker-desktop简单启动" class="headerlink" title="docker-desktop简单启动"></a>docker-desktop简单启动</h3><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901195130908.png" alt="image-20220901195130908" style="zoom: 50%;" /><ul><li><p>和命令行方式比较一下，就知道啥意思了。但是docker-desktop没找到哪里设置用户名和密码。所以我最终选择是使用命令行方式；</p></li><li><p>但是我们注意到dokcer-desktop下面有一个Volume卷的设置，命令行没有，它是什么意思呢？</p></li><li><p>它表示可以把容器中的数据和本地的数据进行互通；</p></li><li><p>我们知道容器一旦被关闭，或者重启，所有的数据都会丢失；所以我们要把容器中的数据映射到本地磁盘，就是这个意思。</p></li><li><p>不过，上面说了，我们使用命令行的方式启动mysql，那么命令行怎么映射呢？</p></li></ul><h3 id="命令行完整启动"><a href="#命令行完整启动" class="headerlink" title="命令行完整启动"></a>命令行完整启动</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">docker</span> run <span class="token parameter variable">-p</span> <span class="token number">3306</span>:3306 <span class="token parameter variable">--cpus</span> <span class="token number">2</span> <span class="token parameter variable">-m</span> 4GB <span class="token parameter variable">--name</span> mysql5.7.19 <span class="token punctuation">\</span><span class="token parameter variable">-v</span> /Users/zhuansun/workspace/docker/mysql5.7.19/conf:/etc/mysql <span class="token punctuation">\</span><span class="token parameter variable">-v</span> /Users/zhuansun/workspace/docker/mysql5.7.19/logs:/var/log/mysql <span class="token punctuation">\</span><span class="token parameter variable">-v</span> /Users/zhuansun/workspace/docker/mysql5.7.19/data:/var/lib/mysql <span class="token punctuation">\</span><span class="token parameter variable">-e</span> <span class="token assign-left variable">MYSQL_ROOT_PASSWORD</span><span class="token operator">=</span><span class="token number">123456</span> <span class="token punctuation">\</span><span class="token parameter variable">-d</span> mysql:5.7.19<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>解释一下：</p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>-v</td><td>-v表示将本地的一个文件夹挂载到容器中，容器每次启动的时候，本地的文件都会覆盖到容器中；容器中产生的文件，也会写到本地中。这样保证了数据的持久化</td></tr><tr><td>–cpus</td><td>配置cpu，有时候mysql跑的太快，cpu直接就满了，这里可以配置cpu，后面跟的数据，是cpu的核数</td></tr><tr><td>-m</td><td>配置内存，默认内存是2G，可以通过 docker stats 查看</td></tr></tbody></table><ul><li>然后在docker-desktop中也可以看到启动后的app</li></ul><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901195155480.png" alt="image-20220901195155480" style="zoom:50%;" /><h2 id="datagrip连接mysql"><a href="#datagrip连接mysql" class="headerlink" title="datagrip连接mysql"></a>datagrip连接mysql</h2><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">localhost<span class="token number">3306</span>root<span class="token number">123456</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li>datagrip也可以连接成功</li></ul><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901195211507.png" alt="image-20220901195211507" style="zoom:50%;" /><h2 id="中文乱码问题的发现"><a href="#中文乱码问题的发现" class="headerlink" title="中文乱码问题的发现"></a>中文乱码问题的发现</h2><ul><li><p>我以为到这儿就结束了，其实不是的。</p></li><li><p>因为我在mysql中创建了一个数据库，然后创建了一个表，通过代码插入一条记录之后，发现，妈的，乱码了。</p></li></ul><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901195229189.png" alt="image-20220901195229189" style="zoom:50%;" /><ul><li><p>然后排查为什么会乱码：</p></li><li><p>这是因为我们的mysql通过docker启动之后，默认的编码其实并不是utf8，而是latin1；这就导致了中文乱码的原因。</p></li><li><p>通过</p></li></ul><pre class="line-numbers language-mysql" data-language="mysql"><code class="language-mysql">show variables like &#39;%char%&#39;;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>也可以验证这个问题</p><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901195253309.png" alt="image-20220901195253309" style="zoom:50%;" /><h2 id="中文乱码问题的排查思路"><a href="#中文乱码问题的排查思路" class="headerlink" title="中文乱码问题的排查思路"></a>中文乱码问题的排查思路</h2><ul><li><p>那么怎么办呢？就需要修改mysql的配置文件了。</p></li><li><p>这里必须要吐槽一下百度，搜出来的东西，他娘的，都是狗屎；</p></li><li><p>搜索关键字：mac mysql 5.7.19 中文乱码</p></li><li><p>然后百度建议：修改etc下的my.cnf；他娘的，根本没有这个文件好不好。</p></li><li><p>最后还是搜索了好久好久，一直折腾到大半夜，才找到原因</p></li><li><p>在mysql5.7.18版本之前，在etc下会有一个my-default.cnf的文件（网上搜的，没有验证），然后我们安装好mysql之后，把这个文件改个名字就可以了。作为全局配置，但是后续版本，mysql把他删掉了。所以我使用的5.7.19是没有的。</p></li><li><p>那么5.7.19的配置文件在哪里呢？</p></li><li><p>应该是在etc&#x2F;mysql&#x2F;这个文件夹下面，但是我登录我的容器一看：这个文件夹下面是空的？卧槽？</p></li><li><p>后续排查发现，因为我们使用了</p></li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token parameter variable">-v</span> /Users/zhuansun/workspace/docker/mysql5.7.19/conf:/etc/mysql <span class="token punctuation">\</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><p>所以我们本地的文件覆盖了容器中的内容；本地的文件夹是空的，所以容器里面技术空了。</p></li><li><p>那么没办法了，我又重新启动了一个mysql-dokcer叫做 mysql5.7.19-1，然后没有指定-v,同时还得修改端口，要不然端口就冲突了；</p></li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">docker</span> run <span class="token parameter variable">-p</span> <span class="token number">3307</span>:3307 <span class="token parameter variable">--name</span> mysql5.7.19-1 <span class="token punctuation">\</span><span class="token parameter variable">-e</span> <span class="token assign-left variable">MYSQL_ROOT_PASSWORD</span><span class="token operator">=</span><span class="token number">123456</span> <span class="token punctuation">\</span><span class="token parameter variable">-d</span> mysql:5.7.19<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>这样我们创建了一个新的mysql容器，登录进去看看：</li></ul><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901195552187.png" alt="image-20220901195552187" style="zoom:50%;" /><ul><li>通过docker-desktop登录进去看看：</li></ul><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901195603660.png" alt="image-20220901195603660" style="zoom:50%;" /><ul><li>进入到容器中，我们可以看到其实 etc&#x2F;mysql下面是有配置文件的。</li></ul><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901195615455.png" alt="image-20220901195615455" style="zoom:50%;" /><ul><li><p>这样就验证了我们之前的猜测，就是本地的空文件夹把mysql里的配置文件给覆盖掉了。</p></li><li><p>那么怎么办呢？ 我得改配置文件啊。得想办法把配置文件放在本地文件夹中，然后再次重启容器，配置文件就可以加载进去了。</p></li><li><p>怎么把配置文件复制到本地呢？</p></li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">docker</span> cp:mysql5.7.19-1:/etc/mysql /Users/zhuansun/workspace/docker/mysql5.7.19/conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>好了，用上面的命令就可以解决了，但是复制下来之后，发现，我草？这么多文件不知道改哪一个，而且怎么还有一个 连接 文件。</li></ul><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901195651594.png" alt="image-20220901195651594" style="zoom:50%;" /><ul><li>继续百度吧，又被恶心到了一圈。最终决定还是靠自己，但是还是搜到一些有用的东西的。关于这个链接文件是什么？</li><li>其实并不是所有的配置文件都在etc下面的，mysql配置文件分为全局配置，和用户配置；是放在不同的地方的，但是<strong>会通过一个链接文件，链接过来</strong>；</li></ul><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901195658976.png" alt="image-20220901195658976" style="zoom:50%;" /><ul><li>所以按照这个思路，我们去 mysql5.7.19-1 中去看看，链接文件都连接到哪里啦。</li></ul><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901195805841.png" alt="image-20220901195805841" style="zoom:50%;" /><ul><li>打开这两个文件，看看</li></ul><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901195823380.png" alt="image-20220901195823380" style="zoom:50%;" /><ul><li><p>打开之后可以看到一个文件是用来配置 mysql 的一个文件是用来配置mysqld的，</p></li><li><p>到这里就比较明确的，我们把设置编码的命令设置进来。像下面这个样子。注意要在本地的文件夹中修改哦：只需要修改mysqld.cnf就行了，因为它是全局的</p></li></ul><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token comment"># Copyright (c) 2014, 2016, Oracle and/or its affiliates. All rights reserved.</span><span class="token comment">#</span><span class="token comment"># This program is free software; you can redistribute it and/or modify</span><span class="token comment"># it under the terms of the GNU General Public License as published by</span><span class="token comment"># the Free Software Foundation; version 2 of the License.</span><span class="token comment">#</span><span class="token comment"># This program is distributed in the hope that it will be useful,</span><span class="token comment"># but WITHOUT ANY WARRANTY; without even the implied warranty of</span><span class="token comment"># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span><span class="token comment"># GNU General Public License for more details.</span><span class="token comment">#</span><span class="token comment"># You should have received a copy of the GNU General Public License</span><span class="token comment"># along with this program; if not, write to the Free Software</span><span class="token comment"># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA</span>​<span class="token comment">#</span><span class="token comment"># The MySQL  Server configuration file.</span><span class="token comment">#</span><span class="token comment"># For explanations see</span><span class="token comment"># http://dev.mysql.com/doc/mysql/en/server-system-variables.html</span>​<span class="token punctuation">[</span>mysqld<span class="token punctuation">]</span><span class="token keyword">character</span><span class="token operator">-</span><span class="token keyword">set</span><span class="token operator">-</span>server<span class="token operator">=</span>utf8collation<span class="token operator">-</span>server<span class="token operator">=</span>utf8_general_cipid<span class="token operator">-</span><span class="token keyword">file</span>    <span class="token operator">=</span> <span class="token operator">/</span>var<span class="token operator">/</span>run<span class="token operator">/</span>mysqld<span class="token operator">/</span>mysqld<span class="token punctuation">.</span>pidsocket      <span class="token operator">=</span> <span class="token operator">/</span>var<span class="token operator">/</span>run<span class="token operator">/</span>mysqld<span class="token operator">/</span>mysqld<span class="token punctuation">.</span>sockdatadir     <span class="token operator">=</span> <span class="token operator">/</span>var<span class="token operator">/</span>lib<span class="token operator">/</span>mysql<span class="token comment">#log-error  = /var/log/mysql/error.log</span><span class="token comment"># By default we only accept connections from localhost</span><span class="token comment">#bind-address   = 127.0.0.1</span><span class="token comment"># Disabling symbolic-links is recommended to prevent assorted security risks</span>symbolic<span class="token operator">-</span>links<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">[</span>client<span class="token punctuation">]</span><span class="token keyword">default</span><span class="token operator">-</span><span class="token keyword">character</span><span class="token operator">-</span><span class="token keyword">set</span><span class="token operator">=</span>utf8<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901200508795.png" alt="image-20220901200508795" style="zoom:50%;" /><ul><li><p>然后重启docker容器；欣喜若狂</p></li><li><p>结果发现中文还是插入不进来。卧槽？而且查看编码，仍然是拉丁。奇怪了。</p></li><li><p>最后仔细看了一下，发现是拉丁的参数是database级别的。</p></li></ul><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901200533956.png" alt="image-20220901200533956" style="zoom:50%;" /><ul><li><p>然后想到，创建数据库之后，数据库是有编码的，因为这个数据库是在修改编码之前创建的，所以他的编码是不会变的。</p></li><li><p>重新创建一个数据库再次测试，发现新的数据库的编码是正确的。</p></li><li><p>至此，中文乱码问题解决。</p></li></ul><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901200554433.png" alt="image-20220901200554433" style="zoom:50%;" /><img src="docker安装mysql5719并解决中文乱码.assets/image-20220901200614121.png" alt="image-20220901200614121" style="zoom:50%;" />]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;通过docker安装mysql5-7-19并解决中文乱码问题的方法&quot;&gt;&lt;a href=&quot;#通过docker安装mysql5-7-19并解决中文乱码问题的方法&quot; class=&quot;headerlink&quot; title=&quot;通过docker安装mysql5.7.19并解决中文</summary>
      
    
    
    
    <category term="JAVA" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/"/>
    
    <category term="部署与容器" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%AE%B9%E5%99%A8/"/>
    
    <category term="DOCKER" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%AE%B9%E5%99%A8/DOCKER/"/>
    
    
    <category term="docker" scheme="https://zhuansunpengcheng.gitee.io/tags/docker/"/>
    
    <category term="mysql" scheme="https://zhuansunpengcheng.gitee.io/tags/mysql/"/>
    
    <category term="乱码" scheme="https://zhuansunpengcheng.gitee.io/tags/%E4%B9%B1%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>promethues和granafa配置</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/JAVA/%E7%9B%91%E6%8E%A7/promethues%E5%92%8Cgranafa%E9%85%8D%E7%BD%AE/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/JAVA/%E7%9B%91%E6%8E%A7/promethues%E5%92%8Cgranafa%E9%85%8D%E7%BD%AE/</id>
    <published>2022-10-31T10:57:46.470Z</published>
    <updated>2022-10-31T10:57:46.470Z</updated>
    
    <content type="html"><![CDATA[<h2 id="promethues添加新的配置："><a href="#promethues添加新的配置：" class="headerlink" title="promethues添加新的配置："></a>promethues添加新的配置：</h2><p>修改promethues.yml文件</p><img src="promethues和granafa配置.assets/image-20220902160951197.png" alt="image-20220902160951197"  /><p>比如我要添加一个kafka的监控，使用kafka-expoter: 进行如下配置：</p><img src="promethues和granafa配置.assets/image-20220902161001220.png" alt="image-20220902161001220" style="zoom:80%;" /><p>首次启动：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">nohup</span> ./prometheus <span class="token parameter variable">--config.file</span><span class="token operator">=</span>prometheus.yml <span class="token operator">&amp;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>热部署（只加载配置文件，不重启promethues的进程）</p><ul><li><p>找到promethues的进程ID： lsof -i:9090</p></li><li><p>kill -HUP pid</p></li><li><p>观察日志，或者刷新promethus的页面，可以看到新的配置被加载</p></li></ul><img src="promethues和granafa配置.assets/image-20220902161056730.png" alt="image-20220902161056730" style="zoom:80%;" /><img src="promethues和granafa配置.assets/image-20220902161108613.png" alt="image-20220902161108613" style="zoom:80%;" /><p>使用kafka-expoter指定端口号： nohup .&#x2F;kafka_exporter –kafka.server&#x3D;10.13.70.4:9092 –web.listen-address&#x3D;:9309 &amp;</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;promethues添加新的配置：&quot;&gt;&lt;a href=&quot;#promethues添加新的配置：&quot; class=&quot;headerlink&quot; title=&quot;promethues添加新的配置：&quot;&gt;&lt;/a&gt;promethues添加新的配置：&lt;/h2&gt;&lt;p&gt;修改promethu</summary>
      
    
    
    
    <category term="JAVA" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/"/>
    
    <category term="监控" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E7%9B%91%E6%8E%A7/"/>
    
    
    <category term="promethues" scheme="https://zhuansunpengcheng.gitee.io/tags/promethues/"/>
    
    <category term="granafa" scheme="https://zhuansunpengcheng.gitee.io/tags/granafa/"/>
    
  </entry>
  
  <entry>
    <title>kafka从入门到入土</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/KAFKA/kafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/KAFKA/kafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/</id>
    <published>2022-10-31T10:57:46.465Z</published>
    <updated>2022-10-31T10:57:46.465Z</updated>
    
    <content type="html"><![CDATA[<h1 id="kafka从入门到入土"><a href="#kafka从入门到入土" class="headerlink" title="kafka从入门到入土"></a>kafka从入门到入土</h1><hr><h2 id="kafka中的一些术语"><a href="#kafka中的一些术语" class="headerlink" title="kafka中的一些术语"></a>kafka中的一些术语</h2><img src="kafka从入门到入土.assets/58c35d3ab0921bf0476e3ba14069d291.jpg" alt="img" style="zoom:80%;" /><h3 id="所有名词术语"><a href="#所有名词术语" class="headerlink" title="所有名词术语"></a>所有名词术语</h3><p>消息：Record。Kafka 是消息引擎嘛，这里的消息就是指 Kafka 处理的主要对象。</p><p>主题：Topic。主题是承载消息的逻辑容器，在实际使用中多用来区分具体的业务。</p><p>分区：Partition。一个有序不变的消息序列。每个主题下可以有多个分区。</p><p>消息位移：Offset。表示分区中每条消息的位置信息，是一个单调递增且不变的值。</p><p>副本：Replica。Kafka 中同一条消息能够被拷贝到多个地方以提供数据冗余，这些地方就是所谓的副本。副本还分为领导者副本和追随者副本，各自有不同的角色划分。副本是在分区层级下的，即每个分区可配置多个副本实现高可用。</p><p>生产者：Producer。向主题发布新消息的应用程序。</p><p>消费者：Consumer。从主题订阅新消息的应用程序。</p><p>消费者位移：Consumer Offset。表征消费者消费进度，每个消费者都有自己的消费者位移。</p><p>消费者组：Consumer Group。多个消费者实例共同组成的一个组，同时消费多个分区以实现高吞吐。</p><p>重平衡：Rebalance。消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。Rebalance 是 Kafka 消费者端实现高可用的重要手段。</p><h3 id="kafka的三层消息架构"><a href="#kafka的三层消息架构" class="headerlink" title="kafka的三层消息架构"></a>kafka的三层消息架构</h3><ul><li>第一层：主题层（topic）<ul><li>每个topic可以配置M的分区（partition），而每个分区又可以配置N个副本（Replica）</li></ul></li><li>第二层：分区层（partition）<ul><li>每个分区的N个副本中，只能有一个充当领导者（leader），leader对外提供服务；</li><li>剩下的N-1个副本，都是作为追随者（follower），follower只是作为数据冗余；</li></ul></li><li>第三层：消息层<ul><li>分区中包含若干消息，每个消息的位移（offset）都是从0开始，依次递增；</li></ul></li></ul><h3 id="kafka是如何持久化数据的"><a href="#kafka是如何持久化数据的" class="headerlink" title="kafka是如何持久化数据的"></a>kafka是如何持久化数据的</h3><p>kafka使用消息日志（Log）来保存数据，一个日志就是磁盘上一个只能追加写消息的物理文件。</p><p>一个消息日志（Log）包含了多个日志段（Log Segment）,消息其实是被追加写到最新的日志段中（Log Segment）的；</p><p>当写满一个日志段中（Log Segment）的时候，会自动切分一个新的日志段中（Log Segment），老的日志段中（Log Segment）就会被封存；</p><p>kafka会有一个定时任务，定期检查老的日志段中（Log Segment）是否能够被删除，从而释放磁盘空间；</p><h3 id="两种消息模型"><a href="#两种消息模型" class="headerlink" title="两种消息模型"></a>两种消息模型</h3><p>点对点（peer to peer）</p><ul><li>同一个消息只能被下游的一个消费者消费；</li><li>kafka为了实现点对点，引入了消费者组（Consumer Group）的概念</li></ul><p>发布订阅模型（pub&#x2F;sub）：</p><h3 id="消费者组"><a href="#消费者组" class="headerlink" title="消费者组"></a>消费者组</h3><p>kafka为了实现点对点（同一个消息只能被下游的一个消费者消费），引入了消费者组（Consumer Group）的概念；</p><p>消费者组：多个消费者实例组成一组消费某一个topic，这个topic下的一条消息只能被组中的一个消费者实例消费；</p><h4 id="为什么要引入消费者组"><a href="#为什么要引入消费者组" class="headerlink" title="为什么要引入消费者组"></a>为什么要引入消费者组</h4><p>为了提升吞吐量，假设topic的消息的生产速率不变，增加消费者实例，就可以提升吞吐量；</p><h4 id="重平衡"><a href="#重平衡" class="headerlink" title="重平衡"></a>重平衡</h4><p>当组内的某一个消费者实例挂了，kafka会自动重平衡；将这个死亡的消费者实例原先消费的分区，转移给存活的消费者实例；</p><h3 id="消费者位移（Consumer-Offset）和分区位移（Offset）"><a href="#消费者位移（Consumer-Offset）和分区位移（Offset）" class="headerlink" title="消费者位移（Consumer Offset）和分区位移（Offset）"></a>消费者位移（Consumer Offset）和分区位移（Offset）</h3><p>消费者位移：消费者位移是随时变化的，毕竟它是消费者消费进度的指示器嘛。</p><p>分区位移：表征的是分区内的消息位置，它是不变的，即一旦消息被成功写入到一个分区上，它的位移值就是固定的了。</p><p>举个例子：</p><p>一个消息发送到kafka集群，kafka就会给这个消息并一个编号，这个编号就是“分区位移”；而且这个“分区位移”是固定不变的；</p><p>当有消费者消费的时候，消费者会记录我自己消费到了哪里，这个就是消费者位移；（消息者位移其实并不是记录在消费者端的，而是记录在zk或者kafka中的）；</p><h2 id="kafka的发展历史和定位"><a href="#kafka的发展历史和定位" class="headerlink" title="kafka的发展历史和定位"></a>kafka的发展历史和定位</h2><p>kakka既是一个消息引擎系统，同时又是一个分布式流处理平台；</p><h3 id="kafka的发展历史"><a href="#kafka的发展历史" class="headerlink" title="kafka的发展历史"></a>kafka的发展历史</h3><ul><li><p>是Linkedln公司内部的孵化项目。</p></li><li><p>Linkedln一开始是有 数据强实时性处理方面的需求，用了activeMq，但不理想，所以准备自己搞一套。</p></li><li><p>Kafka 自诞生伊始是以<strong>消息引擎系统</strong>的面目出现在大众视野中的。如果翻看 0.10.0.0 之前的官网说明，你会发现 Kafka 社区将其清晰地定位为一个分布式、分区化且带备份功能的提交日志（Commit Log）服务。</p></li><li><p>kafka在设计之初提供三个方面的特性：</p><ul><li>提供一套 API 实现生产者和消费者；</li><li>降低网络传输和磁盘存储开销；</li><li>实现高伸缩性架构。</li></ul></li><li><p>后来用的人越来越多，kafka思考引入了流处理；</p></li><li><p>Kafka 社区于 0.10.0.0 版本正式推出了流处理组件 Kafka Streams，也正是从这个版本开始，Kafka 正式“变身”为分布式的流处理平台，而不仅仅是消息引擎系统了。</p></li></ul><h3 id="kafka与其他的流处理框架的优点"><a href="#kafka与其他的流处理框架的优点" class="headerlink" title="kafka与其他的流处理框架的优点"></a>kafka与其他的流处理框架的优点</h3><ul><li>第一点是更容易实现端到端的正确性（Correctness）</li><li>kafka自己对于流式计算的定位</li></ul><h3 id="kafka的定位"><a href="#kafka的定位" class="headerlink" title="kafka的定位"></a>kafka的定位</h3><ul><li>消息引擎系统</li><li>流处理平台</li><li>分布式存储系统（很少）</li></ul><h2 id="kafka版本"><a href="#kafka版本" class="headerlink" title="kafka版本"></a>kafka版本</h2><h3 id="kafka发行版本"><a href="#kafka发行版本" class="headerlink" title="kafka发行版本"></a>kafka发行版本</h3><p>kafka存在多个不同的发行版本，类似linux系统中的centos，redhat，ununtu等；</p><table><thead><tr><th>类型</th><th>描述</th><th>优点</th><th>缺点</th><th>选择</th></tr></thead><tbody><tr><td>apache kafka</td><td>Apache Kafka 是最“正宗”的 Kafka，是我们学习和使用 Kafka 的基础。</td><td>优势在于迭代速度快，社区响应度高，使用它可以让你有更高的把控度</td><td>缺陷在于仅提供基础核心组件，缺失一些高级的特性。</td><td>如果你仅仅需要一个消息引擎系统亦或是简单的流处理应用场景，同时需要对系统有较大把控度，那么我推荐你使用 Apache Kafka。</td></tr><tr><td>Confluent Kafka</td><td>Confluent 公司：2014 年，Kafka 的 3 个创始人 Jay Kreps、Naha Narkhede 和饶军离开 LinkedIn 创办了 Confluent 公司，专注于提供基于 Kafka 的企业级流处理解决方案。Confluent Kafka 提供了一些 Apache Kafka 没有的高级特性，比如跨数据中心备份、Schema 注册中心以及集群监控工具等。</td><td>优势在于集成了很多高级特性且由 Kafka 原班人马打造，质量上有保证；</td><td>缺陷在于相关文档资料不全，普及率较低，没有太多可供参考的范例。</td><td>如果你需要用到 Kafka 的一些高级特性，那么推荐你使用 Confluent Kafka。</td></tr><tr><td>CDH Kafka &#x2F; HDP Kafka</td><td>Cloudera 提供的 CDH 和 Hortonworks 提供的 HDP 是非常著名的大数据平台，里面集成了目前主流的大数据框架，能够帮助用户实现从分布式存储、集群调度、流处理到机器学习、实时数据库等全方位的数据处理，不管是 CDH 还是 HDP 里面都集成了 Apache Kafka，因此我把这两款产品中的 Kafka 称为 CDH Kafka 和 HDP Kafka。</td><td>操作简单，节省运维成本</td><td>把控度低，演进速度较慢。</td><td>如果你需要快速地搭建消息引擎系统，或者你需要搭建的是多框架构成的数据平台且 Kafka 只是其中一个组件，那么我推荐你使用这些大数据云公司提供的 Kafka。</td></tr></tbody></table><h3 id="apache-kafka版本号"><a href="#apache-kafka版本号" class="headerlink" title="apache kafka版本号"></a>apache kafka版本号</h3><p>在官网上下载 Kafka 时，会看到这样的版本：</p><img src="kafka从入门到入土.assets/c10df9e6f72126e9c721fba38e27ac23.png" alt="img" style="zoom:80%;" /><p>有些人会误将Scala版本看作是Kafka版本，那么就来解释一下这个版本号</p><ul><li><p>2.11&#x2F;2.12：代表着Kafka源代码的Scala编译器版本</p></li><li><p>2.3.0：才是Kafka的版本号，2代表着大版本号；3代表着小版本号；0代表着修订版本号或补丁</p></li></ul><h4 id="版本号演进"><a href="#版本号演进" class="headerlink" title="版本号演进"></a>版本号演进</h4><p>Kafka目前经历了7个大版本，0.7、0.8、0.9、0.10、0.11、1.0和2.0，其中小版本与Patch版本很多就不一一列举</p><p>在上面的7个大版本中，在哪个版本进行了重大的改进，来好好看一下</p><img src="kafka从入门到入土.assets/Kafka版本变迁.png" alt="img" style="zoom:80%;" /><h5 id="0-7版本"><a href="#0-7版本" class="headerlink" title="0.7版本"></a>0.7版本</h5><p>这是个“上古”版本，只提供了基础的消息队列功能，还没有提供副本机制</p><h5 id="0-8版本"><a href="#0-8版本" class="headerlink" title="0.8版本"></a>0.8版本</h5><p>正式引入了副本机制，能够比较好地做到消息无丢失，新版本Producer API不稳定</p><h5 id="0-9版本"><a href="#0-9版本" class="headerlink" title="0.9版本"></a>0.9版本</h5><p>添加了基础的安全认证&#x2F;权限；新版本Producer API在这个版本中算比较稳定，但是0.9版的Consumer API BUG超多，即使提到社区也不会有人管，所以千万别用！</p><h5 id="0-10版本"><a href="#0-10版本" class="headerlink" title="0.10版本"></a>0.10版本</h5><p>是里程碑式的大版本，因为该版本引入了Kafka Streams，但还不能生产大规模部署使用，自0.10.2.2版本起，新版本Consumer API算是比较稳定了</p><h5 id="0-11版本"><a href="#0-11版本" class="headerlink" title="0.11版本"></a>0.11版本</h5><p>引入了两个重量级的功能变更：一个是提供幂等性Producer API以及事务（Transaction） API；另一个是对Kafka消息格式做了重构</p><p>Producer实现幂等性以及支持事务都是Kafka实现流处理结果正确性的基石，由于刚推出，事务API有一些Bug，另外事务API主要是为Kafka Streams应用服务的，不建议用</p><p>这个版本中各个大功能组件都变得非常稳定了，国内该版本的用户也很多，应该算是目前最主流的版本之一了</p><p>如果你对1.0版本是否适用于线上环境依然感到困惑，那么至少将你的环境升级到0.11.0.3，因为这个版本的消息引擎功能已经非常完善了</p><h5 id="1-0-x2F-2-0版本"><a href="#1-0-x2F-2-0版本" class="headerlink" title="1.0&#x2F;2.0版本"></a>1.0&#x2F;2.0版本</h5><p>合并说下1.0和2.0版本吧，因为这两个大版本主要还是Kafka Streams的各种改进，在消息引擎方面并未引入太多的重大功能特性</p><p>Kafka Streams的确在这两个版本有着非常大的变化，也必须承认Kafka Streams目前依然还在积极地发展着，如果你是Kafka Streams的用户，至少选择2.0.0版本吧</p><h5 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h5><p>不论你用的是哪个版本，都请尽量保持服务器端版本和客户端版本一致，否则你将损失很多Kafka为你提供的性能优化收益</p><h2 id="kafka线上集群部署方案怎么做"><a href="#kafka线上集群部署方案怎么做" class="headerlink" title="kafka线上集群部署方案怎么做"></a>kafka线上集群部署方案怎么做</h2><img src="kafka从入门到入土.assets/image-20220827093241211.png" alt="image-20220827093241211" style="zoom:80%;" /><h3 id="操作系统的选择"><a href="#操作系统的选择" class="headerlink" title="操作系统的选择"></a>操作系统的选择</h3><p>操作系统：Windows，Linux，MacOs</p><p>选择：Linux</p><h4 id="IO模型的使用"><a href="#IO模型的使用" class="headerlink" title="IO模型的使用"></a>IO模型的使用</h4><ul><li>阻塞IO</li><li>非阻塞IO</li><li>IO多路复用</li><li>信号驱动IO</li><li>异步IO</li></ul><p>每种IO都有自己的典型使用场景，比如：</p><ul><li>Java中的Socket对象的阻塞模式和非阻塞模式就是对应前两种；</li><li>Linux系统的select函数就属于IO多路复用</li><li>大名鼎鼎的epoll介入第三种和第四种之间；</li><li>第五种模型，目前很少有Linux支持，然而Windos却在操作系统中提供了叫IOCP线程模型属于第五种</li></ul><p>说完了IO模型，再来看kafka与IO模型的关系</p><ul><li>kafka的底层使用的是java的selector<ul><li>java的selector在linux上的实现机制是：epoll</li><li>而在windos上的实现机制是：select（IO多路复用）</li></ul></li><li>所以，将kafka部署在linux机器上，更有优势</li></ul><h4 id="网络传输效率"><a href="#网络传输效率" class="headerlink" title="网络传输效率"></a>网络传输效率</h4><p>kafka的消息是通过网络传输的，而消息又是保存在磁盘中的，所以kafka非常依赖网络和磁盘的性能；</p><p>而linux恰巧有零拷贝（Zero copy）技术，就是当数据在磁盘和网络进行传输的时候，避免昂贵的的内核态数据拷贝从而实现数据的高速传输；</p><p>而windos要到java8的60更新版本才有这个功能；</p><h4 id="社区的支持度"><a href="#社区的支持度" class="headerlink" title="社区的支持度"></a>社区的支持度</h4><p>社区对于windos版的bug不做承诺，基本不会修复；</p><h3 id="磁盘的选择"><a href="#磁盘的选择" class="headerlink" title="磁盘的选择"></a>磁盘的选择</h3><ul><li>选择机械磁盘：kafka多为顺序读写，规避了机械磁盘的弊端，替换成SSD，效益不大</li><li>不用组RAID：kafka在软件层面通过分区副本保证了高可用，基本不需要磁盘组RAID</li></ul><h3 id="磁盘容量的选择"><a href="#磁盘容量的选择" class="headerlink" title="磁盘容量的选择"></a>磁盘容量的选择</h3><ul><li><p>磁盘容量：kafka的日志有保留时间的概念，根据具体的业务量，消息大小，计算好容量；</p><ul><li><p>新增消息量</p></li><li><p>消息留存时间</p></li><li><p>平均消息大小</p></li><li><p>备份数</p></li><li><p>是否启用压缩（压缩比）</p></li></ul></li></ul><h3 id="带宽的选择"><a href="#带宽的选择" class="headerlink" title="带宽的选择"></a>带宽的选择</h3><p>目前公司普遍的带宽配置都是千兆网（每秒处理1G数据），财大气粗的公司会有万兆网（每秒处理10G数据）；</p><p>假设你公司的机房环境是千兆网络，即 1Gbps，现在你有个业务，其业务目标或 SLA 是在 1 小时内处理 1TB 的业务数据。那么问题来了，你到底需要多少台 Kafka 服务器来完成这个业务呢？</p><p>千兆网络下，单台机器，假设kafka占用70%的带宽（总要为其他进程保留一些资源），稍等，这只是它能使用的最大带宽资源，你不能让 Kafka 服务器常规性使用这么多资源，故通常要再额外预留出 2&#x2F;3 的资源，即单台服务器使用带宽 700Mb &#x2F; 3 ≈ 240Mbps。有了 240Mbps，我们就可以计算 1 小时内处理 1TB 数据所需的服务器数量了。根据这个目标，我们每秒需要处理 2336Mb 的数据，除以 240，约等于 10 台服务器。如果消息还需要额外复制两份，那么总的服务器台数还要乘以 3，即 30 台。</p><h2 id="重要的集群参数配置"><a href="#重要的集群参数配置" class="headerlink" title="重要的集群参数配置"></a>重要的集群参数配置</h2><p>参数配置分为四个方面：</p><ul><li>broker端参数配置</li><li>topic的参数配置</li><li>JVM的参数配置</li><li>操作系统的参数配置</li></ul><img src="kafka从入门到入土.assets/image-20220827105050039.png" alt="image-20220827105050039" style="zoom: 50%;" /><img src="kafka从入门到入土.assets/image-20220827111440117.png" alt="image-20220827111440117" style="zoom:50%;" /><h3 id="broker端参数（静态参数）"><a href="#broker端参数（静态参数）" class="headerlink" title="broker端参数（静态参数）"></a>broker端参数（静态参数）</h3><p>静态参数是指修改后需要重启才能生效的参数</p><h4 id="存储信息类参数"><a href="#存储信息类参数" class="headerlink" title="存储信息类参数"></a>存储信息类参数</h4><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>log.dirs</td><td>【必须指定】指定Broker需要使用的若干个文件目录路径，可配置多个</td></tr><tr><td>log.dir</td><td>只能配置一个，用来补充上面参数的</td></tr></tbody></table><h4 id="与ZK相关的参数"><a href="#与ZK相关的参数" class="headerlink" title="与ZK相关的参数"></a>与ZK相关的参数</h4><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>zookeeper.connect</td><td>负责协调管理并保存 Kafka 集群的所有元数据信息，比如集群都有哪些 Broker 在运行、创建了哪些 Topic，每个 Topic 都有多少分区以及这些分区的 Leader 副本都在哪些机器上等信息。</td></tr></tbody></table><h4 id="Broker连接相关的参数"><a href="#Broker连接相关的参数" class="headerlink" title="Broker连接相关的参数"></a>Broker连接相关的参数</h4><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>listeners</td><td>告诉外部连接需要通过什么协议访问指定主机名和端口开放的kafka服务（用于内网访问）</td></tr><tr><td>Advertised.listeners</td><td>表明这组监听器是broker对外发布的（用于外网访问）</td></tr><tr><td>host.name&#x2F;port</td><td>这俩参数是过期参数，忘掉</td></tr></tbody></table><h4 id="topic管理的参数"><a href="#topic管理的参数" class="headerlink" title="topic管理的参数"></a>topic管理的参数</h4><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>auto.create.topics.enable</td><td>是否允许自动创建topic，<br />建议设置成false；</td></tr><tr><td>unclean.leader.election.enable</td><td>是否允许Unclean Leader选举，<br />建议设置为false；<br />kafka的分区有多个副本，并不是所有的副本都有资格竞争Leader，只有保存数据比较多的才有资格；那如果保存数据比较多的副本全都挂了，那还要不要竞选Leader呢？ 就是这个参数控制的；<br />false表示不竞选，后果分区不可用；<br />true表示竞选；后果数据不一致；</td></tr><tr><td>auto.leader.rebalance.enable</td><td>是否允许定期进行Leader选举；true表示到达一定条件，kafka会自动把leader换了，注意是换掉，而不是选举；即使原来的leaderA运行的好好地，也会给换成leaderB；换leader的代价很大，建议设置为false；</td></tr></tbody></table><h4 id="数据留存方面的参数"><a href="#数据留存方面的参数" class="headerlink" title="数据留存方面的参数"></a>数据留存方面的参数</h4><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>log.retention.{hour|minutes|ms}</td><td>控制一条消息被保留多长时间</td></tr><tr><td>log.retention.bytes</td><td>Broker为保留消息提供的磁盘容量的大小</td></tr><tr><td>message.max.bytes</td><td>控制Broker能够接收的最大的消息大小</td></tr></tbody></table><h3 id="Topic的参数配置"><a href="#Topic的参数配置" class="headerlink" title="Topic的参数配置"></a>Topic的参数配置</h3><p>topic端的参数配置会覆盖broker端的参数配置</p><h4 id="数据留存方面的参数-1"><a href="#数据留存方面的参数-1" class="headerlink" title="数据留存方面的参数"></a>数据留存方面的参数</h4><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>retention.ms</td><td>规定了该topic下数据的保存时长，默认7天，如果配置了，就会覆盖broker端的配置</td></tr><tr><td>retention.bytes</td><td>规定了要为该topic预留多少磁盘容量空间</td></tr><tr><td>max.message.bytes</td><td>该参数跟 message.max.bytes 参数的作用是一样的，只不过 max.message.bytes 是作用于某个 topic，而 message.max.bytes 是作用于全局。</td></tr></tbody></table><h4 id="怎么修改topic的参数配置"><a href="#怎么修改topic的参数配置" class="headerlink" title="怎么修改topic的参数配置"></a>怎么修改topic的参数配置</h4><ul><li><p>创建topic的时候设置</p><ul><li><blockquote><p>bin&#x2F;kafka-topics.sh –bootstrap-server localhost:9092 –create –topic transaction –partitions 1 –replication-factor 1 –config retention.ms&#x3D;15552000000 –config max.message.bytes&#x3D;5242880</p></blockquote></li></ul></li><li><p>修改topic的时候设置</p><ul><li><blockquote><p>bin&#x2F;kafka-configs.sh –zookeeper localhost:2181 –entity-type topics –entity-name transaction –alter –add-config max.message.bytes&#x3D;10485760</p></blockquote></li></ul></li></ul><h3 id="JVM的参数配置"><a href="#JVM的参数配置" class="headerlink" title="JVM的参数配置"></a>JVM的参数配置</h3><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>JVM堆大小</td><td>建议设置为6GB，默认的1GB太小了</td></tr><tr><td>垃圾回收设置（java7）</td><td>cpu充足，就用CMS；否则使用ParallelGC</td></tr><tr><td>垃圾回收设置（java8）</td><td>G1</td></tr></tbody></table><h4 id="怎么对kafka设置JVM参数"><a href="#怎么对kafka设置JVM参数" class="headerlink" title="怎么对kafka设置JVM参数"></a>怎么对kafka设置JVM参数</h4><p>指定kafka的环境变量即可</p><ul><li>KAFKA_HEAP_OPTS：指定堆大小</li><li>KAFKA_JVM_PERFORMANCE_OPTS：指定垃圾回收器</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$<span class="token operator">></span> <span class="token builtin class-name">export</span> <span class="token assign-left variable">KAFKA_HEAP_OPTS</span><span class="token operator">=</span>--Xms6g  <span class="token parameter variable">--Xmx6g</span>$<span class="token operator">></span> <span class="token builtin class-name">export</span> <span class="token assign-left variable">KAFKA_JVM_PERFORMANCE_OPTS</span><span class="token operator">=</span> <span class="token parameter variable">-server</span> <span class="token parameter variable">-XX:+UseG1GC</span> <span class="token parameter variable">-XX:MaxGCPauseMillis</span><span class="token operator">=</span><span class="token number">20</span> <span class="token parameter variable">-XX:InitiatingHeapOccupancyPercent</span><span class="token operator">=</span><span class="token number">35</span> <span class="token parameter variable">-XX:+ExplicitGCInvokesConcurrent</span> <span class="token parameter variable">-Djava.awt.headless</span><span class="token operator">=</span>true$<span class="token operator">></span> bin/kafka-server-start.sh config/server.properties<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="操作系统的参数配置"><a href="#操作系统的参数配置" class="headerlink" title="操作系统的参数配置"></a>操作系统的参数配置</h3><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>文件描述符限制</td><td>ulimit -n；其实设置这个参数不重要，但是不设置后果很严重，会看到too many open file 的报错；</td></tr><tr><td>文件系统类型</td><td>文件系统类型（ext3，ext4，XFS），XFS的性能强于ext4，ZFS的性能强于XFS（但技术比较新，使用很少）</td></tr><tr><td>Swappiness</td><td>网上很多文章都提到设置其为 0，将 swap 完全禁掉以防止 Kafka 进程使用 swap 空间。我个人反倒觉得还是不要设置成 0 比较好，我们可以设置成一个较小的值。为什么呢？因为一旦设置成 0，当物理内存耗尽时，操作系统会触发 OOM killer 这个组件，它会随机挑选一个进程然后 kill 掉，即根本不给用户任何的预警。但如果设置成一个比较小的值，当开始使用 swap 空间时，你至少能够观测到 Broker 性能开始出现急剧下降，从而给你进一步调优和诊断问题的时间。基于这个考虑，我个人建议将 swappniess 配置成一个接近 0 但不为 0 的值，比如 1。</td></tr><tr><td>提交时间（系统的刷盘时间）</td><td>提交时间或者说是 Flush 落盘时间。向 Kafka 发送数据并不是真要等数据被写入磁盘才会认为成功，而是只要数据被写入到操作系统的页缓存（Page Cache）上就可以了，随后操作系统根据 LRU 算法会定期将页缓存上的“脏”数据落盘到物理磁盘上。这个定期就是由提交时间来确定的，默认是 5 秒。一般情况下我们会认为这个时间太频繁了，可以适当地增加提交间隔来降低物理磁盘的写操作。当然你可能会有这样的疑问：如果在页缓存中的数据在写入到磁盘前机器宕机了，那岂不是数据就丢失了。的确，这种情况数据确实就丢失了，但鉴于 Kafka 在软件层面已经提供了多副本的冗余机制，因此这里稍微拉大提交间隔去换取性能还是一个合理的做法。</td></tr></tbody></table><h2 id="大数据量均匀分布Broker（分区原理）"><a href="#大数据量均匀分布Broker（分区原理）" class="headerlink" title="大数据量均匀分布Broker（分区原理）"></a>大数据量均匀分布Broker（分区原理）</h2><img src="kafka从入门到入土.assets/image-20220827183614867.png" alt="image-20220827183614867" style="zoom:50%;" /><p>对于那种大批量机器组成的集群环境，每分钟产生的日志量都能以 GB 数，因此如何将这么大的数据量均匀地分配到 Kafka 的各个 Broker 上，就成为一个非常重要的问题。</p><p>我们知道kafka的数据，是以topic为概念进行存储的，而topic是一个逻辑概念，真正存放数据的是topic下的partition；partition是物理概念；</p><p>那么，为了保证大数据量的均匀分布，其实就是保证一个topic下的数据量均匀的分散在各个partition中；</p><p>那么问题来了？</p><h4 id="kafka为什么要分区"><a href="#kafka为什么要分区" class="headerlink" title="kafka为什么要分区"></a>kafka为什么要分区</h4><p>kafka为什么要分区，为什么kafka不直接存储数据，而是要分区存储？为什么要使用分区的概念，而不是直接使用topic？</p><p>分区的目的是为了<strong>负载均衡</strong>；或者说分区的目的是为了<strong>提高系统的可伸缩性</strong>；</p><ul><li>负载均衡<ul><li>如果没有分区，所有的请求全部在一个topic上，请求量大的时候，直接就崩了；</li></ul></li><li>可伸缩性<ul><li>顺丰的kafka一般是32分区，这样每一个分区都可以有一个consumer，提升系统的吞吐量；当数据量增长的时候，可以扩分区，32-&gt;64；提升系统的可伸缩性；</li><li>但是一般不建议直接扩分区，在顺丰，一般是申请新的topic，然后将消息转发到不同的topic中，变相的实现扩分区；</li><li>因为分区过多，kafka管理起来很困难，没必要增加不必要的消耗；</li></ul></li><li>分区可以实现业务上的功能（消息的顺序问题）</li></ul><p>以上说了分区存在的必要性，那么既然存在分区，怎么保证每个分区的数据量的均匀呢？这就涉及到分区的策略</p><h4 id="都有哪些分区策略"><a href="#都有哪些分区策略" class="headerlink" title="都有哪些分区策略"></a>都有哪些分区策略</h4><p>分区策略：就是决定消息被发送到哪个分区</p><table><thead><tr><th>分区策略</th><th>描述</th></tr></thead><tbody><tr><td>轮训</td><td>没有指定partitioner.class这个配置的时候，在没有指定key的时候（消息键保留策略），轮训策略是默认的</td></tr><tr><td>随机</td><td>使用的很少了，已经被废弃了</td></tr><tr><td>自定义</td><td>需要显示的配置partitioner.class这个配置，同时需要编写代码；</td></tr><tr><td>按消息键保留策略</td><td>按照key的顺序进行存放，kafka默认分区策略：如果指定了key，按照key分发；没有指定key，按照轮训；</td></tr></tbody></table><h2 id="生产者压缩算法（消息格式）"><a href="#生产者压缩算法（消息格式）" class="headerlink" title="生产者压缩算法（消息格式）"></a>生产者压缩算法（消息格式）</h2><img src="kafka从入门到入土.assets/image-20220827220455477.png" alt="image-20220827220455477" style="zoom:50%;" /><h3 id="为什么要压缩？"><a href="#为什么要压缩？" class="headerlink" title="为什么要压缩？"></a>为什么要压缩？</h3><p>说起压缩（compression），我相信你一定不会感到陌生。它秉承了用时间去换空间的经典 trade-off 思想，具体来说就是用 CPU 时间去换磁盘空间或网络 I&#x2F;O 传输量，希望以较小的 CPU 开销带来更少的磁盘占用或更少的网络 I&#x2F;O 传输。在 Kafka 中，压缩也是用来做这件事的。</p><h3 id="怎么压缩"><a href="#怎么压缩" class="headerlink" title="怎么压缩"></a>怎么压缩</h3><h4 id="kafka的消息格式"><a href="#kafka的消息格式" class="headerlink" title="kafka的消息格式"></a>kafka的消息格式</h4><p>kafka有两大类消息格式，一类是在0.11.0.0版本之前的消息格式（称作V1版本），一个是0.11.0.0版本之后的格式（称作V2版本）；</p><p>不管是哪个版本，kafka消息层次都是分为两层：</p><table><thead><tr><th>V1版本</th><th>V2版本</th></tr></thead><tbody><tr><td>消息集合（message set） + 消息（message）</td><td>消息集合（record batch） + 消息（record）</td></tr></tbody></table><p>一个消息集合中包含若干个日志项（record item），日志项（record item）才是真正封装消息的地方；</p><p>V2版本对V1版本进行了优化，将日志项（record item）中一些通用的字段抽出来，放在了消息集合中；</p><p>V2版本对V1版本还有一个关于压缩方面的优化</p><h4 id="怎么压缩-1"><a href="#怎么压缩-1" class="headerlink" title="怎么压缩"></a>怎么压缩</h4><p>V2版本对V1版本还有一个关于压缩方面的优化，就是V1版本的压缩方法是把多条消息进行压缩，然后将压缩后的内容放在外层消息的消息体字段中；V2版本则是对整个消息集合进行压缩，显然V2版本的压缩效率应该更高；</p><h4 id="何时压缩"><a href="#何时压缩" class="headerlink" title="何时压缩"></a>何时压缩</h4><p>在kafka中，压缩可能发生在：生产者端和Broker端</p><ul><li><p>生产者【一般都是生产者端做压缩】</p><ul><li>在生产者程序中添加一个配置：compression.type 参数； compression.type&#x3D;gzip表示开启gzip压缩</li></ul></li><li><p>Broker端</p><ul><li>一般Broker端不会对生产者发出来的消息进行修改；有两个例外情况，会让Broker对消息重新压缩<ul><li>Broker端和生产者端指定的消息压缩算法不一致（不一致的时候，broker端会对producer发出来的消息解压然后重新压缩）</li><li>Broker端发生了消息格式转换：kafka新老版本兼容的问题</li></ul></li></ul></li></ul><h4 id="何时解压缩"><a href="#何时解压缩" class="headerlink" title="何时解压缩"></a>何时解压缩</h4><ul><li>consumer端消费到消息的时候，进行解压缩<ul><li>解压缩的时候，压缩算法是在消息中，用一个字段标识的，所以consumer可以拿到消息之后在解压缩</li></ul></li><li>broker端收到producer发出的消息之后，也会解压缩一次，进行消息的校验；</li></ul><h4 id="压缩算法的选择"><a href="#压缩算法的选择" class="headerlink" title="压缩算法的选择"></a>压缩算法的选择</h4><p>一般看两个指标：压缩比 和 压缩&#x2F;解压缩的吞吐量</p><p>GZIP</p><p>Snappy</p><p>LZ4</p><p>zstd</p><h2 id="怎么保证消息不丢失"><a href="#怎么保证消息不丢失" class="headerlink" title="怎么保证消息不丢失"></a>怎么保证消息不丢失</h2><p>kafka只对 已提交（commited message） 的消息做有限度的持久化保证；</p><img src="kafka从入门到入土.assets/image-20220827223422569.png" alt="image-20220827223422569" style="zoom:50%;" /><h4 id="什么是消息丢失"><a href="#什么是消息丢失" class="headerlink" title="什么是消息丢失"></a>什么是消息丢失</h4><p>对于生产者来说：消息发不出去，就是丢失；</p><p>对于消费者来说：消息消费不到，就是丢失；</p><p>对于broker来说：不存在丢失，broker会对 已提交成功 的消息，做有限度的持久化；</p><h4 id="什么时候会消息丢失"><a href="#什么时候会消息丢失" class="headerlink" title="什么时候会消息丢失"></a>什么时候会消息丢失</h4><ul><li><p>生产者程序丢失消息</p><ul><li>producer.send(msg)因为是异步，<code>fire and forget</code> 所以可能会丢消息<ul><li>网络抖动、消息不合法被broker拒收（消息体太大）等都会导致消息发送不成功</li></ul></li><li>解决：不要使用producer.send(msg) ，使用producer.send(msg,callback)；使用带回调的方式</li></ul></li><li><p>消费者程序丢失消息</p><ul><li>消费的消息不存在了。一般只有先提交offset在消费的场景下会发生；</li><li>多线程处理消息的时候，某一个线程消费失败了，但是offset自动提交了；</li></ul></li></ul><h4 id="怎么保证消息不丢失-1"><a href="#怎么保证消息不丢失-1" class="headerlink" title="怎么保证消息不丢失"></a>怎么保证消息不丢失</h4><p>上面几种丢失消息的场景，怎么避免？</p><ul><li>【生产者端】：用 producer.send(msg, callback)发消息</li><li>【broker端】：设置 acks &#x3D; all</li><li>【生产者端】：设置 retries 为一个较大的值，producer发送失败后重试的次数</li><li>【broker端】：设置 unclean.leader.election.enable &#x3D; false</li><li>【broker端】：设置 replication.factor &gt;&#x3D; 3，设置分区的副本数</li><li>【broker端】：设置 min.insync.replicas &gt; 1。表示消息至少要被写入到多少个副本才算是“已提交”</li><li>【broker端】：确保 replication.factor &gt; min.insync.replicas；推荐replication.factor &#x3D; min.insync.replicas + 1。</li><li>【消费者端】：设置enable.auto.commit&#x3D; false，采用手动提交位移的方式</li></ul><p>acks  和  replication.refactor  和  min.insync.replicas 区别： </p><p>replication.refactor是某个分区的副本replica总数；</p><p>min.insync.replicas是要求确保至少有多少个replica副本写入后才算是提交成功，这个参数是个硬指标；</p><p>acks&#x3D;all是个动态指标，确保当前能正常工作的replica副本都写入后才算是提交成功。</p><p>举个例子：比如，此时副本数3，设置min.insync.replicas&#x3D;2，acks&#x3D;all，那如果所有副本都正常工作，消息要都写入3个副本，才算提交成功，此时这个min.insync.replicas&#x3D;2下限值不起作用。如果其中一个副本因为某些原因挂了，此时acks&#x3D;all的动态约束就是写入2个副本即可，触达了min.insync.replicas&#x3D;2这个下限约束。如果三个副本挂了两个，此时ack&#x3D;all的约束就变成了1个副本，但是因为有min.insync.replicas&#x3D;2这个下限约束，写入就会不成功。</p><h2 id="kafka的拦截器"><a href="#kafka的拦截器" class="headerlink" title="kafka的拦截器"></a>kafka的拦截器</h2><img src="kafka从入门到入土.assets/image-20220827224439813.png" alt="image-20220827224439813" style="zoom:50%;" /><h4 id="生产者拦截器"><a href="#生产者拦截器" class="headerlink" title="生产者拦截器"></a>生产者拦截器</h4><p>开发：实现<code>org.apache.kafka.clients.producer.ProducerInterceptor</code>这个接口，这个接口有俩方法</p><ul><li>onSend：消息真正发给broker之前</li><li>onAcknowledgement：消息提交成功之后，在send(msg,callback) callback之前；</li></ul><h4 id="消费者拦截器"><a href="#消费者拦截器" class="headerlink" title="消费者拦截器"></a>消费者拦截器</h4><p>开发：实现<code>org.apache.kafka.clients.consumer.ConsumerInterceptor</code>这个接口，这个接口有俩方法</p><ul><li>onConsume：在消费者真正处理消息之前；</li><li>onCommit：消费者处理完消息，提交offset之后；</li></ul><h2 id="Java生产者是如何管理TCP连接的"><a href="#Java生产者是如何管理TCP连接的" class="headerlink" title="Java生产者是如何管理TCP连接的"></a>Java生产者是如何管理TCP连接的</h2><img src="kafka从入门到入土.assets/image-20220827230759876.png" alt="image-20220827230759876" style="zoom:50%;" /><h3 id="为什么采用TCP作为底层传输协议"><a href="#为什么采用TCP作为底层传输协议" class="headerlink" title="为什么采用TCP作为底层传输协议"></a>为什么采用TCP作为底层传输协议</h3><p>TCP拥有一些高级功能，如多路复用请求和同时轮询多个连接的能力。</p><p>多路复用请求：multiplexing request，是将两个或多个数据合并到底层—物理连接中的过程。TCP的多路复用请求会在一条物理连接上创建若干个虚拟连接，每个虚拟连接负责流转各自对应的数据流。严格讲：TCP并不能多路复用，只是提供可靠的消息交付语义保证，如自动重传丢失的报文。</p><h3 id="生产者是什么时候创建TCP连接的"><a href="#生产者是什么时候创建TCP连接的" class="headerlink" title="生产者是什么时候创建TCP连接的"></a>生产者是什么时候创建TCP连接的</h3><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token class-name">Properties</span> props <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span>“参数<span class="token number">1</span>”<span class="token punctuation">,</span> “参数<span class="token number">1</span>的值”<span class="token punctuation">)</span>；props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span>“参数<span class="token number">2</span>”<span class="token punctuation">,</span> “参数<span class="token number">2</span>的值”<span class="token punctuation">)</span>；……<span class="token keyword">try</span> <span class="token punctuation">(</span><span class="token class-name">Producer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">></span></span> producer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">KafkaProducer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token punctuation">></span></span><span class="token punctuation">(</span>props<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>            producer<span class="token punctuation">.</span><span class="token function">send</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ProducerRecord</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">></span></span><span class="token punctuation">(</span>……<span class="token punctuation">)</span><span class="token punctuation">,</span> callback<span class="token punctuation">)</span><span class="token punctuation">;</span>  ……<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>针对上面的代码，能创建TCP连接的只有两个地方，一是producer实例化的时候；一是produce.send的时候；</p><ul><li><strong>kafka是在producer实例化的时候与broker建立的TCP连接</strong></li><li>所以，当producer.send的时候，其实已经有TCP连接了</li></ul><p>扩展：除了在producer实例化的时候与broker建立的TCP连接之外，还有没有其他情况？</p><ul><li>有，有两个情况，也会创建TCP连接</li><li><strong>元数据更新时，会与元数据中没有连接的broker建立TCP连接；</strong><ul><li>每隔5分钟，producer会定期从broker中获取元数据信息</li><li>producer尝试给一个不存在的topic发送消息时，broker会说这个topic不存在，然后producer会请求broker更新元数据信息</li></ul></li><li><strong>在消息发送时，如果producer发现与要发送消息的topic所在的broker没有TCP连接，就会创建连接；</strong></li></ul><h3 id="是怎么创建TCP连接的"><a href="#是怎么创建TCP连接的" class="headerlink" title="是怎么创建TCP连接的"></a>是怎么创建TCP连接的</h3><p>在创建 KafkaProducer 实例时，生产者应用会在后台创建并启动一个名为 Sender 的线程，该 Sender 线程开始运行时首先会创建与 Broker 的TCP连接。</p><p>broker有1000个，bootstrap.servers 要配置1000个嘛？ 不需要，因为 Producer 一旦连接到集群中的任一台 Broker，就能拿到整个集群的 Broker 信息。</p><h3 id="TCP连接是什么时候被关闭的"><a href="#TCP连接是什么时候被关闭的" class="headerlink" title="TCP连接是什么时候被关闭的"></a>TCP连接是什么时候被关闭的</h3><ul><li>用户主动关闭，调用producer.close</li><li>Kafka自动关闭（虽然是producer端设置的参数，但实际上，是broker关闭的TCP连接）：与 Producer 端参数 connections.max.idle.ms 的值有关。默认情况下该参数值是 9 分钟，即如果在 9 分钟内没有任何请求“流过”某个 TCP 连接，那么 Kafka 会主动帮你把该 TCP 连接关闭。用户可以在 Producer 端设置 connections.max.idle.ms&#x3D;-1 禁掉这种机制。一旦被设置成 -1，TCP 连接将成为永久长连接。</li></ul><h3 id="会存在的一些问题"><a href="#会存在的一些问题" class="headerlink" title="会存在的一些问题"></a>会存在的一些问题</h3><ul><li>producer每5分钟获取一些元数据，然后与元数据中没有连接的broker建立TCP连接，然后9分钟后，broker会中断空闲的连接，然后5分钟后，在此建立连接；9分钟后，再次中断连接；</li></ul><h2 id="幂等生产者和事务生产者"><a href="#幂等生产者和事务生产者" class="headerlink" title="幂等生产者和事务生产者"></a>幂等生产者和事务生产者</h2><img src="kafka从入门到入土.assets/image-20220828000213732.png" alt="image-20220828000213732" style="zoom:50%;" /><h3 id="kafka的消息发送可靠性"><a href="#kafka的消息发送可靠性" class="headerlink" title="kafka的消息发送可靠性"></a>kafka的消息发送可靠性</h3><ul><li><p>最多一次：消息只会被发送一次，可能会丢失，绝不会重复</p></li><li><p>至少一次（默认）：发送消息的时候，至少要有一次broker明确告知已经提交的callback，消息可能重复，但不会丢失</p></li><li><p>精确一次：消息不会丢失，也不会重复</p></li></ul><h4 id="幂等的概念和事务的概念"><a href="#幂等的概念和事务的概念" class="headerlink" title="幂等的概念和事务的概念"></a>幂等的概念和事务的概念</h4><p>不说了</p><h4 id="幂等生产者"><a href="#幂等生产者" class="headerlink" title="幂等生产者"></a>幂等生产者</h4><p>能够保证producer在一个会话中，一个分区的消息不会重复，但是当producer重启后，或者producer在多分区的场景下不能保证消息的幂等性；</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span>“enable<span class="token punctuation">.</span>idempotence”<span class="token punctuation">,</span> ture<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="幂等生产者的实现原理"><a href="#幂等生产者的实现原理" class="headerlink" title="幂等生产者的实现原理"></a>幂等生产者的实现原理</h4><p><a href="https://www.jianshu.com/p/f77ade3f41fd">https://www.jianshu.com/p/f77ade3f41fd</a></p><h4 id="事务生产者"><a href="#事务生产者" class="headerlink" title="事务生产者"></a>事务生产者</h4><p>如果我想实现多分区以及多会话上的消息无重复，应该怎么做呢？答案就是事务（transaction）或者依赖事务型 Producer。这也是幂等性 Producer 和事务型 Producer 的最大区别！</p><p><strong>设置事务型 Producer</strong> ：</p><ul><li><p>和幂等性 Producer 一样，开启 enable.idempotence &#x3D; true。</p></li><li><p>设置 Producer 端参数 transactional. id。最好为其设置一个有意义的名字。</p></li><li><p>&#96;&#96;&#96;java<br>&#x2F;&#x2F;调整代码<br>producer.initTransactions();<br>try {<br>        producer.beginTransaction();&#x2F;&#x2F;开启事务<br>        producer.send(record1);&#x2F;&#x2F;发送消息1<br>        producer.send(record2);&#x2F;&#x2F;发送消息2<br>        producer.commitTransaction();&#x2F;&#x2F;提交事务<br>} catch (KafkaException e) {<br>        producer.abortTransaction();<br>}</p><pre class="line-numbers language-none"><code class="language-none">- 消息1和消息2，要么全部成功，要么全部失败**设置事务型Consumer：**- 为什么要设置，因为事务型生产者即使发送失败了，也会写到kakfa日志中，被消费到；- 设置consumer的 &#96;isolation.level&#96;参数  - read_uncommitted：读未提交，可以消费到发送失败的消息  - read_committed：读已提交，只能消费到发送成功的消息#### 事务生产者的实现原理https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;f77ade3f41fd## 消费者组和消费位移主题&lt;img src&#x3D;&quot;kafka从入门到入土.assets&#x2F;image-20220828151124945.png&quot; alt&#x3D;&quot;image-20220828151124945&quot; style&#x3D;&quot;zoom:50%;&quot; &#x2F;&gt;### 什么是消费者组Consumer Group 是 Kafka 提供的可扩展且具有容错性的消费者机制。组内有多个消费者或消费者实例（Consumer Instance），它们共享一个公共的 Group ID。组内的所有消费者一起消费订阅的主题（Subscribed Topics）的所有分区（Partition）。当然，每个分区只能由同一个消费者组内的一个 Consumer 实例来消费。### 消费者组的特性是什么- Consumer Group 下可以有一个或多个 Consumer 实例。这里的实例可以是一个单独的进程，也可以是同一进程下的线程。在实际场景中，使用进程更为常见一些。- Group ID 是一个字符串，在一个 Kafka 集群中，它标识唯一的一个 Consumer Group。- Consumer Group 下所有实例订阅的主题的单个分区，只能分配给组内的某个 Consumer 实例消费。这个分区当然也可以被其他的 Group 消费。### 怎么创建一个消费者组？### 消费者组怎么用？### 传统的消息引擎模型- 点对点：一个消息只能被一个消费者消费到- 发布订阅：一个topic下的消息，可以被订阅该topic的所有消费者都消费到- kafka使用消费者组，实现了两种消息引擎模型；   - 如果所有的消费者属于一个消费者组，那就是点对点  - 如果所有的消费者属于不同的消费者组，那就是发布订阅### 消费者组是如何维护offset的对于一个单独的消费者来说，offset就是一个数值；但是对于一个消费者组来说，因为组内有多个消费者，那么消费者组维护offset是通过一个map来维护的，这个map简单的可以理解为是：Map&lt;TopicPartition,Long&gt;对于老版本的kafka来说，offset是保存在zk中的，但是后来kafka的开发者发现，offset的更新太过于频繁，频繁的封信会拖慢zk的性能，所以在新版本的kafka中，offset是保存在broker内部的一个特殊的topic中的(__consumer_offset)。下面我们来看看位移主题### __consumer_offset位移主题&lt;img src&#x3D;&quot;kafka从入门到入土.assets&#x2F;image-20220828155528590.png&quot; alt&#x3D;&quot;image-20220828155528590&quot; style&#x3D;&quot;zoom:50%;&quot; &#x2F;&gt;__consumer_offsets 在 Kafka 源码中有个更为正式的名字，叫位移主题，即 Offsets Topic。#### 为什么会有__consumer_offset对于老版本的kafka来说，offset是保存在zk中的，但是后来kafka的开发者发现，offset的更新太过于频繁，频繁的封信会拖慢zk的性能，所以在新版本的kafka中，offset是保存在broker内部的一个特殊的topic中的(__consumer_offset)。#### __consumer_offset是什么是kafka中一个内部topic，这个topic的主要作用是用来管理offset，offset管理机制其实很简单，就是将 Consumer 的位移数据作为一条条普通的 Kafka 消息，提交到 __consumer_offsets 中。可以这么说，consumer_offsets 的主要作用是保存 Kafka 消费者的位移信息。#### __consumer_offset怎么被创建的当 Kafka 集群中的第一个 Consumer 程序启动时，Kafka 会自动创建位移主题。我们知道__consumer_offset虽然是内部主题，但是它仍然是一个主题，既然是主题，那么它的分区数和副本是多少呢？- 分区数：50；由Broker 端参数 offsets.topic.num.partitions指定- 副本数：3；由Broker 端参数 offsets.topic.replication.factor指定#### __consumer_offset中存了什么数据- 位移的管理消息- 位移的信息消息- 墓碑消息这个主题存的到底是什么格式的消息呢？所谓的消息格式，你可以简单地理解为是一个 KV 对K：consumer group Id + topic + 分区号V：offset即使是单个消费者，也是会有groupid的除了普通的offset的消息之外，还保存了另外两类消息- consumer group的信息的消息（一般不用关注）- 用户删除过期offset消息的墓碑消息（tombstone）#### 什么时候提交(怎么提交offset)&lt;img src&#x3D;&quot;kafka从入门到入土.assets&#x2F;image-20220828174212922.png&quot; alt&#x3D;&quot;image-20220828174212922&quot; style&#x3D;&quot;zoom:50%;&quot; &#x2F;&gt;Kafka Consumer 提交位移时会写入该主题，那 Consumer 是怎么提交位移的呢？目前 Kafka Consumer 提交位移的方式有两种：自动提交位移和手动提交位移。##### 自动提交位移- consumer端参数：enable.auto.commit- consumer端参数：auto.commit.interval.ms&#96;&#96;&#96;javaProperties props &#x3D; new Properties();     props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);     props.put(&quot;group.id&quot;, &quot;test&quot;);     props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);     props.put(&quot;auto.commit.interval.ms&quot;, &quot;2000&quot;);     props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);     props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);     KafkaConsumer&lt;String, String&gt; consumer &#x3D; new KafkaConsumer&lt;&gt;(props);     consumer.subscribe(Arrays.asList(&quot;foo&quot;, &quot;bar&quot;));     while (true) &#123;         ConsumerRecords&lt;String, String&gt; records &#x3D; consumer.poll(100);         for (ConsumerRecord&lt;String, String&gt; record : records)             System.out.printf(&quot;offset &#x3D; %d, key &#x3D; %s, value &#x3D; %s%n&quot;, record.offset(), record.key(), record.value());     &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h5 id="手动提交位移"><a href="#手动提交位移" class="headerlink" title="手动提交位移"></a>手动提交位移</h5><ul><li>consumer端参数：enable.auto.commit</li></ul><h6 id="同步提交方式"><a href="#同步提交方式" class="headerlink" title="同步提交方式"></a>同步提交方式</h6><ul><li>KafkaConsumer#commitSync()</li></ul><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>            <span class="token class-name">ConsumerRecords</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">></span></span> records <span class="token operator">=</span>                        consumer<span class="token punctuation">.</span><span class="token function">poll</span><span class="token punctuation">(</span><span class="token class-name">Duration</span><span class="token punctuation">.</span><span class="token function">ofSeconds</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">process</span><span class="token punctuation">(</span>records<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// 处理消息</span>            <span class="token keyword">try</span> <span class="token punctuation">&#123;</span>                        consumer<span class="token punctuation">.</span><span class="token function">commitSync</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">&#125;</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">CommitFailedException</span> e<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>                        <span class="token function">handle</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// 处理提交失败异常</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h6 id="异步提交方式"><a href="#异步提交方式" class="headerlink" title="异步提交方式"></a>异步提交方式</h6><ul><li>KafkaConsumer#commitAsync()</li></ul><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>            <span class="token class-name">ConsumerRecords</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">></span></span> records <span class="token operator">=</span>   consumer<span class="token punctuation">.</span><span class="token function">poll</span><span class="token punctuation">(</span><span class="token class-name">Duration</span><span class="token punctuation">.</span><span class="token function">ofSeconds</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">process</span><span class="token punctuation">(</span>records<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// 处理消息</span>            consumer<span class="token punctuation">.</span><span class="token function">commitAsync</span><span class="token punctuation">(</span><span class="token punctuation">(</span>offsets<span class="token punctuation">,</span> exception<span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">&#123;</span>  <span class="token keyword">if</span> <span class="token punctuation">(</span>exception <span class="token operator">!=</span> <span class="token keyword">null</span><span class="token punctuation">)</span>  <span class="token function">handle</span><span class="token punctuation">(</span>exception<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h6 id="同步提交方式-异步提交方式"><a href="#同步提交方式-异步提交方式" class="headerlink" title="同步提交方式+异步提交方式"></a>同步提交方式+异步提交方式</h6><ul><li>同步会出现的问题：是阻塞的，会降低consumer的TPS；好处是会自动重试，提交不成功的话，不会拉取新的消息；</li><li>异步会出现的问题：提交异常的话，不会重试；会导致消息重复消费</li><li>怎么办呢？结合两者，先使用异步提交一次，如果失败了，finally里使用同步方式</li></ul><pre class="line-numbers language-java" data-language="java"><code class="language-java">   <span class="token keyword">try</span> <span class="token punctuation">&#123;</span>           <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>                        <span class="token class-name">ConsumerRecords</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">></span></span> records <span class="token operator">=</span>                                     consumer<span class="token punctuation">.</span><span class="token function">poll</span><span class="token punctuation">(</span><span class="token class-name">Duration</span><span class="token punctuation">.</span><span class="token function">ofSeconds</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                        <span class="token function">process</span><span class="token punctuation">(</span>records<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// 处理消息</span>                        <span class="token function">commitAysnc</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// 使用异步提交规避阻塞</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span> <span class="token keyword">catch</span><span class="token punctuation">(</span><span class="token class-name">Exception</span> e<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>            <span class="token function">handle</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// 处理异常</span><span class="token punctuation">&#125;</span> <span class="token keyword">finally</span> <span class="token punctuation">&#123;</span>            <span class="token keyword">try</span> <span class="token punctuation">&#123;</span>                        consumer<span class="token punctuation">.</span><span class="token function">commitSync</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// 最后一次提交使用同步阻塞式提交</span>  <span class="token punctuation">&#125;</span> <span class="token keyword">finally</span> <span class="token punctuation">&#123;</span>       consumer<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果你选择的是自动提交位移，那么就可能存在一个问题：只要 Consumer 一直启动着，它就会无限期地向位移主题写入消息。导致磁盘爆满；因为自动提交位移是后台定时提交的（auto.commit.interval.ms默认是5s）；</p><p>那么满了怎么办？满了就删除，怎么删除呢？</p><h4 id="consumer-offset中的数据删除"><a href="#consumer-offset中的数据删除" class="headerlink" title="__consumer_offset中的数据删除"></a>__consumer_offset中的数据删除</h4><p>Kafka 是怎么删除位移主题中的过期消息的呢？答案就是 Compaction。国内很多文献都将其翻译成压缩，我个人是有一点保留意见的。在英语中，压缩的专有术语是 Compression，它的原理和 Compaction 很不相同，我更倾向于翻译成压实，或干脆采用 JVM 垃圾回收中的术语：整理。</p><img src="kafka从入门到入土.assets/image-20220828155424662.png" alt="image-20220828155424662" style="zoom:80%;" /><p>图中位移为 0、2 和 3 的消息的 Key 都是 K1。Compact 之后，分区只需要保存位移为 3 的消息，因为它是最新发送的。</p><p>Kafka 提供了专门的后台线程定期地巡检待 Compact 的主题，看看是否存在满足条件的可删除数据。这个后台线程叫 Log Cleaner。很多实际生产环境中都出现过位移主题无限膨胀占用过多磁盘空间的问题，如果你的环境中也有这个问题，我建议你去检查一下 Log Cleaner 线程的状态，通常都是这个线程挂掉了导致的。</p><h4 id="consumer-API位移提交失败怎么办"><a href="#consumer-API位移提交失败怎么办" class="headerlink" title="consumer API位移提交失败怎么办"></a>consumer API位移提交失败怎么办</h4><img src="kafka从入门到入土.assets/image-20220828182330032.png" alt="image-20220828182330032" style="zoom:50%;" /><p>一般的失败，API（同步的）会自动重试；但是有一个异常叫做CommitFailedException，这个异常抛出，说明位移的提交出现了大问题，需要人工介入了</p><ul><li>当消息处理的总时间超过预设的 max.poll.interval.ms 参数值时，Kafka Consumer 端会抛出 CommitFailedException 异常。</li><li>应用中同时出现了设置相同 group.id 值的<strong>消费者组</strong>和<strong>独立消费者</strong>，那么当独立消费者程序手动提交位移时，Kafka 就会立即抛出 CommitFailedException 异常</li></ul><h2 id="重平衡（协调者coordinator）"><a href="#重平衡（协调者coordinator）" class="headerlink" title="重平衡（协调者coordinator）"></a>重平衡（协调者coordinator）</h2><img src="kafka从入门到入土.assets/image-20220828162925680.png" alt="image-20220828162925680" style="zoom:50%;" /><h3 id="什么是重平衡"><a href="#什么是重平衡" class="headerlink" title="什么是重平衡"></a>什么是重平衡</h3><p>Rebalance 本质上是一种协议，规定了一个 Consumer Group 下的所有 Consumer 如何达成一致，来分配订阅 Topic 的每个分区。比如某个 Group 下有 20 个 Consumer 实例，它订阅了一个具有 100 个分区的 Topic。正常情况下，Kafka 平均会为每个 Consumer 分配 5 个分区。这个分配的过程就叫 Rebalance。</p><h3 id="什么时候会重平衡"><a href="#什么时候会重平衡" class="headerlink" title="什么时候会重平衡"></a>什么时候会重平衡</h3><ul><li>消费者组内消费者数量变化（新增或减少）；</li><li>消费者组订阅的topic数变化；</li><li>订阅的主题的分区数变化</li></ul><h3 id="怎么保证重平衡后的公平"><a href="#怎么保证重平衡后的公平" class="headerlink" title="怎么保证重平衡后的公平"></a>怎么保证重平衡后的公平</h3><ul><li>举例：比如组内有2个消费者，这个组消费topicA和topicB,其中消费者1消费topicA，消费者2消费topicB，当新加入一个topicC的时候，会不会消费者1消费到topicB，消费者2消费到topicA</li></ul><p>kafka有三种策略保证重平衡后的公平</p><ul><li><p>1、Range分配策略是面向每个主题的，首先会对同一个主题里面的分区按照序号进行排序，并把消费者线程按照字母顺序进行排序。然后用分区数除以消费者线程数量来判断每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。 </p></li><li><p>2、RoundRobin策略的原理是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序，然后通过轮询算法逐个将分区以此分配给每个消费者。 使用RoundRobin分配策略时会出现两种情况： </p><ul><li><p>①、如果同一消费组内，所有的消费者订阅的消息都是相同的，那么 RoundRobin 策略的分区分配会是均匀的。</p></li><li><p>②、如果同一消费者组内，所订阅的消息是不相同的，那么在执行分区分配的时候，就不是完全的轮询分配，有可能会导致分区分配的不均匀。如果某个消费者没有订阅消费组内的某个 topic，那么在分配分区的时候，此消费者将不会分配到这个 topic 的任何分区。</p></li></ul></li><li><p>3、Sticky分配策略，这种分配策略是在kafka的0.11.X版本才开始引入的，是目前最复杂也是最优秀的分配策略。 Sticky分配策略的原理比较复杂，它的设计主要实现了两个目的： </p><ul><li><p>①、分区的分配要尽可能的均匀；</p></li><li><p>②、分区的分配尽可能的与上次分配的保持相同。 如果这两个目的发生了冲突，优先实现第一个目的。</p></li></ul></li></ul><h3 id="重平衡的缺点"><a href="#重平衡的缺点" class="headerlink" title="重平衡的缺点"></a>重平衡的缺点</h3><ul><li>会STW（stop the world）：消费者会全部停止消费</li><li>时间太慢了，几百个消费者重平衡一次，要几个小时</li><li>Rebalance 的设计是要求所有consumer实例共同参与，全部重新分配所有用分区。</li><li>在 Rebalance 过程中，所有 Consumer 实例共同参与，在协调者组件的帮助下，完成订阅主题分区的分配。但是，在整个过程中，所有实例都不能消费任何消息，因此它对 Consumer 的 TPS 影响很大。</li></ul><h3 id="什么是协调者coordinator"><a href="#什么是协调者coordinator" class="headerlink" title="什么是协调者coordinator"></a>什么是协调者coordinator</h3><ul><li><p>协调管理消费者组的一个程序，运行在broker上的</p></li><li><p>每一个broker在启动时都会启动coordinator组件（coordinator程序）</p></li></ul><p>所谓协调者，在 Kafka 中对应的术语是 Coordinator，它专门为 Consumer Group 服务，负责为 Group 执行 Rebalance 以及提供位移管理和组成员管理等。</p><p>具体来讲，Consumer 端应用程序在提交位移时，其实是向 Coordinator 所在的 Broker 提交位移。同样地，当 Consumer 应用启动时，也是向 Coordinator 所在的 Broker 发送各种请求，然后由 Coordinator 负责执行消费者组的注册、成员管理记录等元数据管理操作。</p><h4 id="消费者组是怎么找到自己的coordinator的？"><a href="#消费者组是怎么找到自己的coordinator的？" class="headerlink" title="消费者组是怎么找到自己的coordinator的？"></a>消费者组是怎么找到自己的coordinator的？</h4><p>既然coordinator是运行在broker上的一个程序，那么一个消费者组，是怎么找到自己的coordinator的呢？</p><p>怎么找？是通过之前说过的__consumer_offset这个主题来找的</p><p>分为两步</p><ul><li>获取当前消费者组所在的groupid，然后hash得到hash值；</li><li>获取__consumer_offset的分区数，默认是50</li><li>计算 ：abs ( hash % 50 ) &#x3D; 分区号</li><li>然后，找到这个分区号的leader副本所在的broker；这个broker就是这个消费者的coordinator</li></ul><h3 id="消费者组重平衡能避免嘛"><a href="#消费者组重平衡能避免嘛" class="headerlink" title="消费者组重平衡能避免嘛"></a>消费者组重平衡能避免嘛</h3><p>首先，明确一个概念，目前Rebalance的弊端（慢，STW）这2个弊端，社区是没有办法解决的；</p><p>针对Rebalance的效率低的情况，社区采用了StickyAssignor的分区分配策略来提升性能；</p><p>既然无法解决，那我们只能尽量避免，怎么避免呢？就要从导致Rebalance发生的三种情况来看</p><ul><li>组成员数量发生变化（99%的Rebalance都是这个原因）</li><li>订阅主题数量发生变化（一般是程序开发者主动操作，无法避免）</li><li>订阅主题的分区数发生变化（一般是程序开发者主动操作，无法避免）</li></ul><p>组成员数量发生变化，变化分为两种，一种是增加，一种是减少</p><ul><li>增加：一般都是程序开发者主动操作，比如为了提升topic的消费速率</li><li>减少：如果是主动停掉的，那自不必说，关键是在某些情况下，Consumer 实例会被 Coordinator 错误地认为“已停止”从而被“踢出”Group。如果是这个原因导致的 Rebalance，我们就不能不管了。</li></ul><h4 id="什么时候coordinator会认为consumer实例已经挂掉了"><a href="#什么时候coordinator会认为consumer实例已经挂掉了" class="headerlink" title="什么时候coordinator会认为consumer实例已经挂掉了"></a>什么时候coordinator会认为consumer实例已经挂掉了</h4><ul><li>coordinator没有收到consumer的心跳，就会让consumer离组，重新Rebalance<ul><li>Consumer 端有个参数，叫 session.timeout.ms，默认10秒；coordinator在10s内没有收到consumer的心跳，就Rebalance</li><li>心跳是consumer主动给coordinator的，那么多久一次呢？<ul><li>是由参数：heartbeat.interval.ms控制的；</li></ul></li><li>说明：session.timeout.ms&#x3D;6s，heartbeat.interval.ms&#x3D;2s：要保证 Consumer 实例在被判定为“dead”之前，能够发送至少 3 轮的心跳请求，即 session.timeout.ms &gt;&#x3D; 3 * heartbeat.interval.ms。</li></ul></li><li>consumer实例在一定时间内无法消费完拉取到数据，就会主动离组，重新Rebalance<ul><li>Consumer 端有个参数，max.poll.interval.ms 参数，默认五分钟</li><li>一个consumer在5分钟内，没有消费完拉取的数据，就Rebalance</li></ul></li><li>consumer端的GC情况</li></ul><p>standalone consumer就没有rebalance一说了。 它的特点主要是灵活。</p><p>虽然社区一直在改进rebalance的性能，但大数据量下consumer group机制依然有很多弊病（比如rebalance太慢等）</p><p>所以很多大数据框架(Spark &#x2F;Flink)的kafka connector并不使用group机制，而是使用standalone consumer</p><h3 id="怎么排查生产是否Rebalance过多"><a href="#怎么排查生产是否Rebalance过多" class="headerlink" title="怎么排查生产是否Rebalance过多"></a>怎么排查生产是否Rebalance过多</h3><p>去找Coordinator所在的broker日志，如果经常发生rebalance，会有类似于”(Re)join group” 之类的日志</p><h2 id="Java消费者是如何管理TCP连接的"><a href="#Java消费者是如何管理TCP连接的" class="headerlink" title="Java消费者是如何管理TCP连接的"></a>Java消费者是如何管理TCP连接的</h2><img src="kafka从入门到入土.assets/image-20220828211956299.png" alt="image-20220828211956299" style="zoom:50%;" /><h3 id="消费者是什么时候创建TCP连接的"><a href="#消费者是什么时候创建TCP连接的" class="headerlink" title="消费者是什么时候创建TCP连接的"></a>消费者是什么时候创建TCP连接的</h3><ul><li>生产者是在new KakfaProducer的时候，后台开启一个Sender的线程用来创建TCP连接的；</li><li>消费者<strong>不是</strong>在实例化的时候创建的，而是在开始消费消息的时候（consumer.poll）才主动创建TCP连接，准确的说有三个时机<ul><li>发起FindCoordinator请求时（连接的brokerId是-1，因为不知道连哪一个）</li><li>连接协调者的时候（连接的brokerId是Interger.Max-协调者所在broker的Id号，为什么这么设计，为了防止连接重用）</li><li>真正消费消息的时候（连接某个topic的某个分区的leader副本所在的broker）</li></ul></li></ul><h3 id="创建多少个TCP连接"><a href="#创建多少个TCP连接" class="headerlink" title="创建多少个TCP连接"></a>创建多少个TCP连接</h3><p>会创建三类TCP连接</p><ul><li>FindCoordinator请求与任意一个broker的TCP连接</li><li>与Coordinator的连接，此时消费者才能真正的开始工作</li><li>与分区所在leader副本的TCP连接，拉取消息，真正开始处理</li></ul><p>其中第一类（FIndCoordinator请求与任意一个Broker的连接）会在消费者真正开始处理消息的时候，也就是后面两类TCP连接建立好之后，第一类连接会被关闭掉；</p><h3 id="消费者是什么时候关闭TCP连接的"><a href="#消费者是什么时候关闭TCP连接的" class="headerlink" title="消费者是什么时候关闭TCP连接的"></a>消费者是什么时候关闭TCP连接的</h3><p>上面说的三类连接，其中第一类连接会在二，三类连接创建好之后，被关闭掉；</p><p>二，三类连接的关闭场景有两种：</p><ul><li>主动关闭，这个不说了</li><li>kafka自动关闭，由 消费者端参数connection.max.idle.ms控制。当超过指定时间，该消费者没有消息消费时，就会被关闭连接（但是如果我们的消费逻辑是while循环的情况，则永远不会被关闭，因为一直与broker保持通信，实现了“长链接”的效果）</li></ul><h3 id="可能存在的问题"><a href="#可能存在的问题" class="headerlink" title="可能存在的问题"></a>可能存在的问题</h3><p>第一类 TCP 连接仅仅是为了首次获取元数据而创建的，后面就会被废弃掉。最根本的原因是，消费者在启动时还不知道 Kafka 集群的信息，只能使用“-1” 去注册，即使消费者获取了真实的 Broker ID，它依旧无法区分这个“-1”对应的是哪台 Broker，因此也就无法重用这个 Socket 连接，只能再重新创建一个新的连接。</p><p>为什么会出现这种情况呢？主要是因为目前 Kafka 仅仅使用 ID 这一个维度的数据来表征 Socket 连接信息。这点信息明显不足以确定连接的是哪台 Broker，也许在未来，社区应该考虑使用 &lt; 主机名、端口、ID&gt; 三元组的方式来定位 Socket 资源，这样或许能够让消费者程序少创建一些 TCP 连接。</p><p>也许你会问，反正 Kafka 有定时关闭机制，这算多大点事呢？其实，在实际场景中，我见过很多将 connection.max.idle.ms 设置成 -1，即禁用定时关闭的案例，如果是这样的话，这些 TCP 连接将不会被定期清除，只会成为永久的“僵尸”连接。基于这个原因，社区应该考虑更好的解决方案。</p><h2 id="kafka副本机制详解"><a href="#kafka副本机制详解" class="headerlink" title="kafka副本机制详解"></a>kafka副本机制详解</h2><h2 id="请求是怎么被处理的"><a href="#请求是怎么被处理的" class="headerlink" title="请求是怎么被处理的"></a>请求是怎么被处理的</h2><h2 id="消费者组重平衡全流程解析"><a href="#消费者组重平衡全流程解析" class="headerlink" title="消费者组重平衡全流程解析"></a>消费者组重平衡全流程解析</h2><h2 id="你一定不能错过得kafka控制器"><a href="#你一定不能错过得kafka控制器" class="headerlink" title="你一定不能错过得kafka控制器"></a>你一定不能错过得kafka控制器</h2><h2 id="关于高水位和Leader-Epoch的讨论"><a href="#关于高水位和Leader-Epoch的讨论" class="headerlink" title="关于高水位和Leader Epoch的讨论"></a>关于高水位和Leader Epoch的讨论</h2><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>为什么kafka不像mysql那样允许追随者副本（follower replica）对外提供只读服务？</p><p>kafka是怎么做到 提供一套 API 实现生产者和消费者？</p><p>kafka是怎么做到 降低网络传输和磁盘存储开销；</p><p>kafka是怎么做到 实现高伸缩性架构。</p><p>kafka为什么快，为什么高吞吐？</p><ul><li>消息日志（Log）只能追加写，避免了随机IO，改成了顺序IO，大大提高了写能力；</li></ul><p>你觉得 Kafka 未来的演进路线是怎么样的？如果你是 Kafka 社区的“掌舵人”，你准备带领整个社区奔向什么方向呢？</p><p>想你是一家创业公司的架构师，公司最近准备改造现有系统，引入 Kafka 作为消息中间件衔接上下游业务。作为架构师的你会怎么选择合适的 Kafka 发行版呢</p><p>kafka每天 1 亿条 1KB 大小的消息，保存两份且留存两周的时间，需要多大的磁盘空间？</p><p>如果需要kafka1小时内处理1TB的业务数据，在千兆网络下，需要多少台kafka机器？</p><p>kafka怎么实现的故障转移？</p><p>kafka是怎么保障大数据量均匀的分布在各个Broker上的？</p><p>kafka的零拷贝技术是什么？</p><ul><li><a href="https://blog.csdn.net/ljheee/article/details/99652448">https://blog.csdn.net/ljheee/article/details/99652448</a></li><li><a href="https://www.jianshu.com/p/835ec2d4c170">https://www.jianshu.com/p/835ec2d4c170</a></li></ul><p>broker端收到消息也会解压缩，进行消息校验，那么零拷贝还有用嘛？</p><p>consumer可以先提交offset，在处理消息嘛？</p><p>kafka的producer是在producer实例化的时候，创建的TCP连接，那么这个时候，producer都不知道要往那个topic发消息，那么就不知道要连接到哪个broker？kafka是怎么做的呢？</p><p>kafka在建立TCP连接的步骤中，有没有可以优化的地方，目前社区做的不好的地方？</p><p>丰网的kafka的消费者重复注册是怎么做的？是同一个消费者实例的多个线程，还是同一个消费者类，注册了多个bean；</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;kafka从入门到入土&quot;&gt;&lt;a href=&quot;#kafka从入门到入土&quot; class=&quot;headerlink&quot; title=&quot;kafka从入门到入土&quot;&gt;&lt;/a&gt;kafka从入门到入土&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;kafka中的一些术语&quot;&gt;&lt;a href=&quot;#k</summary>
      
    
    
    
    <category term="JAVA" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/"/>
    
    <category term="消息中间件" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    <category term="KAFKA" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/KAFKA/"/>
    
    
    <category term="kafka" scheme="https://zhuansunpengcheng.gitee.io/tags/kafka/"/>
    
    <category term="生产者" scheme="https://zhuansunpengcheng.gitee.io/tags/%E7%94%9F%E4%BA%A7%E8%80%85/"/>
    
    <category term="消费者" scheme="https://zhuansunpengcheng.gitee.io/tags/%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    
    <category term="消息" scheme="https://zhuansunpengcheng.gitee.io/tags/%E6%B6%88%E6%81%AF/"/>
    
    <category term="中间件" scheme="https://zhuansunpengcheng.gitee.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>oracle查询表的字段名类型注释</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/ORACLE/oracle%E6%9F%A5%E8%AF%A2%E8%A1%A8%E7%9A%84%E5%AD%97%E6%AE%B5%E5%90%8D%E7%B1%BB%E5%9E%8B%E6%B3%A8%E9%87%8A/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/ORACLE/oracle%E6%9F%A5%E8%AF%A2%E8%A1%A8%E7%9A%84%E5%AD%97%E6%AE%B5%E5%90%8D%E7%B1%BB%E5%9E%8B%E6%B3%A8%E9%87%8A/</id>
    <published>2022-10-31T10:57:46.464Z</published>
    <updated>2022-10-31T10:57:46.464Z</updated>
    
    <content type="html"><![CDATA[<pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">select</span> a<span class="token punctuation">.</span>COLUMN_ID<span class="token punctuation">,</span> a<span class="token punctuation">.</span>TABLE_NAME<span class="token punctuation">,</span> a<span class="token punctuation">.</span>COLUMN_NAME<span class="token punctuation">,</span> a<span class="token punctuation">.</span>DATA_TYPE<span class="token punctuation">,</span> a<span class="token punctuation">.</span>DATA_LENGTH<span class="token punctuation">,</span> a<span class="token punctuation">.</span>NULLABLE<span class="token punctuation">,</span>b<span class="token punctuation">.</span>COMMENTS <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">SELECT</span> COLUMN_ID<span class="token punctuation">,</span> TABLE_NAME<span class="token punctuation">,</span> COLUMN_NAME<span class="token punctuation">,</span> DATA_TYPE<span class="token punctuation">,</span> DATA_LENGTH<span class="token punctuation">,</span> NULLABLE  <span class="token keyword">FROM</span> ALL_TAB_COLUMNS <span class="token keyword">WHERE</span> TABLE_NAME <span class="token operator">=</span> <span class="token string">'这里改成表名'</span><span class="token punctuation">)</span> a <span class="token keyword">left</span> <span class="token keyword">join</span> <span class="token punctuation">(</span><span class="token keyword">select</span> TABLE_NAME<span class="token punctuation">,</span>COLUMN_NAME<span class="token punctuation">,</span>COMMENTS<span class="token keyword">from</span> user_col_comments<span class="token keyword">where</span> Table_Name<span class="token operator">=</span><span class="token string">'这里改成表名'</span><span class="token punctuation">)</span> b <span class="token keyword">on</span> a<span class="token punctuation">.</span>COLUMN_NAME <span class="token operator">=</span> b<span class="token punctuation">.</span>COLUMN_NAME <span class="token keyword">order</span> <span class="token keyword">by</span> a<span class="token punctuation">.</span>COLUMN_ID <span class="token keyword">asc</span> <span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;pre class=&quot;line-numbers language-sql&quot; data-language=&quot;sql&quot;&gt;&lt;code class=&quot;language-sql&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;select&lt;/span&gt; a&lt;span class</summary>
      
    
    
    
    <category term="JAVA" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/"/>
    
    <category term="数据库" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="ORACLE" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/ORACLE/"/>
    
    
    <category term="oracle" scheme="https://zhuansunpengcheng.gitee.io/tags/oracle/"/>
    
    <category term="表结构" scheme="https://zhuansunpengcheng.gitee.io/tags/%E8%A1%A8%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>oracle创建索引的一些规范</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/ORACLE/oracle%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95%E7%9A%84%E4%B8%80%E4%BA%9B%E8%A7%84%E8%8C%83/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/ORACLE/oracle%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95%E7%9A%84%E4%B8%80%E4%BA%9B%E8%A7%84%E8%8C%83/</id>
    <published>2022-10-31T10:57:46.463Z</published>
    <updated>2022-10-31T10:57:46.463Z</updated>
    
    <content type="html"><![CDATA[<p>1、表的主键、外键必须有索引；</p><p>2、数据量超过300的表应该有索引；</p><p>3、经常与其他表进行连接的表，在连接字段上应该建立索引；</p><p>4、经常出现在Where子句中的字段，特别是大表的字段，应该建立索引；</p><p>5、索引应该建在选择性高的字段上；</p><p>6、索引应该建在小字段上，对于大的文本字段甚至超长字段，不要建索引；</p><p>7、复合索引的建立需要进行仔细分析；尽量考虑用单字段索引代替：</p><p>A、正确选择复合索引中的主列字段，一般是选择性较好的字段；</p><p>B、复合索引的几个字段是否经常同时以AND方式出现在Where子句中？单字段查询是否极少甚至没有？如果是，则可以建立复合索引；否则考虑单字段索引；</p><p>C、如果复合索引中包含的字段经常单独出现在Where子句中，则分解为多个单字段索引；</p><p>D、如果复合索引所包含的字段超过3个，那么仔细考虑其必要性，考虑减少复合的字段；</p><p>E、如果既有单字段索引，又有这几个字段上的复合索引，一般可以删除复合索引；</p><p>8、频繁进行数据操作的表，不要建立太多的索引；</p><p>9、删除无用的索引，避免对执行计划造成负面影响；</p><p>以上是一些普遍的建立索引时的判断依据。一言以蔽之，索引的建立必须慎重，对每个索引的必要性都应该经过仔细分析，要有建立的依据。 因为太多的索引与不充分、不正确的索引对性能都毫无益处：在表上建立的每个索引都会增加存储开销，索引对于插入、删除、更新操作也 会增加处理上的开销。另外，过多的复合索引，在有单字段索引的情况下，一般都是没有存在价值的；相反，还会降低数据增加删除时的性 能，特别是对频繁更新的表来说，负面影响更大</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1、表的主键、外键必须有索引；&lt;/p&gt;
&lt;p&gt;2、数据量超过300的表应该有索引；&lt;/p&gt;
&lt;p&gt;3、经常与其他表进行连接的表，在连接字段上应该建立索引；&lt;/p&gt;
&lt;p&gt;4、经常出现在Where子句中的字段，特别是大表的字段，应该建立索引；&lt;/p&gt;
&lt;p&gt;5、索引应该建在选</summary>
      
    
    
    
    <category term="JAVA" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/"/>
    
    <category term="数据库" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="ORACLE" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/ORACLE/"/>
    
    
    <category term="索引" scheme="https://zhuansunpengcheng.gitee.io/tags/%E7%B4%A2%E5%BC%95/"/>
    
    <category term="oracle" scheme="https://zhuansunpengcheng.gitee.io/tags/oracle/"/>
    
  </entry>
  
  <entry>
    <title>mysql如何给大表加索引</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/mysql%E7%BB%99%E5%A4%A7%E8%A1%A8%E5%8A%A0%E7%B4%A2%E5%BC%95/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/mysql%E7%BB%99%E5%A4%A7%E8%A1%A8%E5%8A%A0%E7%B4%A2%E5%BC%95/</id>
    <published>2022-10-31T10:57:46.462Z</published>
    <updated>2022-10-31T10:57:46.462Z</updated>
    
    <content type="html"><![CDATA[<h1 id="mysql如何给大表加索引"><a href="#mysql如何给大表加索引" class="headerlink" title="mysql如何给大表加索引"></a>mysql如何给大表加索引</h1><p>给大表加索引、加字段属于DDL（数据定义语言）操作，任何对MySQL大表的DDL操作都值得警惕，不然很可能会引起锁表，报错<code>Waiting for meta data lock</code>，造成业务崩溃。那么如何对大表进行加索引操作？</p><h2 id="早期DDL原理"><a href="#早期DDL原理" class="headerlink" title="早期DDL原理"></a>早期DDL原理</h2><p>再谈如何对加大表加索引前，先谈一下MySQL DDL操作为什么会锁表？对于这个问题，需要先了解一下MySQL5.6.7之前的早期DDL原理。</p><p>早期DDL操作分为<code>copy table</code>和<code>inplace</code>两种方式。</p><h3 id="copy-table-方式"><a href="#copy-table-方式" class="headerlink" title="copy table 方式"></a>copy table 方式</h3><ol><li>创建与原表相同的<strong>临时表</strong>，并在临时表上执行DDL语句</li><li><strong>锁原表，不允许DML（数据操作语言），允许查询</strong></li><li>将原表中数据逐行拷贝至临时表（过程没有排序）</li><li>原表升级锁，禁止读写，即原表暂停服务</li><li>rename操作，将临时表重命名原表</li></ol><h3 id="inplace-方式"><a href="#inplace-方式" class="headerlink" title="inplace 方式"></a>inplace 方式</h3><p>fast index creation，仅支持索引的创建跟删除</p><ol><li>创建<strong>frm</strong>（表结构定义文件）临时文件</li><li><strong>锁原表，不允许DML（数据操作语言），允许查询</strong></li><li>根据聚集索引顺序构建新的索引项，按照顺序插入新的索引页</li><li>原表升级锁，禁止读写，即原表暂停服务</li><li>rename操作，替换原表的frm文件</li></ol><h3 id="copy方式-VS-inplace-方式？"><a href="#copy方式-VS-inplace-方式？" class="headerlink" title="copy方式  VS inplace 方式？"></a>copy方式  VS inplace 方式？</h3><p>inplace 方式相对于 copy 方式来说，inplace 不会生成临时表，不会发生数据拷贝，所以<strong>减少了I&#x2F;O资源占用</strong>。</p><p>inplace 只适用于<strong>索引的创建与删除</strong>，不适用于其他类的DDL语句。</p><p>不论是早期copy还是早期inplace方式的DDL，都会进行<strong>锁表操作，不允许DML操作，仅允许查询</strong>。</p><p>知道了DDL的机制，下面就了解一下“如何对大表进行加索引操作”！</p><h2 id="方案一：“影子策略”"><a href="#方案一：“影子策略”" class="headerlink" title="方案一：“影子策略”"></a>方案一：“影子策略”</h2><p>此方法来自《高性能MySQL》一书中的方案。</p><h3 id="方案思路"><a href="#方案思路" class="headerlink" title="方案思路"></a>方案思路</h3><ol><li>创建一张与原表（tb）结构相同的新表（tb_new）</li><li>在新表上创建索引</li><li>重命名原表为其他表名（tb &#x3D;&gt; tb_tmp），新表重命名为原表名（tb_new &#x3D;&gt; tb），此时新表（tb）承担业务</li><li>为原表（tb_tmp）新增索引</li><li>交换表，新表改回最初的名称（tb &#x3D;&gt; tb_new），原表改回最初的名称（tb_tmp &#x3D;&gt; tb），原表（tb）重新承担业务</li><li>把新表数据导入原表（即把新表承担业务期间产生的数据和到原表中）</li></ol><h3 id="如何实践"><a href="#如何实践" class="headerlink" title="如何实践"></a>如何实践</h3><p>SQL实现：</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token comment"># 以下sql对应上面六步</span><span class="token keyword">create</span> <span class="token keyword">table</span> tb_new <span class="token operator">like</span> tb<span class="token punctuation">;</span><span class="token keyword">alter</span> <span class="token keyword">table</span> tb_new <span class="token keyword">add</span> <span class="token keyword">index</span> idx_col_name <span class="token punctuation">(</span>col_name<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">rename</span> <span class="token keyword">table</span> tb <span class="token keyword">to</span> tb_tmp<span class="token punctuation">,</span> tb_new <span class="token keyword">to</span> tb<span class="token punctuation">;</span><span class="token keyword">alter</span> <span class="token keyword">table</span> tb_tmp <span class="token keyword">add</span> <span class="token keyword">index</span> idx_col_name <span class="token punctuation">(</span>col_name<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">rename</span> <span class="token keyword">table</span> tb <span class="token keyword">to</span> tb_new<span class="token punctuation">,</span> tb_tmp <span class="token operator">=</span><span class="token operator">></span> tb<span class="token punctuation">;</span><span class="token keyword">insert</span> <span class="token keyword">into</span> tb <span class="token punctuation">(</span>col_name1<span class="token punctuation">,</span> col_name2<span class="token punctuation">)</span> <span class="token keyword">select</span> col_name1<span class="token punctuation">,</span> col_name2 <span class="token keyword">from</span> tb_new<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="有哪些问题"><a href="#有哪些问题" class="headerlink" title="有哪些问题"></a>有哪些问题</h3><p>步骤3之后，新表改为原表名后（tb）开始承担业务，步骤3到结束之前这段时间的新产生的数据都是存在新表中的，但是如果有业务对老数据进行修改或删除操作，那将无法实现，所以步骤3到结束这段时间可能会产生数据（更新和删除）丢失。</p><h2 id="方案二：pt-online-schema-change"><a href="#方案二：pt-online-schema-change" class="headerlink" title="方案二：pt-online-schema-change"></a>方案二：pt-online-schema-change</h2><p>PERCONA提供若干维护MySQL的小工具，其中 pt-online-schema-change（简称pt-osc）便可用来相对安全地对大表进行DDL操作。</p><p>pt-online-schema-change 方案利用三个触发器（DELETE \ UPDATE \ INSERT触发器）解决了“影子策略”存在的问题，让新老表数据同步时发生的数据变动也能得到同步。</p><h3 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h3><ol><li>创建一张与原表结构相同的新表</li><li>对新表进行DDL操作（如加索引）</li><li>在原表上创建3个触发器（DELETE\UPDATE\INSERT），用来原表复制到新表时（步骤4）的数据改动时的同步</li><li>将原表数据以数据块（chunk）的形式复制到新表</li><li>表交换，原表重命名为old表，新表重命名原表名</li><li>删除旧表，删除触发器</li></ol><h3 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h3><p>见<a href="https://link.segmentfault.com/?enc=LZyogq6Wt0yUIXj9hpxKBw==.mTz7mGjIRe+qR0GQwjJJbmtqPnn6lrupLe5wCR/Ifh53ZSk4VgqnepCKNNljhT7ZB7O05511WulBpY8nkzJzgQ5naGBmq5SEkORgF1Gherdcemq6mzYwFFWwOCZsjHDu3xySOppPU4/a7DEPqjky2ZHYHysFX9DSq/MBxf+28uE=">使用 pt-online-schema-change 工具不锁表在线修改 MySQL 表结构</a>一文</p><h3 id="问题疑惑"><a href="#问题疑惑" class="headerlink" title="问题疑惑"></a>问题疑惑</h3><p>见<a href="https://link.segmentfault.com/?enc=vdd59uzwtDTSyc6GGmW28g==.GmAUReMDuDxsUXZZAz4TtsBNrADSH0I/7ue01BhFOnLOdi5ov0BegypEZQNbysVYFPaVXcTD2ShJmVMAT3L4bA==">pt-online-schema-change的原理解析与应用说明-问题解答</a></p><h2 id="方案三：ONLINE-DDL"><a href="#方案三：ONLINE-DDL" class="headerlink" title="方案三：ONLINE DDL"></a>方案三：ONLINE DDL</h2><p>MySQL5.6.7 之前由于DDL实现机制的局限性，常用“影子策略”和 pt-online-schema-change 方案进行DDL操作，保证相对安全性。在 MySQL5.6.7 版本中新推出了 Online DDL 特性，支持“无锁”DDL。5.7版本已趋于成熟，所以在5.7之后可以直接利用 ONLINE DDL特性。</p><p>对于 ONLINE DDL 下的 inplace 方式，分为了 <code>rebuild table</code> 和 <code>no-rebuild table</code>。</p><h3 id="Online-DDL执行阶段"><a href="#Online-DDL执行阶段" class="headerlink" title="Online DDL执行阶段"></a>Online DDL执行阶段</h3><p>大致可分为三个阶段：初始化、执行和提交</p><h4 id="Initialization阶段"><a href="#Initialization阶段" class="headerlink" title="Initialization阶段"></a>Initialization阶段</h4><p>此阶段会使用MDL读锁，禁止其他并发线程修改表结构<br>服务器将考虑存储引擎能力、语句中指定的操作以及用户指定的ALGORITHM 和 LOCK选项，确定操作期间允许的并发数</p><h4 id="Execution阶段"><a href="#Execution阶段" class="headerlink" title="Execution阶段"></a>Execution阶段</h4><p>此阶段分为两个步骤 Prepared and Executed<br>此阶段是否需要MDL写锁取决于Initialization阶段评估的因素，如果需要MDL写锁的话，仅在Prepared过程会短暂的使用MDL写锁<br>其中最耗时的是Excuted过程</p><h4 id="Commit-Table-Definition阶段"><a href="#Commit-Table-Definition阶段" class="headerlink" title="Commit Table Definition阶段"></a>Commit Table Definition阶段</h4><p>此阶段会将MDL读锁升级到MDL写锁，此阶段一般较快，因此独占锁的时间也较短<br>用新的表定义替换旧的表定义(如果rebuild table)</p><h3 id="ONLINE-DDL-过程"><a href="#ONLINE-DDL-过程" class="headerlink" title="ONLINE DDL 过程"></a>ONLINE DDL 过程</h3><ol><li>获取对应要操作表的 MDL（metadata lock）写锁</li><li>MDL写锁 降级成 MDL读锁</li><li>真正做DDL操作</li><li>MDL读锁 升级成 MDL写锁</li><li>释放MDL锁</li></ol><p>在第3步时，DDL操作时是不会进行锁表的，可以进行DML操作。但可能在拿DML写锁时锁住，见文章<a href="https://link.segmentfault.com/?enc=yKSiYtlA+/DHBQARytIswA==.NUd8BeocCVMVlmgOSLJOnJqrdCy9I5PIEfxDJPad8PVd/Sgo7PKv6xKNLtLSHj4Y">MySQL · 源码阅读 · 白话Online DDL</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;mysql如何给大表加索引&quot;&gt;&lt;a href=&quot;#mysql如何给大表加索引&quot; class=&quot;headerlink&quot; title=&quot;mysql如何给大表加索引&quot;&gt;&lt;/a&gt;mysql如何给大表加索引&lt;/h1&gt;&lt;p&gt;给大表加索引、加字段属于DDL（数据定义语言）操作，</summary>
      
    
    
    
    <category term="JAVA" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/"/>
    
    <category term="数据库" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="MYSQL" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/"/>
    
    
    <category term="mysql" scheme="https://zhuansunpengcheng.gitee.io/tags/mysql/"/>
    
    <category term="索引" scheme="https://zhuansunpengcheng.gitee.io/tags/%E7%B4%A2%E5%BC%95/"/>
    
  </entry>
  
  <entry>
    <title>单精度与双精度是什么意思，有什么区别？</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/%E5%8D%95%E7%B2%BE%E5%BA%A6%E4%B8%8E%E5%8F%8C%E7%B2%BE%E5%BA%A6%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/%E5%8D%95%E7%B2%BE%E5%BA%A6%E4%B8%8E%E5%8F%8C%E7%B2%BE%E5%BA%A6%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/</id>
    <published>2022-10-31T10:57:46.462Z</published>
    <updated>2022-10-31T10:57:46.462Z</updated>
    
    <content type="html"><![CDATA[<h1 id="单精度与双精度是什么意思，有什么区别？"><a href="#单精度与双精度是什么意思，有什么区别？" class="headerlink" title="单精度与双精度是什么意思，有什么区别？"></a>单精度与双精度是什么意思，有什么区别？</h1><hr><ul><li>单精度是这样的格式：1位符号，8位指数，23位小数（总共占32位）</li><li>所以在mysql中，float占用4个字节（32位）</li></ul><img src="单精度与双精度是什么意思有什么区别.assets/image-20220902113054136.png" alt="image-20220902113054136" style="zoom:80%;" /><ul><li>双精度是这样的格式：1位符号，11位指数，52位小数（总共占64位）</li><li>所以在mysql中，double占用8个字节（64位）</li></ul><img src="单精度与双精度是什么意思有什么区别.assets/image-20220902113110508.png" alt="image-20220902113110508" style="zoom:80%;" /><ul><li>那请问单精度为什么叫单精度，精度代表的是什么？单和双针对的又是谁？</li></ul><p>根据IEEE754的规范，要表达的数字占32位是个基准，称为“单”，32位的整数倍即为“几倍精度”。所以还有“半精度”、“双精度”、“四倍精度”、“任意精度”等。</p><img src="https://pic1.zhimg.com/v2-08470f89b22d899d31b192cf67c78624.png" alt="preview" style="zoom:80%;" />]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;单精度与双精度是什么意思，有什么区别？&quot;&gt;&lt;a href=&quot;#单精度与双精度是什么意思，有什么区别？&quot; class=&quot;headerlink&quot; title=&quot;单精度与双精度是什么意思，有什么区别？&quot;&gt;&lt;/a&gt;单精度与双精度是什么意思，有什么区别？&lt;/h1&gt;&lt;hr&gt;
</summary>
      
    
    
    
    <category term="JAVA" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/"/>
    
    <category term="数据库" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="MYSQL" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/"/>
    
    
    <category term="mysql" scheme="https://zhuansunpengcheng.gitee.io/tags/mysql/"/>
    
    <category term="单精度" scheme="https://zhuansunpengcheng.gitee.io/tags/%E5%8D%95%E7%B2%BE%E5%BA%A6/"/>
    
    <category term="双精度" scheme="https://zhuansunpengcheng.gitee.io/tags/%E5%8F%8C%E7%B2%BE%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>mysql的索引从入门到入土</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/mysql%E7%9A%84%E7%B4%A2%E5%BC%95%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/mysql%E7%9A%84%E7%B4%A2%E5%BC%95%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/</id>
    <published>2022-10-31T10:57:46.461Z</published>
    <updated>2022-10-31T10:57:46.461Z</updated>
    
    <content type="html"><![CDATA[<h1 id="mysql的索引从入门到入土"><a href="#mysql的索引从入门到入土" class="headerlink" title="mysql的索引从入门到入土"></a>mysql的索引从入门到入土</h1><h2 id="索引的XMIND图"><a href="#索引的XMIND图" class="headerlink" title="索引的XMIND图"></a>索引的XMIND图</h2><p>文件位置：<a href="./mysql%E7%9A%84%E7%B4%A2%E5%BC%95%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F.assets/Mysql%E7%B4%A2%E5%BC%95.xmind">点我打开</a></p><h2 id="InnoB的索引模型（B-树）"><a href="#InnoB的索引模型（B-树）" class="headerlink" title="InnoB的索引模型（B+树）"></a>InnoB的索引模型（B+树）</h2><p>每一个索引在 InnoDB 里面对应一棵 B+ 树。</p><p>索引类型分为主键索引和非主键索引（普通索引或者二级索引）。</p><p>主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。</p><p>非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。</p><ul><li>InnoDB的索引和数据是放在同一个文件中的，索引innoDB：数据即索引，索引即数据</li><li>MyISAM的索引文件和数据文件是分开的</li><li>聚簇索引：聚簇索引不是一个索引类型，而是一个数据存储的方式，将索引和数据存放在一起，就叫做聚簇索引。</li><li>非聚簇索引：就是数据和索引分开存储；</li><li>在InnoDB中，主键索引的叶子节点存的就是数据，所以说InnoDB的主键索引就是聚簇索引</li><li>在InnoDB中，普通索引（二级索引）的叶子节点存的是主键，并不是数据，索引普通索引（二级索引）就是非聚簇索引；</li><li>在MyIASM中，因为索引文件和数据文件是分开的，所以MyISAM天然就是非聚簇索引。</li></ul><h2 id="索引维护（更新，页分裂，页合并）"><a href="#索引维护（更新，页分裂，页合并）" class="headerlink" title="索引维护（更新，页分裂，页合并）"></a>索引维护（更新，页分裂，页合并）</h2><p>为什么我们一般在建表的时候都会创建一个自增主键，及时表中有业务唯一的id，也会创建一个自增主键？</p><p>1、因为主键只会自增，在B+树中一直都是往后写的，不会触发页分裂；但是如果删除过多的话，会触发页合并；</p><p>2、要考虑业务唯一的id的长度，比如身份证号，如果用身份证号作为主键，比直接使用整型自增的主键占用的字节数要多，这样每一个页存放的数据就会少，每个页存的数据少了，这样查询的时候，效率就会低。 同时主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。</p><p>3、所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。</p><h2 id="基于主键索引和普通索引的查询有什么区别？"><a href="#基于主键索引和普通索引的查询有什么区别？" class="headerlink" title="基于主键索引和普通索引的查询有什么区别？"></a>基于主键索引和普通索引的查询有什么区别？</h2><p>回表</p><p>回表的优化：覆盖索引，最左前缀原则，索引下推</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="普通索引和唯一索引怎么选择？"><a href="#普通索引和唯一索引怎么选择？" class="headerlink" title="普通索引和唯一索引怎么选择？"></a>普通索引和唯一索引怎么选择？</h3><ul><li><p>普通索引和唯一索引对查询的影响？ </p></li><li><p>微乎其微，为什么？</p></li><li><p>普通索引和唯一索引对更新的影响？</p></li><li><p>change buffer是什么？</p></li><li><p>为什么唯一索引的更新就不能使用 change buffer，只有普通索引可以使用。</p></li><li><p>change buffer的优点和缺点是什么： 写多读少的场景和写多读多的场景；</p></li><li><p>change buffer 和 redo log的联系和区别。</p></li></ul><h3 id="为什么会选错索引，选错了怎么办？"><a href="#为什么会选错索引，选错了怎么办？" class="headerlink" title="为什么会选错索引，选错了怎么办？"></a>为什么会选错索引，选错了怎么办？</h3><ul><li><p>由于索引统计的更新机制，索引统计信息不准确导致的。</p></li><li><p>解决：重新采集统计信息：analyze table</p></li><li><p>解决：手动指定索引；force index</p></li></ul><h3 id="Mysql中有哪些索引"><a href="#Mysql中有哪些索引" class="headerlink" title="Mysql中有哪些索引"></a>Mysql中有哪些索引</h3><p>普通索引：最基本的索引，没有任何限制</p><p>唯一索引：与”普通索引”类似，不同的就是：索引列的值必须唯一，但允许有空值。</p><p>主键索引：它 是一种特殊的唯一索引，不允许有空值。</p><p>全文索引：仅可用于 MyISAM 表，针对较大的数据，生成全文索引很耗时好空间。</p><p>组合索引：为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则。</p><h3 id="怎么对长字符串加索引"><a href="#怎么对长字符串加索引" class="headerlink" title="怎么对长字符串加索引"></a>怎么对长字符串加索引</h3><p>前缀索引</p><p>前缀索引对覆盖索引的影响</p><p>如果长字段选择性不够怎么办，比如身份证号，同一个地区前面都是一样的：</p><ul><li><p>倒序存储，多加一列hash；</p></li><li><p>不支持范围查询，只能等值查询</p></li></ul><h3 id="主键索引的B-树结构是什么样子的？"><a href="#主键索引的B-树结构是什么样子的？" class="headerlink" title="主键索引的B+树结构是什么样子的？"></a>主键索引的B+树结构是什么样子的？</h3><h3 id="普通索引的B-树结构是什么样子的？"><a href="#普通索引的B-树结构是什么样子的？" class="headerlink" title="普通索引的B+树结构是什么样子的？"></a>普通索引的B+树结构是什么样子的？</h3><h3 id="联合索引的B-树结构是什么样子的？"><a href="#联合索引的B-树结构是什么样子的？" class="headerlink" title="联合索引的B+树结构是什么样子的？"></a>联合索引的B+树结构是什么样子的？</h3><h3 id="索引的页分裂和页合并是怎么导致的，会有什么影响？具体的分裂和合并的过程是什么样子的？"><a href="#索引的页分裂和页合并是怎么导致的，会有什么影响？具体的分裂和合并的过程是什么样子的？" class="headerlink" title="索引的页分裂和页合并是怎么导致的，会有什么影响？具体的分裂和合并的过程是什么样子的？"></a>索引的页分裂和页合并是怎么导致的，会有什么影响？具体的分裂和合并的过程是什么样子的？</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;mysql的索引从入门到入土&quot;&gt;&lt;a href=&quot;#mysql的索引从入门到入土&quot; class=&quot;headerlink&quot; title=&quot;mysql的索引从入门到入土&quot;&gt;&lt;/a&gt;mysql的索引从入门到入土&lt;/h1&gt;&lt;h2 id=&quot;索引的XMIND图&quot;&gt;&lt;a hre</summary>
      
    
    
    
    <category term="JAVA" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/"/>
    
    <category term="数据库" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="MYSQL" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/"/>
    
    
    <category term="mysql" scheme="https://zhuansunpengcheng.gitee.io/tags/mysql/"/>
    
    <category term="索引" scheme="https://zhuansunpengcheng.gitee.io/tags/%E7%B4%A2%E5%BC%95/"/>
    
  </entry>
  
  <entry>
    <title>mysql的数据宽度与数据长度</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/mysql%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AE%BD%E5%BA%A6%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%95%BF%E5%BA%A6/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/mysql%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AE%BD%E5%BA%A6%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%95%BF%E5%BA%A6/</id>
    <published>2022-10-31T10:57:46.459Z</published>
    <updated>2022-10-31T10:57:46.459Z</updated>
    
    <content type="html"><![CDATA[<h1 id="mysql的数据宽度与数据长度"><a href="#mysql的数据宽度与数据长度" class="headerlink" title="mysql的数据宽度与数据长度"></a>mysql的数据宽度与数据长度</h1><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在mysql的表定义中</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> <span class="token identifier"><span class="token punctuation">`</span>ag_aging_warn_clean<span class="token punctuation">`</span></span> <span class="token punctuation">(</span><span class="token identifier"><span class="token punctuation">`</span>id<span class="token punctuation">`</span></span> <span class="token keyword">BIGINT</span> <span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token operator">NOT</span> <span class="token boolean">NULL</span> <span class="token keyword">AUTO_INCREMENT</span><span class="token punctuation">,</span><span class="token identifier"><span class="token punctuation">`</span>status_code<span class="token punctuation">`</span></span> <span class="token keyword">VARCHAR</span> <span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span> <span class="token keyword">COMMENT</span> <span class="token string">'状态code'</span><span class="token punctuation">,</span><span class="token identifier"><span class="token punctuation">`</span>money<span class="token punctuation">`</span></span> <span class="token keyword">float</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">DEFAULT</span> <span class="token boolean">NULL</span><span class="token punctuation">,</span><span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span> <span class="token punctuation">(</span><span class="token identifier"><span class="token punctuation">`</span>id<span class="token punctuation">`</span></span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token keyword">KEY</span> <span class="token identifier"><span class="token punctuation">`</span>ag_aging_warn_clean<span class="token punctuation">`</span></span> <span class="token punctuation">(</span><span class="token identifier"><span class="token punctuation">`</span>status_code<span class="token punctuation">`</span></span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">ENGINE</span> <span class="token operator">=</span> <span class="token keyword">INNODB</span> <span class="token keyword">AUTO_INCREMENT</span> <span class="token operator">=</span> <span class="token number">66</span> <span class="token keyword">DEFAULT</span> <span class="token keyword">CHARSET</span> <span class="token operator">=</span> utf8mb4<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>会有字段长度的定义，比如下面这种</p><ul><li>bigint(20)</li><li>varchar(255)</li><li>float(5,2)</li></ul><p>括号外面的我们知道是数据类型，括号里面的是什么意思呢？</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>说到这个，就要说到mysql的数据类型了，跟着我一步一步走，在mysql中数据类型大致分为五类（下面列的比较全）：</p><h3 id="mysql中数据类型"><a href="#mysql中数据类型" class="headerlink" title="mysql中数据类型"></a>mysql中数据类型</h3><ul><li>整数类型：BIT、BOOL、TINY INT、SMALL INT、MEDIUM INT、 INT、 BIG INT</li><li>浮点数类型：FLOAT、DOUBLE、DECIMAL</li><li>字符串类型：CHAR、VARCHAR、TINY TEXT、TEXT、MEDIUM TEXT、LONGTEXT、TINY BLOB、BLOB、MEDIUM BLOB、LONG BLOB</li><li>日期类型：Date、DateTime、TimeStamp、Time、Year</li><li>其他数据类型：BINARY、VARBINARY、ENUM、SET、Geometry、Point、MultiPoint、LineString、MultiLineString、Polygon、GeometryCollection等</li></ul><p>然后针对每一个数据类型，它的长度和范围是不一样的（只列出常用的数据类型，全的可以参考 ： <a href="https://m.php.cn/article/460317.html%EF%BC%89">https://m.php.cn/article/460317.html）</a></p><h3 id="mysql中各数据类型及字节长度"><a href="#mysql中各数据类型及字节长度" class="headerlink" title="mysql中各数据类型及字节长度"></a>mysql中各数据类型及字节长度</h3><table><thead><tr><th>数据类型</th><th>字节长度</th><th>范围或用法</th></tr></thead><tbody><tr><td>Bit</td><td>1</td><td>无符号[0,255]，有符号[-128,127]，<strong>备注</strong>：BIT和BOOL布尔型都占用1字节</td></tr><tr><td>TinyInt</td><td>1</td><td>整数[0,255]</td></tr><tr><td>SmallInt</td><td>2</td><td>无符号[0,65535]，有符号[-32768,32767]</td></tr><tr><td>MediumInt</td><td>3</td><td>无符号[0,2^24-1]，有符号[-2^23,2^23-1]]</td></tr><tr><td>Int</td><td>4</td><td>无符号[0,2^32-1]，有符号[-2^31,2^31-1]</td></tr><tr><td>BigInt</td><td>8</td><td>无符号[0,2^64-1]，有符号[-2^63 ,2^63 -1]</td></tr><tr><td>Float(M,D)</td><td>4</td><td>单精度浮点数。<strong>备注</strong>：这里的D是精度，如果D&lt;&#x3D;24则为默认的FLOAT，如果D&gt;24则会自动被转换为DOUBLE型。</td></tr><tr><td>Double(M,D)</td><td>8</td><td>双精度浮点。<strong>备注</strong>：关于精度可以参考：<a href="%E5%8D%95%E7%B2%BE%E5%BA%A6%E4%B8%8E%E5%8F%8C%E7%B2%BE%E5%BA%A6%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB.md">单精度与双精度是什么意思有什么区别.md</a></td></tr><tr><td>Decimal(M,D)</td><td>M+1或M+2</td><td>未打包的浮点数，用法类似于FLOAT和DOUBLE，天缘博客提醒您如果在ASP中使用到Decimal数据类型，直接从数据库读出来的Decimal可能需要先转换成Float或Double类型后再进行运算。</td></tr><tr><td>Date</td><td>3</td><td>以YYYY-MM-DD的格式显示，比如：2009-07-19</td></tr><tr><td>Date Time</td><td>8</td><td>以YYYY-MM-DD HH:MM:SS的格式显示，比如：2009-07-19 11：22：30</td></tr><tr><td>TimeStamp</td><td>4</td><td>以YYYY-MM-DD的格式显示，比如：2009-07-19</td></tr><tr><td>Time</td><td>3</td><td>以HH:MM:SS的格式显示。比如：11：22：30</td></tr><tr><td>Year</td><td>1</td><td>以YYYY的格式显示。比如：2009</td></tr><tr><td>Char(M)</td><td>M</td><td>定长字符串。</td></tr><tr><td>VarChar(M)</td><td>M</td><td>变长字符串，要求M&lt;&#x3D;255</td></tr><tr><td>Binary(M)</td><td>M</td><td>类似Char的二进制存储，特点是插入定长不足补0</td></tr><tr><td>VarBinary(M)</td><td>M</td><td>类似VarChar的变长二进制存储，特点是定长不补0</td></tr><tr><td>Tiny Text</td><td>Max:255</td><td>大小写不敏感</td></tr><tr><td>Text</td><td>Max:64K</td><td>大小写不敏感</td></tr><tr><td>Medium Text</td><td>Max:16M</td><td>大小写不敏感</td></tr><tr><td>Long Text</td><td>Max:4G</td><td>大小写不敏感</td></tr><tr><td>TinyBlob</td><td>Max:255</td><td>大小写敏感</td></tr><tr><td>Blob</td><td>Max:64K</td><td>大小写敏感</td></tr><tr><td>MediumBlob</td><td>Max:16M</td><td>大小写敏感</td></tr><tr><td>LongBlob</td><td>Max:4G</td><td>大小写敏感</td></tr><tr><td>Enum</td><td>1或2</td><td>最大可达65535个不同的枚举值</td></tr><tr><td>Set</td><td>可达8</td><td>最大可达64个不同的值</td></tr></tbody></table><h3 id="整数型的可选属性"><a href="#整数型的可选属性" class="headerlink" title="整数型的可选属性"></a>整数型的可选属性</h3><p>观察第三列【范围或用法】，可以看到，整数型根据数据类型的不同，存储的数值大小也不同，而且还有<code>无符号</code>的概念，那么这俩是什么意思呢？</p><p>其实在mysql中，整数型的可选属性有三个：</p><ul><li>M   ： 宽度(在0填充的时候才有意义，否则不需要指定)</li><li>unsigned   ： 无符号类型(非负)</li><li>zerofill   ：  0填充,(如果某列是zerofill，那么默认就是无符号)，如果指定了zerofill只是表示不够M位时，用0在左边填充，如果超过M位，只要不超过数据存储范围即可</li></ul><p>那么这三个有什么用的？我们来一个一个看：</p><p>当我们设置了<code>bigint(5)</code>的时候，数据库中存储了 -1,-12,-123,-12345,-123456,1,12,123,12345,123456，分别是什么效果？</p><p>要分设置了 zerofill（0填充）的情况，和不设置 zerofill（0填充） 的情况</p><ul><li>不设置zerofill（0填充）的情况，如下图</li></ul><img src="mysql的数据宽度与数据长度.assets/image-20220902144000573.png" alt="image-20220902144000573" style="zoom:80%;" /><ul><li>设置了zerofill（0填充）的情况，如下图<ul><li>当使用<em>zerofill</em> 时，默认会自动加unsigned（无符号）属性</li></ul></li></ul><img src="mysql的数据宽度与数据长度.assets/image-20220902144158868.png" alt="image-20220902144158868" style="zoom:80%;" /><p>看完了 zerofill（零填充）的效果之后，我们接着来看 unsigned （无符号）</p><ul><li>无符号没什么可以说的，以bigint为例</li><li>无符号[0,2^64-1]，有符号[-2^63 ,2^63 -1]</li><li>设置无符号之后，负数是无法插入的，网上一些傻逼博客说可以插入，只不过展示0，全是放屁</li></ul><img src="mysql的数据宽度与数据长度.assets/image-20220902153330878.png" alt="image-20220902153330878" style="zoom:80%;" /><p>问：bigint(5)和bigint(20)有什么区别？</p><ul><li>在数据插入方面，没有任何区别；两者占用的空间都是8字节；</li><li>只不过展示方面不一样，见上</li></ul><h3 id="字符串型的宽度"><a href="#字符串型的宽度" class="headerlink" title="字符串型的宽度"></a>字符串型的宽度</h3><p>上面了解了 整数型 的【宽度】，引出了无符号和零填充的概念；那么针对字符串呢？</p><ul><li>字符串不支持【无符号】和【零填充】</li><li>但是字符串支持【宽度】</li></ul><p>比如下面这种： varchar(255)，其中的255表示什么含义呢？</p><ul><li>表示这个列只能存储255个字节，是真正的长度限制。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;mysql的数据宽度与数据长度&quot;&gt;&lt;a href=&quot;#mysql的数据宽度与数据长度&quot; class=&quot;headerlink&quot; title=&quot;mysql的数据宽度与数据长度&quot;&gt;&lt;/a&gt;mysql的数据宽度与数据长度&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a h</summary>
      
    
    
    
    <category term="JAVA" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/"/>
    
    <category term="数据库" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="MYSQL" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/"/>
    
    
    <category term="单精度" scheme="https://zhuansunpengcheng.gitee.io/tags/%E5%8D%95%E7%B2%BE%E5%BA%A6/"/>
    
    <category term="双精度" scheme="https://zhuansunpengcheng.gitee.io/tags/%E5%8F%8C%E7%B2%BE%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>mysql的性能调优</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/mysql%E7%9A%84%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/mysql%E7%9A%84%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</id>
    <published>2022-10-31T10:57:46.455Z</published>
    <updated>2022-10-31T10:57:46.455Z</updated>
    
    <content type="html"><![CDATA[<p>在出现IO瓶颈的时候，可以将sync_binglog设置为100-1000内的值，表示累计多少个事务之后才会刷盘，默认是1表示每一个事务都会刷盘写binlog；</p><p>在出现IO瓶颈的时候，可以设置binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count，表示提升binlog的组提交的效果，但是会增加sql的响应时间</p><p>写多读少的场景，由于 change buffer 的优化机制，因此如果业务可以接受，从性能角度出发我建议你优先考虑普通索引。</p><p>mysql要调优，就要知道它为什么慢，哪里慢</p><p>在低版本：show profiles</p><p>在高版本：performance schema</p><p>以上有什么用？</p><ul><li>比如在实际环境中，有一个sql，非常慢</li><li>sql本身没有问题，看执行计划也比较慢，这个时候就可以用到上面的</li><li>可以更加详细的看到，这个sql到底是哪里慢</li></ul><p>process list 可以查看mysql的连接数量，顺丰云上的回话连接是不是？</p><p>不过一般有druid，不用太关注这个。但是呢，druid是什么，要去看看github上面的官网了。了解一下它的优点</p><p>字符集和字符编码的区别</p><p>大表拆分，将不常用的数据从表里拆出去，表小了之后，每个数据页存的数据就会多，查询的时候，就会减少磁盘IO</p><p>优化：</p><ul><li>不同的数据选择对应的mysql自建数据类型，比如数字就用数字类型，不要用字符串，时间就用时间类型，不要用字符串；用错了数据类型，对业务可能没影响，但是对mysql来说，内部多了一层转换，sql执行会比较慢</li><li>事件类型，建议用date，而不是timestamp，date类型</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在出现IO瓶颈的时候，可以将sync_binglog设置为100-1000内的值，表示累计多少个事务之后才会刷盘，默认是1表示每一个事务都会刷盘写binlog；&lt;/p&gt;
&lt;p&gt;在出现IO瓶颈的时候，可以设置binlog_group_commit_sync_delay 和 b</summary>
      
    
    
    
    <category term="JAVA" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/"/>
    
    <category term="数据库" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="MYSQL" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/"/>
    
    
    <category term="mysql" scheme="https://zhuansunpengcheng.gitee.io/tags/mysql/"/>
    
    <category term="性能" scheme="https://zhuansunpengcheng.gitee.io/tags/%E6%80%A7%E8%83%BD/"/>
    
    <category term="调优" scheme="https://zhuansunpengcheng.gitee.io/tags/%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
  <entry>
    <title>mysql排序字段相同导致结果不一致</title>
    <link href="https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/mysql%E6%8E%92%E5%BA%8F%E5%AD%97%E6%AE%B5%E7%9B%B8%E5%90%8C%E5%AF%BC%E8%87%B4%E7%BB%93%E6%9E%9C%E4%B8%8D%E4%B8%80%E8%87%B4/"/>
    <id>https://zhuansunpengcheng.gitee.io/note/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/mysql%E6%8E%92%E5%BA%8F%E5%AD%97%E6%AE%B5%E7%9B%B8%E5%90%8C%E5%AF%BC%E8%87%B4%E7%BB%93%E6%9E%9C%E4%B8%8D%E4%B8%80%E8%87%B4/</id>
    <published>2022-10-31T10:57:46.454Z</published>
    <updated>2022-10-31T10:57:46.454Z</updated>
    
    <content type="html"><![CDATA[<h2 id="mysql排序字段相同导致结果不一致"><a href="#mysql排序字段相同导致结果不一致" class="headerlink" title="mysql排序字段相同导致结果不一致"></a>mysql排序字段相同导致结果不一致</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近公司业务有这么一个功能：对5000条数据进行批量导入，然后通过表格的形式展示在前端，需要根据创建时间进行排序。</p><p>所以，很简单的我们会想到</p><pre class="line-numbers language-mysql" data-language="mysql"><code class="language-mysql">ORDER BY gmt_create DESC;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>但是在实际的使用中，我发现一个问题，就是当时间相同的时候，这个排序是不确定的，是随机的。</p><p>我们看一个例子：</p><ul><li>首先这两条sql语句是一样的，只不过一个查询 <code>*</code> 一个只查询<code>id</code>。那么理所当然的：他们的结果应该是一样的。</li></ul><pre class="line-numbers language-mysql" data-language="mysql"><code class="language-mysql">SELECT * FROM customer WHERE employee_id&#x3D;39 AND assigned&#x3D;1 and status&#x3D;0 ORDER BY gmt_last_transfer DESC ;SELECT id FROM customer WHERE employee_id&#x3D;39 AND assigned&#x3D;1 and status&#x3D;0 ORDER BY gmt_last_transfer DESC ;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><img src="mysql排序字段相同导致结果不一致.assets/image-20220831203859308.png" alt="image-20220831203859308" style="zoom:50%;" /><img src="mysql排序字段相同导致结果不一致.assets/image-20220831203918310.png" alt="image-20220831203918310" style="zoom:50%;" /><p>我们可以看到两次的查询结果是不一样的。</p><h3 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h3><h3 id="为什么结果不一样"><a href="#为什么结果不一样" class="headerlink" title="为什么结果不一样"></a>为什么结果不一样</h3><p>查阅了Goole和相关资料，大概总结了这种情况的原因。其实发生这种现象是“故意”设计的。</p><p>如果没有指定ORDER BY语句，则SQL Server（或任何RDBMS）不保证以特定顺序返回结果。</p><p>有些人认为，如果没有指定order by子句，行总是以聚簇索引顺序或物理磁盘顺序返回。</p><p>然而，这是不正确的，因为在查询处理期间可以改变行顺序的许多因素，例如并行的HASH连接是更改行顺序的操作符的一个很好的例子。</p><p>如果指定ORDER BY语句，SQL Server将对行进行排序，并按请求的顺序返回。</p><p>但是，如果该顺序不是确定性的，即可能有重复的值，则在每个具有相同值的组中，由于与上述相同的原因，该顺序是“随机的”。</p><h3 id="那么怎么保证顺序唯一呢？"><a href="#那么怎么保证顺序唯一呢？" class="headerlink" title="那么怎么保证顺序唯一呢？"></a>那么怎么保证顺序唯一呢？</h3><ul><li><p>不使用可能重复的字段进行排序，即不用时间排序，而使用主键进行排序。</p></li><li><p>但是在某些必须使用时间排序，应该怎么办呢？</p></li><li><p>这个时候就需要使用多字段排序的功能。</p></li><li><p>所谓的多字段排序，其实也很简单。</p></li></ul><p>mysql多个字段排序：</p><pre class="line-numbers language-mysql" data-language="mysql"><code class="language-mysql">select * from table order by id desc,name desc;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>多字字段排序只需要添加多个排序条件，并且每个排序的条件之前用逗号分开。</p><blockquote><p>order by id desc,name desc; </p></blockquote><p>表示先按照id降序排序，再按照name降序排序。</p><p>同理：</p><blockquote><p>order by id desc,name asc;</p></blockquote><p> 表示先按照id降序排序，再按照name升序排序。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;mysql排序字段相同导致结果不一致&quot;&gt;&lt;a href=&quot;#mysql排序字段相同导致结果不一致&quot; class=&quot;headerlink&quot; title=&quot;mysql排序字段相同导致结果不一致&quot;&gt;&lt;/a&gt;mysql排序字段相同导致结果不一致&lt;/h2&gt;&lt;h3 id=&quot;前</summary>
      
    
    
    
    <category term="JAVA" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/"/>
    
    <category term="数据库" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="MYSQL" scheme="https://zhuansunpengcheng.gitee.io/categories/JAVA/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL/"/>
    
    
    <category term="mysql" scheme="https://zhuansunpengcheng.gitee.io/tags/mysql/"/>
    
  </entry>
  
</feed>
