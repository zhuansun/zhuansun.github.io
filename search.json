[{"title":"hexo+butterfly更新mermaid版本","url":"/note/TOOLS/hexo/hexo+butterfly更新mermaid版本/","content":"\n# hexo+butterfly更新mermaid版本\n\n\n\n怎么使用的mermaid，可以参考butterfly的官网：https://butterfly.js.org/posts/4aa8abbe/#mermaid\n\n\n\n最近在本地重新编辑一篇文章之后，部署起来，发现有一个`mermaid`图表不支持了。\n\n原因是因为：我的 `hexo` 引用的 `butterfly` 主题默认使用的 `mermaid` 版本是 `8.13.8`\n\n而我本地是使用 `typora` 编辑的，`typora` 版本是 `1.5.8`，引用的 mermaid 版本是 `9.2.0`\n\n所以我是用了 新版本的特性之后，在低版本中就无法渲染了。\n\n\n\n<img src=\"hexo+butterfly更新mermaid版本.assets/image-20230217231526333.png\" alt=\"image-20230217231526333\" style=\"zoom:50%;\" />\n\n\n\n**解决办法**\n\n更新 hexo+butterfly 的 mermaid 版本\n\nmermaid 版本是主题引入的，其实就是主题引入了一个js\n\n我们找到这个 js ，给它升级了就完事了\n\n\n\n找到主题目录\n\n> .../你的主题目录/hexo-theme-butterfly/layout/includes/footer.pug\n\n<img src=\"hexo+butterfly更新mermaid版本.assets/image-20230217230355932.png\" alt=\"image-20230217230355932\" style=\"zoom:50%;\" />\n\n打开`footer.pug`文件，将引入的低版本mermaid直接升级就可以了\n\n<img src=\"hexo+butterfly更新mermaid版本.assets/image-20230217230516681.png\" alt=\"image-20230217230516681\" style=\"zoom: 33%;\" />\n\n\n\n然后重新部署一遍就可以了。\n\n<img src=\"hexo+butterfly更新mermaid版本.assets/image-20230217231546566.png\" alt=\"image-20230217231546566\" style=\"zoom:50%;\" />\n\n\n\n","tags":["hexo","butterfly","mermaid"],"categories":["TOOLS","hexo"]},{"title":"在页面上一键部署hexo","url":"/note/TOOLS/hexo/在页面上一键部署hexo/","content":"\n\n\nhttps://github.com/msoap/shell2http\n","tags":["hexo"],"categories":["TOOLS","hexo"]},{"title":"docker+hexo+gitee部署完美个人博客","url":"/note/TOOLS/hexo/docker+hexo+gitee部署完美个人博客/","content":"\n\n\n## 引用\n\n因为引用的文章在下面都说到了，可以先看看\n\n[Docker + Git 部署Hexo发布](https://zhuanlan.zhihu.com/p/372398281)\n\n[【hexo指南】hexo配置ER图流程图时序图插件](https://www.cnblogs.com/moshuying/p/15801437.html)\n\n\n\n## 前言\n\n**本文仅适用：x86_64架构**\n\n\n\n因为平时使用typora写文章，除了基本的MD语法之外，还使用了\n\n- 本地图片\n- mermaid\n\n\n\n就是上面两点，导致部署hexo比较麻烦，特别是第一点。\n\n\n\n为什么呢？\n\n\n\nhexo新版本不是支持了本地图片吗？\n\n\n\n但是支持的前提是：图片的文件夹和MD文件名一致，像下面这样\n\n- xxxxx.md （MD文件）\n- xxxxx （图片文件夹）\n\n而我的图片存储方式是：\n\n- xxxxx.md（MD文件）\n- xxxxx.assets（图片文件夹）\n\n对应typora的设置就是：\n\n<img src=\"docker+hexo+gitee部署完美个人博客.assets/image-20221030220722156.png\" alt=\"image-20221030220722156\" style=\"zoom:50%;\" />\n\n所以hexo就不支持了，我就很难受\n\n\n\n## 第一步：准备hexo文件\n\n你需要准备你的所有笔记文件：比如我的是放在gitee的，大致有下面这些笔记\n\n<img src=\"docker+hexo+gitee部署完美个人博客.assets/image-20221030220831563.png\" alt=\"image-20221030220831563\" style=\"zoom:50%;\" />\n\n然后需要准备一个hexo的主题，我推荐是 butterfly，把主题下载下来\n\n然后需要准备hexo的配置文件_config.yml，配置好你所需要的所有内容\n\n然后准备下面的脚本，命名为：hexo-img-move.js\n\n```js\nconst fs = require('fs-extra');\n\n\n//note/xxx/xx.assets  2020/10/30/xxx/xx.assets\nfunction copy(dir,dest){\n\tfs.pathExists(dir, (err, exists) => {\n\t  if (exists) {\n\t\tfs.copy(dir, dest, err => {\n\t\t  if(err) \n\t\t  \treturn console.error(err);\n\t\t  console.log('success!');\n\t\t});\n\t  }\n\t})\n}\n\n\n\n\n//遍历递归public文件夹，将\nfunction readFileList(path, filesList) {\n    var files = fs.readdirSync(path);\n    files.forEach(function (itm, index) {\n        var stat = fs.statSync(path + itm);\n        if (stat.isDirectory()) {//递归读取文件\n            readFileList(path + itm + \"/\", filesList)\n        } else {\n            var obj = {};//定义一个对象存放文件的路径和名字\n            obj.path = path;//路径\n            obj.filename = itm//名字\n            filesList.push(obj);\n        }\n    })\n}\nvar getFiles = {\n    //获取文件夹下的所有文件\n    getFileList: function (path) {\n        var filesList = [];\n        readFileList(path, filesList);\n        return filesList;\n    },\n};\n\n\ngetFiles.getFileList('/var/www/hexo/public/').forEach(function (obj){\n\t// console.log(obj.path);\n\n\n\t// var paths = obj.path.split('public/');\n\t// var dir_part = paths[1];\n\t// var dir_full = '/var/www/hexo/source/_posts/'+dir_part;\n\t// var dir = dir_full.substring(0,dir_full.length-1)+'.assets';\n\n\n\tvar dir_part = obj.path.split('note')[1];\n\tvar dir_full = '/var/www/hexo/source/_posts/note'+dir_part;\n\tvar dir = dir_full.substring(0,dir_full.length-1)+'.assets';\n\n\n\t//第一种：复制到 dir 的下一级目录下\n\tvar dirs = dir.split('/');\n\tvar aessets_name = dirs[dirs.length-1]\n\tvar dest1 = obj.path+aessets_name;\n\n\n\n\t// 第二种：复制到 dir 同级目录下\n\t// var dest2 = obj.path.substring(0,obj.path.length-1)+'.assets';\n\n\n\t//第三种：复制到 public 下\n\tvar dirs = dir.split('/');\n\tvar forder_name = dirs[dirs.length-1].split('.assets')[0];\n\tvar dest_full = obj.path.substring(0,obj.path.length-1)+'.assets';\n\tvar dests = dest_full.split('/');\n\tvar dest3 = '/var/www/hexo/public/'+dests[dests.length-1];\n\n\t//\n\n\t///var/www/hexo/public/2022/10/30/note/JAVA/数据库/MYSQL/mysql的日志从入门到入土/\n\t// console.log(dir)\n\tcopy(dir,dest1);\n\tcopy(dir,dest3);\n\n})\n\n\n```\n\n\n\n\n\n<img src=\"docker+hexo+gitee部署完美个人博客.assets/image-20221030221220970.png\" alt=\"image-20221030221220970\" style=\"zoom:80%;\" />\n\n\n\n## 第二步：准备dockerfile\n\n将下面的文件 命名为：Dockerfile  没有后缀名\n\n```dockerfile\nFROM node:14-alpine\nWORKDIR /var/www/hexo\n\nRUN echo \"Asia/Shanghai\" > /etc/timezone \\\n    && echo \"https://mirrors.aliyun.com/alpine/v3.9/main/\" > /etc/apk/repositories  \\\n    && npm config set registry https://registry.npm.taobao.org \\\n    && apk add --no-cache git \\\n    && apk add --no-cache openssh-client \\\n    && npm install hexo-cli -g \\\n    && hexo init \\\n    && npm install hexo-renderer-swig \\\n    && npm install \\\n    && npm install fs-extra --save \n    && npm install hexo-deployer-git --save \\\n    && npm install hexo-renderer-jade hexo-generator-feed hexo-generator-sitemap hexo-browsersync hexo-generator-archive --save \\\n    && npm install hexo-filter-mermaid-diagrams --save \\\n    && npm install hexo-blog-encrypt --save \\\n    && npm install hexo-generator-search --save \\\n    && git config --global user.email \"zhuansunpengcheng@qq.com\" \\\n    && git config --global user.name \"zhuansun\" \\\n    && ssh-keygen -t RSA -C \"zhuansunpengcheng@qq.com\" -P \"\" -N \"\" -f /root/.ssh/id_rsa \\\n    && echo \"StrictHostKeyChecking no\" >> /etc/ssh/ssh_config \\\n    && cat /root/.ssh/id_rsa.pub\n\n```\n\n\n\n- 我们使用alpine作为基本镜像，因为够小，对于hexo来说，足够了\n- 设置工作目录是 /var/www/hexo\n- 下面就是安装git，ssh，hexo\n- npm install fs-extra --save ：安装fs-extra的依赖，是为了我们的 hexo-img-move.js能正常运行\n- 安装了一些hexo的插件\n  - hexo-deployer-git：让hexo支持直接部署到git上\n  - hexo-renderer-jade hexo-generator-feed hexo-generator-sitemap hexo-browsersync hexo-generator-archive：这些是为了butterfly主题支持用的\n  - hexo-filter-mermaid-diagrams：让hexo支持mermaid流程图（解决了我的第二个痛点）：https://www.cnblogs.com/moshuying/p/15801437.html\n  - hexo-blog-encrypt：加密文章，使用方法见github：https://github.com/rdou/hexo-blog-encrypt\n    - 说明：在http环境下不支持加密，这是因为脚本中的使用的crypto对象，有一个subtle属性只能在https环境下获取到。\n    - <img src=\"docker+hexo+gitee部署完美个人博客.assets/image-20221031001047545.png\" alt=\"image-20221031001047545\" style=\"zoom: 30%;float:left\" />\n    - <img src=\"docker+hexo+gitee部署完美个人博客.assets/image-20221031001405503.png\" alt=\"image-20221031001405503\" style=\"zoom:33%;float:left\" />\n- hexo-generator-search：实现本地搜索，hexo本地搜索的实现原理是在 public 目录下，生成一个 search.xml 文件\n- 设置git的邮箱和用户名，填自己的就行\n- 设置ssh公钥，是为了让git提交的时候不用每次都输入密码，直接用公钥提交\n- echo \"StrictHostKeyChecking no\" >> /etc/ssh/ssh_config \\： 这一步很重要，是为了容器启动的时候，跳过ssh公钥的检查，避免手动输入yes\n- cat /root/.ssh/id_rsa.pub：打印公钥\n\n\n\n\n\n## 第三步：构建docker镜像\n\n在Dockerfile的目录下，运行下面的命令\n\n```shell\ndocker build -t zhuansun/hexo:v1.0 .\n```\n\n- -t：表示指定镜像的名字和标签\n- zhuansun/hexo:v1.0 : 表示镜像的名字，v1.0是标签\n- . ：这个点，不知道啥意思，写上\n\n\n\n查看构建的镜像\n\n```shell\nash-4.3# docker image ls\nREPOSITORY                                          TAG                       IMAGE ID       CREATED         SIZE\nzhuansun/hexo                                       v1.0                      71b6e1b7ea19   4 minutes ago   249MB\n```\n\n\n\n\n\n## 第四步：运行容器\n\n我用的是群辉的docker，挂载了四个目录，和一个端口\n\n<img src=\"docker+hexo+gitee部署完美个人博客.assets/image-20221030222106418.png\" alt=\"image-20221030222106418\" style=\"zoom: 80%;\" />\n\n<img src=\"docker+hexo+gitee部署完美个人博客.assets/image-20221030222129007.png\" alt=\"image-20221030222129007\" style=\"zoom:50%;\" />\n\n其他的都是默认。然后启动\n\n注意：\n\n挂载的文件夹里面，要有内容哦。按照第一步，该放的都放好。\n\n\n\n## 第五步：启动容器\n\n<img src=\"docker+hexo+gitee部署完美个人博客.assets/image-20221030222250742.png\" alt=\"image-20221030222250742\" style=\"zoom:80%;\" />\n\n\n\n## 第六步：使用容器\n\n先开启群辉的ssh，然后进入到容器里面\n\n```sh\ndocker exec -it xxxxxxx sh\n```\n\n进来之后，默认就是我们设置的：/var/www/hexo\n\n```sh\nash-4.3# docker exec -it fb29ef2559ef sh\n/var/www/hexo # ls\n_config.landscape.yml  db.json                package-lock.json      public                 shell                  themes\n_config.yml            node_modules           package.json           scaffolds              source                 yarn.lock\n/var/www/hexo # hexo clean\nINFO  Validating config\nINFO  Deleted database.\nINFO  Deleted public folder.\n/var/www/hexo # hexo g\nINFO  Validating config\nINFO  Start processing\nINFO  Generated: note/JAVA/数据库/MYSQL/mysql中的交集差集并集/index.html\nINFO  Generated: note/JAVA/GIT/git批量删除分支/index.html\nINFO  Generated: note/PROJECT/时效项目/index.html\nINFO  73 files generated in 7.03 s\n/var/www/hexo # node shell/hexo-img-move.js \nsuccess!\nsuccess!\nsuccess!\n/var/www/hexo # hexo server -d\nINFO  Validating config\nINFO  \n[Browsersync] Access URLs:\n ----------------------------------\n          UI: http://localhost:3001\n ----------------------------------\n UI External: http://localhost:3001\n ----------------------------------\nINFO  Start processing\nINFO  Hexo is running at http://localhost:4000/ . Press Ctrl+C to stop.\n```\n\n- 说一下 node shell/hexo-img-move.js  这一步，hexo生成静态文件之后，只有html，并没有图片的\n- 如果你用的是图床，那么完全啥问题都没有\n- 如果是本地图片，我们就需要把本地图片移动到指定的文件夹中\n- 然后hexo server启动后，html才可以找到图片（具体怎么移动的可以看上面的代码）\n\n## 第七步：本地验证hexo服务\n\n图片可以正常显示\n\n<img src=\"docker+hexo+gitee部署完美个人博客.assets/image-20221030222950902.png\" alt=\"image-20221030222950902\" style=\"zoom:80%;\" />\n\n\n\n代码可以正常显示\n\n<img src=\"docker+hexo+gitee部署完美个人博客.assets/image-20221030223012143.png\" alt=\"image-20221030223012143\" style=\"zoom:80%;\" />\n\n\n\n时序图可以正常显示\n\n<img src=\"docker+hexo+gitee部署完美个人博客.assets/image-20221030223026695.png\" alt=\"image-20221030223026695\" style=\"zoom:80%;\" />\n\n## 第八步：部署到gitee\n\n首先需要配置上面的公钥到gitee上，百度很简单\n\n然后再hexo中：\n\n```sh\n/var/www/hexo # hexo d\nINFO  Validating config\nINFO  Deploying: git\nINFO  Clearing .deploy_git folder...\nINFO  Copying files from public folder...\nINFO  Copying files from extend dirs...\n[master c3fe339] Site updated: 2022-10-30 14:31:27\nEnumerating objects: 308, done.\nCounting objects: 100% (308/308), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (247/247), done.\nWriting objects: 100% (284/284), 22.02 MiB | 1.09 MiB/s, done.\nTotal 284 (delta 51), reused 0 (delta 0)\nremote: Resolving deltas: 100% (51/51), completed with 3 local objects.\nremote: Powered by GITEE.COM [GNK-6.4]\nTo gitee.com:zhuansunpengcheng/zhuansunpengcheng.git\n   e4984bb..c3fe339  HEAD -> master\nBranch 'master' set up to track remote branch 'master' from 'git@gitee.com:zhuansunpengcheng/zhuansunpengcheng.git'.\nINFO  Deploy done: git\n```\n\n提交成功之后，到gitee上，开始gitee page服务\n\n<img src=\"docker+hexo+gitee部署完美个人博客.assets/image-20221030223331923.png\" alt=\"image-20221030223331923\" style=\"zoom:50%;\" />\n\n\n\n然后就可以使用了\n\n<img src=\"docker+hexo+gitee部署完美个人博客.assets/image-20221030223426237.png\" alt=\"image-20221030223426237\" style=\"zoom:80%;\" />\n","tags":["docker","hexo","gitee"],"categories":["TOOLS","hexo"]},{"title":"黑群晖使用DNSPod设置外网访问","url":"/note/TOOLS/DSM/黑群晖使用DNSPod设置外网访问/","content":"\n# 黑群晖使用DNSPod设置外网访问\n\n\n\n我折腾群辉的 DDNS 已经很久了，之前使用的是花生壳，虽然能用，但是我觉得并不好用，所以使用了腾讯云的 DSNPod 做 DDNS\n\n\n\n## **步骤1:  注册域名**\n\n1) 登录 [**DNSPod 管理控制台**](https://console.dnspod.cn/)，进行购买\n\n2) 完成购买后，即可进入 [**我的域名**](https://console.dnspod.cn/dns/list) 管理页面查看您注册的域名。\n\n\n\n## **步骤2：启用 DDNS**\n\n1) 在 “[**我的域名**](https://console.dnspod.cn/dns/list)” 管理页面，单击您已注册的域名，即可进入【记录管理】页面。\n\n2) 单击【添加记录】，添加一条主机记录为 @，记录值为 0.0.0.0  的 A 记录。如下图所示：\n\n注意：记录值可以填写为任意IP地址，完成操作步骤后将会自动更新为您的公网IP地址。此处以 0.0.0.0记录值为例。\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223171928275.png\" alt=\"image-20230223171928275\" style=\"zoom: 67%;\" />\n\n\n\n3) 进入 [**密钥管理**](https://console.dnspod.cn/account/token) 页面，单击【创建密钥】，输入自定义的密钥名称后并单击【确定】。如下图所示：\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223172005486.png\" alt=\"image-20230223172005486\" style=\"zoom:80%;\" />\n\n\n\n4) 请妥善保管对话框中的 ID 与 Token。如下图所示：\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223172030142.png\" alt=\"image-20230223172030142\" style=\"zoom:80%;\" />\n\n\n\n5) 请使用具有管理员权限的账号登录您的群晖（Synology） NAS，依次单击【控制面板】>【外部访问】。如下图所示\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223172051836.png\" alt=\"image-20230223172051836\" style=\"zoom:80%;\" />\n\n\n\n6) 在【DDNS】页签中，单击【新增】。如下图所示：\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223172118673.png\" alt=\"image-20230223172118673\" style=\"zoom:80%;\" />\n\n\n\n7) 在弹出的对话框中的【服务供应商】选单内下拉选择【DNSPod.cn】,并填写相关信息。如下图所示：\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223172142806.png\" alt=\"image-20230223172142806\" style=\"zoom:80%;\" />\n\n- **主机名称：**填写您购买的域名。\n- **用户名/电子邮箱：**填写您获取到的DNSPod ID。\n- **密码/密钥：**填写您获取到的密钥。\n\n\n\n注意：您可单击【测试联机】，测试是否能成功联机。状态栏显示为正常，即代表成功联机。\n\n\n\n8) 单击【确定】。\n\n\n\n9) 单击【立即更新】，确认状态栏显示正常。如下图所示：\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223172238586.png\" alt=\"image-20230223172238586\" style=\"zoom:80%;\" />\n\n\n\n10) 返回 [**我的域名**](https://console.dnspod.cn/dns/list) 管理页面，查看记录值的是否已变更为您的公网 IP 地址，已变更为设置成功。未变更，请进行相关排查。\n\n\n\n**完成以上步骤，我们的DDNS就搭建好啦！**\n\n\n\n## 步骤3：配置外网访问\n\n\n\n一般家庭网络，公网ip都是禁用80端口和443端口的，所以要想配置外网访问，只能修改端口\n\n\n\n1.打开群辉的设置，选择【网络】，然后选择【DSM设置】，设置 http 端口和 https 端口\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223172711400.png\" alt=\"image-20230223172711400\" style=\"zoom:80%;\" />\n\n\n\n2.配置路由器端口转发\n\n我用的是小米路由器，在【高级设置】中【端口转发】，将内网的要转发的端口转发出去\n\n比如我这里的设置的就是：当从公网访问 1337 端口后，会转发到内网的 1337 端口上\n\n注意：这里我只配置了 https 的端口，因为我绑定了证书，可以使用 https 访问，更加安全；如果没有https，可以使用http\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223173032070.png\" alt=\"image-20230223173032070\" style=\"zoom: 67%;\" />\n\n\n\n3.这样，你就可以通过公网访问了。\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223173158744.png\" alt=\"image-20230223173158744\" style=\"zoom:67%;\" />\n\n\n\n## 步骤4：配置https安全访问\n\n要想配置 https 访问，首先必须要有 证书，证书可以直接取 DNSPod 控制台免费申请，一个账号可以免费申请 20本 证书，足够用了\n\n这里的步骤，简单的描述就是：\n\n- 申请证书\n- 把证书导入到群辉\n- 将证书设置为默认证书\n- 搞定\n\n具体的步骤可以直接参考DNSPod的官方文档：[群晖（Synology）NAS 安装免费 SSL 证书](#https://cloud.tencent.com/document/product/302/79821)\n\n\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223203835651.png\" alt=\"image-20230223203835651\" style=\"zoom:80%;\" />\n\n至此，HTTPS 安全访问就可以了，也就意味着，你可以通过公网，使用https访问你的群辉了。\n\n注意在路由器中，将你的 https 服务对应的端口号转发出去哦\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223173158744.png\" alt=\"image-20230223173158744\" style=\"zoom:67%;\" />\n\n## 步骤5：配置其他服务的HTTPS访问\n\n经过上面的配置，我们的群辉已经可以通过外网安全访问了。\n\n使用我们的域名：xxxx.xxx\n\n使用我们配置的https的端口：1337\n\n就可以在外网通过  xxxx.xxx:1337  愉快的访问群辉了。\n\n\n\n下面我们来看看：在群辉中，我有一个内网服务，怎么把这个内网服务映射到外网中。\n\n我以 firefox 为例子；配置一个可以外网访问的 firefox 浏览器，可以在外网访问家里内网的各种服务\n\n通过域名：firefox.xxxx.xxx:1337 访问内网的firefox浏览器，就像下面这样：\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223204238222.png\" alt=\"image-20230223204238222\" style=\"zoom:80%;\" />\n\n\n\n**在 docker 中安装 firefox**\n\n在 docker 中搜索 firefox，并安装：\n\ndocker 镜像：https://registry.hub.docker.com/r/jlesage/firefox/\n\ngithub网站：[GitHub - jlesage/docker-firefox: Docker container for Firefox](https://github.com/jlesage/docker-firefox)\n\n\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223204413138.png\" alt=\"image-20230223204413138\" style=\"zoom:80%;\" /> \n\n\n\n安装教程和相关文档，作者已经说得很清楚了。我这里再简单的描述一下\n\n本地新建一个文件夹：/docker/firefo/data\n\n然后配置docker\n\n**端口配置**：\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223204644619.png\" alt=\"image-20230223204644619\" style=\"zoom:80%;\" />\n\n**存储空间配置**：\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223204702767.png\" alt=\"image-20230223204702767\" style=\"zoom:80%;\" />\n\n**环境变量配置**：\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223204955490.png\" alt=\"image-20230223204955490\" style=\"zoom:80%;\" />\n\n然后启动docker\n\n因为字体包比较大， 所以启动的时间会比较慢，等，，我等了15分钟左右\n\n启动之后，在本地就可以通过 192.168.x.x:5888 端口访问 firefox 了；\n\n\n\n**配置外网访问**\n\n因为上面我们对群辉配置了 https 安全访问，所以 firefox 也要设置https安全访问\n\n通过作者的文档可以看到，firefox是支持https访问的\n\n只需要将配置项：SECURE_CONNECTION 设置为 1 就可以了\n\n设置为 1 之后，会自动生成 证书文件， 放在 config/certs 文件夹下面\n\n**但是呢**\n\n上面的方法，我亲身验证，其实并不需要这么做。 所以我们不需要设置：SECURE_CONNECTION ，让它保持默认值 0 就可以啦。\n\n那怎么配置呢？\n\n\n\n打开群辉，【设置】【Synology应用程序门户】【反向代理服务器】【新增】一个反向代理配置\n\n配置入图\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223205555072.png\" alt=\"image-20230223205555072\" style=\"zoom:80%;\" />\n\n到这里，群辉的配置就结束了。\n\n但是别忘了，要想使用 https 安全的访问firefox还需要什么？ 对头，还需要证书！\n\n\n\n所以我们要为我们的二级域名，也就是 ：firefox.xxxx.xxx 去单独申请一个证书。\n\n\n\n**申请二级域名证书**\n\n1) 在 “[**我的域名**](https://console.dnspod.cn/dns/list)” 管理页面，单击您已注册的域名，即可进入【记录管理】页面。\n\n2) 单击【添加记录】，添加一条主机记录为 firefox，记录值为 xxxx.xxx(这里是指你的域名)  的 CNAME 解析记录。如下图所示：\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223205841510.png\" alt=\"image-20230223205841510\" style=\"zoom:80%;\" />\n\n\n\n然后在这个记录后面，去申请 SSL 证书，申请方法和上面讲的一样，这里不再赘述；\n\n证书申请下来之后\n\n按照DNSPod的官方文档：[群晖（Synology）NAS 安装免费 SSL 证书](#https://cloud.tencent.com/document/product/302/79821)，导入到群辉中，**注意，只是导入进来， 不需要任何配置**\n\n也就是说，步骤是：\n\n- 申请证书\n- 把证书导入到群辉\n- ~~将证书设置为默认证书（**这一步可千万不要做**）~~\n- 搞定\n\n\n\n\n\n搞定之后，我们来到群辉中，打开【设置】【安全性】【证书】在进行接下来的配置\n\n按照图片上的内容，进行如下的配置\n\n<img src=\"黑群晖使用DNSPod设置外网访问.assets/image-20230223210520238.png\" alt=\"image-20230223210520238\" style=\"zoom:80%;\" />\n\n\n\n等你配置完之后，就完结撒花了。\n\n\n\n在浏览器中，就可以通过 https://firefox.xxx.xx:1337 访问你的firefox火狐浏览器啦。\n\n\n\n举一反三：其他任何的内网服务，都可以通过这样的步骤，暴露到公网中。 \n\n所以，每一个二级域名，都要单独申请一本证书，还是挺麻烦的。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["dsm","群辉","ddns"],"categories":["TOOLS","DSM"]},{"url":"/note/MYSELF/群辉配置外网访问/","content":"\n\n\n\nfirefox\n\n\n\n直接把 .key 改成 pem 就行啦"},{"title":"kafka从入门到入土","url":"/note/JAVA/消息中间件/KAFKA/kafka从入门到入土/","content":"\n\n\n# kafka从入门到入土\n\n---\n\n\n\n## 基本概念\n\n\n\n### 名词术语\n\n消息：`Record`。Kafka 是消息引擎嘛，这里的消息就是指 Kafka 处理的主要对象。\n\n主题：`Topic`。Topic 是承载消息的逻辑容器，在实际使用中多用来区分具体的业务。\n\n分区：`Partition`。是一个物理概念，可以理解为一个有序不变的消息序列。每个 Topic 下可以有多个 Partition。\n\n分区位移：`Offset`。表示 Partition 中每条消息的位置信息，这个值是存在消息中的，是一个单调递增且不变的值。\n\n副本：`Replica`。Kafka 中同一条消息能够被拷贝到多个地方以提供数据冗余，这些地方就是所谓的`Replica`副本。`Replica`还分为`Leader Replica`和`Follower Replica`，各自有不同的角色划分。`Replica`是在`Partition`层级下的，即每个`Partition`可配置多个`Replica`实现高可用。\n\n生产者：`Producer`。向`Topic`发布新消息的应用程序。\n\n消费者：`Consumer`。从`Topic`订阅新消息的应用程序。\n\n消费者位移：`Consumer Offset`。表示`Consumer`的消费进度，每个`Consumer`都有自己的`Consumer Offset`。\n\n消费者组：`Consumer Group`。多个`Consumer`实例共同组成的一个组`Group`，同时消费多个`Partition`以实现高吞吐。\n\n重平衡：`Rebalance`。`Consumer Group`内某个`Consumer`实例挂掉后，其他`Consumer`实例自动重新分配订阅`Topic Partiton`的过程。`Rebalance` 是 Kafka 消费端实现高可用的重要手段。\n\n<img src=\"kafka从入门到入土.assets/58c35d3ab0921bf0476e3ba14069d291.jpg\" alt=\"img\" style=\"zoom: 20%;\" />\n\n\n\n### 三层消息架构\n\n- 第一层：主题层`Topic`\n  - 每个`Topic`可以配置`M`的`Partition`，而每个`Partition`又可以配置`N`个`Replica`\n- 第二层：分区层`Partition`\n  - 每个`Partition`下的`N`个`Replica`中，只能有一个充当`Leader Replica`，`Leader Replica`负责对外提供服务；\n  - 剩下的`N-1`个`Replica`，都是作为`Follower Replica`，`Follower Replica`只是作为数据冗余，不对外提供服务；\n- 第三层：消息层\n  - 每个`Partition`中包含若干消息，每个消息的`Offset`（注意不是`Consumer Offset`）都是从0开始，依次递增；\n\n\n\n### 数据持久化（Log）\n\n`kafka`使用消息日志`Log`来保存数据，一个`Log`就是磁盘上一个只能追加写消息的物理文件。\n\n一个`Log`包含了多个日志段`Log Segment`，消息其实是被追加写到最新的`Log Segment`中的；\n\n当写满一个`Log Segment`的时候，会自动切分一个新的`Log Segment`中，老的`Log Segment`就会被封存；\n\n`kafka`会有一个定时任务，定期检查老的`Log Segment`是否能够被删除，从而释放磁盘空间；\n\n\n\n### 两种消息模型\n\n点对点`peer to peer`\n\n- 同一个消息只能被下游的一个`Consumer`消费；\n- kafka实现点对点，用到的是`Consumer Group`的概念\n\n发布订阅模型`pub/sub`\n\n- 我们常用到的其实就是这种发布订阅模型\n\n\n\n## 发展历史和定位\n\n\n\nkakka既是一个消息引擎系统，同时又是一个分布式流处理平台；\n\n\n\n### 发展历史\n\n- 是`Linkedln`公司内部的孵化项目。\n- `Linkedln`一开始是有 数据强实时性处理方面的需求，用了`activeMq`，但不理想，所以准备自己搞一套。\n- `Kafka` 自诞生伊始是以**消息引擎系统**的面目出现在大众视野中的。如果翻看 `0.10.0.0` 之前的官网说明，你会发现 `Kafka` 社区将其清晰地定位为一个分布式、分区化且带备份功能的提交日志`Commit Log`服务。\n- `Kafka`在设计之初提供三个方面的特性：\n  - 提供一套 API 实现`Producer`和`Consumer`；\n  - 降低网络传输和磁盘存储开销；\n  - 实现高伸缩性架构。\n\n- 后来用的人越来越多，`kafka`思考引入了流处理；\n- `Kafka` 社区于 `0.10.0.0` 版本正式推出了流处理组件 `Kafka Streams`，也正是从这个版本开始，`Kafka` 正式“变身”为分布式的流处理平台，而不仅仅是消息引擎系统了。\n\n\n\n### 与其他的流处理框架的优点\n\n- 第一点是更容易实现端到端的正确性`Correctness`\n- `kafka`自己对于流式计算的定位\n\n\n\n### 定位\n\n- 消息引擎系统\n- 流处理平台\n- 分布式存储系统（很少）\n\n\n\n## kafka版本\n\n### 发行版本\n\n`kafka`存在多个不同的发行版本，类似`linux`系统中的`centos`，`redhat`，`ununtu`等；\n\n| 类型                  | 描述                                                         | 优点                                                         | 缺点                                                         | 选择                                                         |\n| --------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| apache kafka          | Apache Kafka 是最“正宗”的 Kafka，是我们学习和使用 Kafka 的基础。 | 优势在于迭代速度快，社区响应度高，使用它可以让你有更高的把控度 | 缺陷在于仅提供基础核心组件，缺失一些高级的特性。             | 如果你仅仅需要一个消息引擎系统亦或是简单的流处理应用场景，同时需要对系统有较大把控度，那么我推荐你使用 Apache Kafka。 |\n| Confluent Kafka       | Confluent 公司：2014 年，Kafka 的 3 个创始人 Jay Kreps、Naha Narkhede 和饶军离开 LinkedIn 创办了 Confluent 公司，专注于提供基于 Kafka 的企业级流处理解决方案。Confluent Kafka 提供了一些 Apache Kafka 没有的高级特性，比如跨数据中心备份、Schema 注册中心以及集群监控工具等。 | 优势在于集成了很多高级特性且由 Kafka 原班人马打造，质量上有保证； | 缺陷在于相关文档资料不全，普及率较低，没有太多可供参考的范例。 | 如果你需要用到 Kafka 的一些高级特性，那么推荐你使用 Confluent Kafka。 |\n| CDH Kafka / HDP Kafka | Cloudera 提供的 CDH 和 Hortonworks 提供的 HDP 是非常著名的大数据平台，里面集成了目前主流的大数据框架，能够帮助用户实现从分布式存储、集群调度、流处理到机器学习、实时数据库等全方位的数据处理，不管是 CDH 还是 HDP 里面都集成了 Apache Kafka，因此我把这两款产品中的 Kafka 称为 CDH Kafka 和 HDP Kafka。 | 操作简单，节省运维成本                                       | 把控度低，演进速度较慢。                                     | 如果你需要快速地搭建消息引擎系统，或者你需要搭建的是多框架构成的数据平台且 Kafka 只是其中一个组件，那么我推荐你使用这些大数据云公司提供的 Kafka。 |\n\n\n\n### 版本号\n\n\n\n在官网上下载 `Kafka` 时，会看到这样的版本：\n\n<img src=\"kafka从入门到入土.assets/c10df9e6f72126e9c721fba38e27ac23.png\" alt=\"img\" style=\"zoom:80%;\" />\n\n有些人会误将`Scala`版本看作是`Kafka`版本，那么就来解释一下这个版本号\n\n- `2.11/2.12`：代表着`Kafka`源代码的`Scala`编译器版本\n\n- `2.3.0`：才是Kafka的版本号，`2`代表着大版本号；`3`代表着小版本号；`0`代表着修订版本号或补丁\n\n\n\nKafka目前经历了7个大版本，0.7、0.8、0.9、0.10、0.11、1.0和2.0，其中小版本与Patch版本很多就不一一列举\n\n在上面的7个大版本中，在哪个版本进行了重大的改进，来好好看一下\n\n<img src=\"kafka从入门到入土.assets/Kafka版本变迁.png\" alt=\"img\" style=\"zoom:80%;\" />\n\n#### 0.7版本\n\n这是个“上古”版本，只提供了基础的消息队列功能，还没有提供副本机制\n\n#### 0.8版本\n\n正式引入了副本机制，能够比较好地做到消息无丢失，新版本Producer API不稳定\n\n#### 0.9版本\n\n添加了基础的安全认证/权限；新版本Producer API在这个版本中算比较稳定，但是0.9版的Consumer API BUG超多，即使提到社区也不会有人管，所以千万别用！\n\n#### 0.10版本\n\n是里程碑式的大版本，因为该版本引入了Kafka Streams，但还不能生产大规模部署使用，自0.10.2.2版本起，新版本Consumer API算是比较稳定了\n\n#### 0.11版本\n\n引入了两个重量级的功能变更：一个是提供幂等性Producer API以及事务（Transaction） API；另一个是对Kafka消息格式做了重构\n\nProducer实现幂等性以及支持事务都是Kafka实现流处理结果正确性的基石，由于刚推出，事务API有一些Bug，另外事务API主要是为Kafka Streams应用服务的，不建议用\n\n这个版本中各个大功能组件都变得非常稳定了，国内该版本的用户也很多，应该算是目前最主流的版本之一了\n\n如果你对1.0版本是否适用于线上环境依然感到困惑，那么至少将你的环境升级到0.11.0.3，因为这个版本的消息引擎功能已经非常完善了\n\n#### 1.0/2.0版本\n\n合并说下1.0和2.0版本吧，因为这两个大版本主要还是Kafka Streams的各种改进，在消息引擎方面并未引入太多的重大功能特性\n\nKafka Streams的确在这两个版本有着非常大的变化，也必须承认Kafka Streams目前依然还在积极地发展着，如果你是Kafka Streams的用户，至少选择2.0.0版本吧\n\n#### 建议\n\n不论你用的是哪个版本，都请尽量保持服务器端版本和客户端版本一致，否则你将损失很多Kafka为你提供的性能优化收益\n\n\n\n## kafka生产集群部署\n\n上面了解了kafka的基本概念之后，下面看一下kafka的生产部署，需要怎么部署\n\n同时在本小节之后，我们会搭建一个简单的kafka集群，用于后续的学习\n\nkafka的集群搭建需要考虑一下几个因素\n\n| 因素     | 考量点                                  | 建议                                                         |\n| -------- | --------------------------------------- | ------------------------------------------------------------ |\n| 操作系统 | 操作系统的IO模型                        | 将kafka部署在linux上                                         |\n| 磁盘     | 磁盘的IO性能                            | 普通环境使用机械硬盘，不需要搭建RAID                         |\n| 磁盘容量 | 根据消息数，留存时间预估磁盘容量        | 实际使用中磁盘预留20%~30%的空间                              |\n| 带宽     | 根据实际带宽资源和业务SLA预估服务器数量 | 对于千兆网络，建议每台服务器按照700Mbps来计算，避免大流量下的丢包 |\n\n\n\n### 操作系统的选择\n\n操作系统：Windows，Linux，MacOs\n\n选择：Linux\n\n为什么：\n\n- IO模型的使用\n- 网络传输效率\n- 社区支持度\n\n#### IO模型的使用\n\n- 阻塞IO\n- 非阻塞IO\n- IO多路复用\n- 信号驱动IO\n- 异步IO\n\n每种IO都有自己的典型使用场景，比如：\n\n- Java中的Socket对象的阻塞模式和非阻塞模式就是对应前两种\n- Linux系统的select函数就属于IO多路复用\n- 大名鼎鼎的epoll介入第三种和第四种之间\n- 第五种模型，目前很少有Linux支持，然而Windos却在操作系统中提供了叫IOCP线程模型属于第五种\n\n说完了IO模型，再来看kafka与IO模型的关系\n\n- kafka的底层使用的是java的selector\n  - java的selector在linux上的实现机制是：epoll\n  - 而在windos上的实现机制是：select（IO多路复用）\n- 所以，将kafka部署在linux机器上，更有优势\n\n\n\n#### 网络传输效率\n\nkafka的消息是通过网络传输的，而消息又是保存在磁盘中的，所以kafka非常依赖网络和磁盘的性能；\n\n而linux恰巧有零拷贝（Zero copy）技术，就是当数据在磁盘和网络进行传输的时候，避免昂贵的的内核态数据拷贝从而实现数据的高速传输；\n\n而windos要到java8的60更新版本才有这个功能；\n\n\n\n#### 社区的支持度\n\n社区对于windos版的bug不做承诺，基本不会修复；\n\n\n\n### 磁盘的选择\n\n- 选择机械磁盘：kafka多为顺序读写，规避了机械磁盘的弊端，替换成SSD，效益不大\n- 不用组RAID：kafka在软件层面通过分区副本保证了高可用，基本不需要磁盘组RAID\n\n\n\n### 磁盘容量的选择\n\n- 磁盘容量：kafka的日志有保留时间的概念，根据具体的业务量，消息大小，计算好容量；\n\n  - 新增消息量\n\n  - 消息留存时间\n\n  - 平均消息大小\n\n  - 备份数\n\n  - 是否启用压缩（压缩比）\n\n\n\n### 带宽的选择\n\n目前公司普遍的带宽配置都是千兆网（每秒处理1G数据），财大气粗的公司会有万兆网（每秒处理10G数据）；\n\n假设你公司的机房环境是千兆网络，即 1Gbps，现在你有个业务，其业务目标或 SLA 是在 1 小时内处理 1TB 的业务数据。那么问题来了，你到底需要多少台 Kafka 服务器来完成这个业务呢？\n\n千兆网络下，单台机器，假设kafka占用70%的带宽（总要为其他进程保留一些资源），稍等，这只是它能使用的最大带宽资源，你不能让 Kafka 服务器常规性使用这么多资源，故通常要再额外预留出 2/3 的资源，即单台服务器使用带宽 700Mb / 3 ≈ 240Mbps。有了 240Mbps，我们就可以计算 1 小时内处理 1TB 数据所需的服务器数量了。根据这个目标，我们每秒需要处理 2336Mb 的数据，除以 240，约等于 10 台服务器。如果消息还需要额外复制两份，那么总的服务器台数还要乘以 3，即 30 台。\n\n\n\n\n\n## 重要的集群参数配置\n\n\n\n参数配置分为四个方面：\n\n- broker端参数配置\n- topic的参数配置\n- JVM的参数配置\n- 操作系统的参数配置\n\n\n\n### broker端参数（静态参数）\n\n静态参数是指修改后需要重启才能生效的参数；\n\n是配置在 kafka安装的这个机器上的。通过静态的配置文件配置的。\n\n\n\n#### 存储信息类参数\n\n表示 Broker 使用哪些磁盘\n\n| 参数     | 描述                                                         |\n| -------- | ------------------------------------------------------------ |\n| log.dirs | 【没有默认值的，必须手动指定】指定Broker需要使用的若干个文件目录路径，可配置多个 |\n| log.dir  | 【一般不用设置，新版本已经取消了】只能配置一个，用来补充上面参数的 |\n\n\n\n#### 与ZK相关的参数\n\nZK负责协调管理并保存 Kafka 集群的所有元数据信息，比如集群都有哪些 Broker 在运行、创建了哪些 Topic，每个 Topic 都有多少分区以及这些分区的 Leader 副本都在哪些机器上等信息\n\n| 参数              | 描述                                          |\n| ----------------- | --------------------------------------------- |\n| zookeeper.connect | 负责协调管理并保存 Kafka 集群的所有元数据信息 |\n\n\n\n#### broker连接相关的参数\n\n表示客户端程序或其他 Broker 如何与该 Broker 进行通信的设置\n\n| 参数                 | 描述                                                         |\n| -------------------- | ------------------------------------------------------------ |\n| listeners            | 告诉外部连接需要通过什么协议访问指定主机名和端口开放的kafka服务（用于内网访问） |\n| Advertised.listeners | 表明这组监听器是broker对外发布的（用于外网访问）             |\n| host.name/port       | 这俩参数是过期参数，忘掉                                     |\n\n\n\n#### topic管理的参数\n\n\n\n| 参数                           | 描述                                                         |\n| ------------------------------ | ------------------------------------------------------------ |\n| auto.create.topics.enable      | 是否允许自动创建topic，<br />建议设置成false；               |\n| unclean.leader.election.enable | 是否允许Unclean Leader选举，<br />建议设置为false；<br />kafka的分区有多个副本，并不是所有的副本都有资格竞争Leader，只有保存数据比较多的才有资格；那如果保存数据比较多的副本全都挂了，那还要不要竞选Leader呢？ 就是这个参数控制的；<br />false表示不竞选，后果：分区不可用；<br />true表示竞选；后果：数据不一致； |\n| auto.leader.rebalance.enable   | 是否允许定期进行Leader选举；true表示到达一定条件，kafka会自动把leader换了，注意是换掉，而不是选举；即使原来的leaderA运行的好好地，也会给换成leaderB；换leader的代价很大，建议设置为false； |\n\n\n\n#### 数据留存方面的参数\n\n| 参数                              | 描述                                 |\n| --------------------------------- | ------------------------------------ |\n| log.retention.{hour\\|minutes\\|ms} | 控制一条消息被保留多长时间           |\n| log.retention.bytes               | Broker为保留消息提供的磁盘容量的大小 |\n| message.max.bytes                 | 控制Broker能够接收的最大的消息大小   |\n\n\n\n### Topic的参数配置\n\ntopic端的参数配置会覆盖broker端的参数配置\n\nTopic 端的参数是在创建Topic的时候，手动设置的。[怎么修改topic的参数配置](#怎么修改topic的参数配置)\n\n\n\n#### 数据留存方面的参数\n\n| 参数              | 描述                                                         |\n| ----------------- | ------------------------------------------------------------ |\n| retention.ms      | 规定了该topic下数据的保存时长，默认7天，如果配置了，就会覆盖broker端的配置 |\n| retention.bytes   | 规定了要为该topic预留多少磁盘容量空间                        |\n| max.message.bytes | 该参数跟 Broker 端的 message.max.bytes 参数的作用是一样的，只不过 max.message.bytes 是作用于某个 topic，而 message.max.bytes 是作用于全局。 |\n\n\n\n#### 怎么修改topic的参数配置\n\n- 创建topic的时候设置\n\n  - > bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic transaction --partitions 1 --replication-factor 1 --config retention.ms=15552000000 --config max.message.bytes=5242880\n\n- 修改topic的时候设置\n\n  - > bin/kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name transaction --alter --add-config max.message.bytes=10485760\n\n\n\n\n\n### JVM的参数配置\n\n设置kafka的JVM参数，只需要设置环境变量就可以啦。[怎么对kafka设置JVM参数](#怎么对kafka设置JVM参数)\n\n\n\n| 参数                       | 描述                                                         |\n| -------------------------- | ------------------------------------------------------------ |\n| KAFKA_HEAP_OPTS            | JVM堆大小，建议设置为6GB，默认的1GB太小了                    |\n| KAFKA_JVM_PERFORMANCE_OPTS | 指定垃圾回收器<br>在java7下：cpu充足，就用CMS；否则使用ParallelGC<br>在java8下：选择G1 |\n\n\n\n#### 怎么对kafka设置JVM参数\n\n指定kafka的环境变量即可\n\n- KAFKA_HEAP_OPTS：指定堆大小\n- KAFKA_JVM_PERFORMANCE_OPTS：指定垃圾回收器\n\n```shell\n\n$> export KAFKA_HEAP_OPTS=--Xms6g  --Xmx6g\n$> export KAFKA_JVM_PERFORMANCE_OPTS= -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true\n$> bin/kafka-server-start.sh config/server.properties\n```\n\n\n\n### 操作系统的参数配置\n\n\n\n| 参数                       | 描述                                                         |\n| -------------------------- | ------------------------------------------------------------ |\n| 文件描述符限制             | ulimit -n；其实设置这个参数不重要，但是不设置后果很严重，会看到too many open file 的报错； |\n| 文件系统类型               | 文件系统类型（ext3，ext4，XFS），XFS的性能强于ext4，ZFS的性能强于XFS（但技术比较新，使用很少） |\n| Swappiness                 | 网上很多文章都提到设置其为 0，将 swap 完全禁掉以防止 Kafka 进程使用 swap 空间。我个人反倒觉得还是不要设置成 0 比较好，我们可以设置成一个较小的值。为什么呢？因为一旦设置成 0，当物理内存耗尽时，操作系统会触发 OOM killer 这个组件，它会随机挑选一个进程然后 kill 掉，即根本不给用户任何的预警。但如果设置成一个比较小的值，当开始使用 swap 空间时，你至少能够观测到 Broker 性能开始出现急剧下降，从而给你进一步调优和诊断问题的时间。基于这个考虑，我个人建议将 swappniess 配置成一个接近 0 但不为 0 的值，比如 1。 |\n| 提交时间（系统的刷盘时间） | 提交时间或者说是 Flush 落盘时间。向 Kafka 发送数据并不是真要等数据被写入磁盘才会认为成功，而是只要数据被写入到操作系统的页缓存（Page Cache）上就可以了，随后操作系统根据 LRU 算法会定期将页缓存上的“脏”数据落盘到物理磁盘上。这个定期就是由提交时间来确定的，默认是 5 秒。一般情况下我们会认为这个时间太频繁了，可以适当地增加提交间隔来降低物理磁盘的写操作。当然你可能会有这样的疑问：如果在页缓存中的数据在写入到磁盘前机器宕机了，那岂不是数据就丢失了。的确，这种情况数据确实就丢失了，但鉴于 Kafka 在软件层面已经提供了多副本的冗余机制，因此这里稍微拉大提交间隔去换取性能还是一个合理的做法。 |\n\n\n\n\n\n## 分区机制\n\n\n\n对于那种大批量机器组成的集群环境，每分钟产生的日志量都能以 GB 数，因此如何将这么大的数据量均匀地分配到 Kafka 的各个 Broker 上，就成为一个非常重要的问题。\n\n我们知道 kafka 的数据，是以 Topic 为概念进行存储的，而`topic`是一个逻辑概念，真正存放数据的是`topic`下的`partition`；`partition`是物理概念；\n\n一个消息只会保存在一个`topic`下的一个`partition`中，不会保存在多个`partition`中（`Replica`除外）\n\n那么，为了保证大数据量的均匀分布，其实就是保证一个`topic`下的数据量均匀的分散在各个`partition`中；\n\n那么问题来了？\n\n\n\n\n\n### 为什么要分区\n\n为什么要`Partiton`，为什么`kafka`不直接存储数据，而是要分区存储？为什么要使用`Partiton`，而不是直接使用`topic`？\n\n分区的目的是为了**负载均衡**；或者说分区的目的是为了**提高系统的可伸缩性**；\n\n- 负载均衡\n  - 如果没有分区，所有的请求全部在一个`topic`上，请求量大的时候，只对一个磁盘进行大量的读写（分钟`GB`级别的数据量），可能直接就崩了；\n- 可伸缩性\n  - 顺丰的`kafka`一般是32分区，这样每一个`Partition`都可以有一个`consumer`，提升系统的吞吐量；当数据量增长的时候，可以扩`Partition`，32->64；提升系统的可伸缩性；\n  - 但是一般不建议直接扩`Partition`，在顺丰，一般是申请新的`topic`，然后将消息转发到不同的`topic`中，变相的实现扩`Partition`；\n  - 因为`Partition`过多，`kafka`管理起来很困难，没必要增加不必要的消耗；\n- `Partition`可以实现业务上的功能（消息的顺序问题）\n\n\n\n以上说了`Partiton`存在的必要性\n\n那么既然存在`Partiton`，怎么保证每个`Partiton`的数据量的均匀呢，避免数据倾斜？这就涉及到分区的策略\n\n\n\n### 分区策略\n\n分区策略：就是决定消息被发送到哪个分区\n\n| 分区策略         | 描述                                                         |\n| ---------------- | ------------------------------------------------------------ |\n| 轮训             | 没有指定`partitioner.class`这个配置的时候，在没有指定key的时候（消息键保留策略），轮训策略是兜底的 |\n| 随机             | 使用的很少了，已经被废弃了                                   |\n| 自定义           | 需要显示的配置`partitioner.class`这个配置，同时需要编写代码； |\n| 按消息键保留策略 | 按照key的顺序进行存放                                        |\n\n\n\n默认分区策略：如果指定了key，按照key分发；没有指定key，按照轮训；\n\n\n\n### 怎么设置分区策略\n\n**轮训**\n\n```java\n不需要配置，默认的就是这个。\n```\n\n\n\n**随机**\n\n```java\nList<PartitionInfo> partitions = cluster.partitionsForTopic(topic);\nreturn ThreadLocalRandom.current().nextInt(partitions.size());\n```\n\n\n\n**按消息键保序策略**\n\n```java\nList<PartitionInfo> partitions = cluster.partitionsForTopic(topic);\nreturn Math.abs(key.hashCode()) % partitions.size();\n```\n\n\n\n**自定义**\n\n比如我想实现：根据 `Broker` 所在的 `IP` 地址判断是南方还是北方，实现定制化的分区策略\n\n- 编写一个具体的类实现`org.apache.kafka.clients.producer.Partitioner`接口\n- 实现其中的两个方法：`partition()`和`close()`\n- 显式地配置生产者端的参数`partitioner.class`为你自己实现类的 `Full Qualified Name`\n\n```java\nList<PartitionInfo> partitions = cluster.partitionsForTopic(topic);\nreturn partitions.stream().filter(p -> isSouth(p.leader().host())).map(PartitionInfo::partition).findAny().get()\n```\n\n\n\n## 消息压缩（消息格式）\n\n\n\n### 为什么要压缩？\n\n说起压缩`compression`，我相信你一定不会感到陌生。它秉承了用时间去换空间的经典 `trade-off` 思想，具体来说就是用 CPU 时间去换磁盘空间或网络 I/O 传输量，希望以较小的 CPU 开销带来更少的磁盘占用或更少的网络 I/O 传输。在 `Kafka` 中，压缩也是用来做这件事的。\n\n\n\n### kafka的消息格式\n\nkafka有两大类消息格式，一类是在`0.11.0.0`版本之前的消息格式（称作V1版本），一个是`0.11.0.0`版本之后的格式（称作V2版本）；\n\n不管是哪个版本，kafka消息层次都是分为两层：\n\n| V1版本                                    | V2版本                                    |\n| ----------------------------------------- | ----------------------------------------- |\n| 消息集合（message set） + 消息（message） | 消息集合（record batch） + 消息（record） |\n\n一个消息集合中包含若干个日志项`record item`，日志项`record item`才是真正封装消息的地方；（注意这里不要和日志段（`Log Segment`）混为一谈）\n\nV2版本对V1版本进行了优化，将日志项`record item`中一些通用的字段抽出来，放在了消息集合中；\n\nV2版本对V1版本还有一个关于压缩方面的优化\n\n\n\n### 怎么压缩\n\nV2 版本对 V1 版本还有一个关于压缩方面的优化\n\nV1 版本：是把多条消息进行压缩，然后将压缩后的内容放在外层消息的消息体字段中； \n\nV2 版本：是对整个消息集合进行压缩，显然V2版本的压缩效率应该更高；\n\n压缩使用到的是压缩算法：[压缩算法的选择](#压缩算法的选择)\n\n\n\n### 何时压缩\n\n在 `kafka` 中，压缩可能发生在：`Producer`端和`Broker`端\n\n- `Producer`端【一般都是`Producer`端做压缩】\n  - 在`Producer`程序中添加一个配置：`compression.type` 参数\n  - compression.type=gzip 表示开启gzip压缩\n  \n- `Broker`端\n  - 一般`Broker`端不会对`Producer`发出来的消息进行修改；\n  - 有两个例外情况，会让`Broker`对消息重新压缩\n    - `Broker`端和`Producer`端指定的消息压缩算法不一致（不一致的时候，`broker`端会对`producer`端发出来的消息解压然后重新压缩）\n    - `Broker`端发生了消息格式转换：新老版本消息格式（V1版本和V2版本）兼容的问题\n\n\n\n### 何时解压缩\n\n- `consumer`端消费到消息的时候，进行解压缩\n  - 解压缩的时候，压缩算法是在消息中，用一个字段标识的，所以`consumer`可以拿到消息之后在解压缩\n- `broker`端收到`producer`发出的消息之后，也会解压缩一次，进行消息的校验；\n\n\n\n### 压缩的时机\n\n记住这句话：`Producer`端压缩，`Broker`端保持，`Consumer`端解压缩\n\n\n\n### 压缩算法的选择\n\n一般看两个指标：压缩比 和 压缩/解压缩的吞吐量\n\nGZIP\n\nSnappy\n\nLZ4\n\nzstd\n\n\n\n## 消息丢失\n\n`kafka`只对 已提交成功 的消息做有限度的持久化保证；\n\n\n\n### 什么是消息丢失\n\n对于`Producer`来说：消息发不出去，就是丢失；\n\n对于`Consumer`来说：消息消费不到，就是丢失；\n\n对于`Broker`来说：不存在丢失，`Broker`会对 已提交成功 的消息，做有限度的持久化；\n\n\n\n### 什么时候会消息丢失\n\n- `Producer`丢失消息\n  - `producer.send(msg) `因为是异步，`fire and forget` 所以可能会丢消息\n  - 网络抖动、消息不合法被`broker`拒收（比如：消息体太大）等都会导致消息发送不成功\n- `Consumer`丢失消息\n  - 消费的消息不存在了。一般只有先提交`offset`在消费的场景下会发生；\n  - 多线程处理消息的时候，某一个线程消费失败了，但是`offset`自动提交了；\n\n\n\n### 怎么保证消息不丢失\n\n上面几种丢失消息的场景，怎么避免？\n\n- 【`Producer`端】：不要使用 `producer.send(msg)`，而要使用 `producer.send(msg, callback)`。一定要使用带有回调通知的 `send `方法。\n- 【`Producer`端】：设置 `acks` = `all`。是个动态值（如果原来有`3`个`Replica`，就要写入`3`个，如果有`1`个挂了，那就只需要写入`2`个），表明所有`Replica`都要接收到消息，该消息才算是“已提交”。这是最高等级的“已提交”定义。\n- 【`Producer`端】：设置 `retries` 为一个较大的值。 表示`Producer `自动重试。当出现网络的瞬时抖动时，消息发送可能会失败，这里开启自动重试，避免消息丢失。（重试会导致消息乱序吗：会）\n- 【Broker端】：设置 `unclean.leader.election.enable` = `false`。它控制的是哪些 `Replica `有资格竞选分区的 `Leader`。如果一个 `Replica ` 落后原先的 `Leader `太多，就不要让它竞选，即不允许这种情况的发生。\n- 【Broker端】：设置` replication.factor` >= `3`。表示某个分区的`Replica`总数，最好将消息多保存几份，毕竟目前防止消息丢失的主要机制就是冗余\n- 【Broker端】：设置 `min.insync.replicas` > 1。表示至少写入多少个`Replica`才算是“已提交”。设置成大于 1 可以提升消息持久性。在实际环境中千万不要使用默认值 1。\n- 【Broker端】：确保` replication.factor` > `min.insync.replicas`。如果两者相等，那么只要有一个副本挂机，整个分区就无法正常工作了。我们不仅要改善消息的持久性，防止数据丢失，还要在不降低可用性的基础上完成。推荐设置成 `replication.factor` = `min.insync.replicas` + `1`。\n- 【`Consumer`端】：设置`enable.auto.commit`= `false`，采用手动提交位移的方式。就像前面说的，这对于单 `Consumer `多线程处理的场景而言是至关重要的。\n\n\n\n举个例子：\n\n比如`Replica`=`3`，设置`min.insync.replicas`=`2`，`acks`=`all`\n\n如果`Replica`都正常工作：此时`acks`=`all`的约束就是写入`3`个`Replica`，才算提交成功，此时满足`min.insync.replicas`=`2`约束。\n\n如果`Replica`挂了`1`个，此时`acks`=`all`的约束就是写入`2`个`Replica`即可，此时满足`min.insync.replicas`=`2`约束。\n\n如果`Replica`挂了`2`个，此时`acks`=`all`的约束就是写入`1`个`Replica`即可，此时不满足`min.insync.replicas`=`2`这个下限约束，写入失败。\n\n补充：\n\n> 设置 ack = all，其实就是需要保证 ISR 集合中所有的 Replica 都写入成功才能返回\n\n\n\n公司的生产者的设置\n\n```java\n    public KafkaProducer24(String brokers, ProduceOptionalConfig extraConfig) {\n        this.extraConfig = extraConfig == null ? ProduceOptionalConfig.defaultConfig : extraConfig;\n        Properties props = new Properties();\n        props.put(\"bootstrap.servers\", brokers);\n        props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n        props.put(\"acks\", \"默认是Leader Replica收到就行\");\n        props.put(\"request.timeout.ms\", \"默认是30000\");\n        props.put(\"compression.type\", \"snappy\");\n        props.put(\"batch.size\", \"默认是16384\");\n        props.put(\"linger.ms\", \"默认是5\");\n        this.producer = new KafkaProducer(props);\n    }\n```\n\n\n\n\n\n## 拦截器\n\nKafka 拦截器自` 0.10.0.0` 版本被引入后并未得到太多的实际应用，我也从未在任何 Kafka 技术峰会上看到有公司分享其使用拦截器的成功案例。\n\n拦截器是一个小众功能。\n\nKafka 拦截器分为生产者拦截器和消费者拦截器\n\n\n\n### 生产者拦截器\n\n开发：实现`org.apache.kafka.clients.producer.ProducerInterceptor`这个接口，这个接口有俩方法\n\n- `onSend`：消息真正发给broker之前\n- `onAcknowledgement`：消息提交成功之后，在`callback`之前\n\n\n\n### 消费者拦截器\n\n开发：实现`org.apache.kafka.clients.consumer.ConsumerInterceptor`这个接口，这个接口有俩方法\n\n- `onConsume`：在消费者真正处理消息之前\n- `onCommit`：消费者处理完消息，提交offset之后\n\n\n\n### 配置拦截器\n\n拦截器开发完成了，怎么让它生效呢？\n\n当前 Kafka 拦截器的设置方法是通过参数配置完成的\n\n生产者和消费者两端有一个相同的参数，名字叫 interceptor.classes，它指定的是一组类的列表\n\n```java\nProperties props = new Properties();\nList<String> interceptors = new ArrayList<>();\ninterceptors.add(\"com.yourcompany.kafkaproject.interceptors.AddTimestampInterceptor\"); // 拦截器1\ninterceptors.add(\"com.yourcompany.kafkaproject.interceptors.UpdateCounterInterceptor\"); // 拦截器2\nprops.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);\n```\n\n\n\n## 生产者与TCP连接\n\n\n\n### 为什么采用TCP作为底层传输协议\n\nTCP 拥有一些高级功能，如多路复用请求和同时轮询多个连接的能力。\n\n多路复用请求：multiplexing request，是将两个或多个数据合并到底层—物理连接中的过程。TCP 的多路复用请求会在一条物理连接上创建若干个虚拟连接，每个虚拟连接负责流转各自对应的数据流。严格讲：TCP 并不能多路复用，只是提供可靠的消息交付语义保证，如自动重传丢失的报文。\n\n\n\n### 生产者是什么时候创建TCP连接的\n\n```java\nProperties props = new Properties ();\nprops.put(“参数1”, “参数1的值”)；\nprops.put(“参数2”, “参数2的值”)；\n……\ntry (Producer<String, String> producer = new KafkaProducer<>(props)) {\n            producer.send(new ProducerRecord<String, String>(……), callback);\n  ……\n}\n```\n\n\n\n针对上面的代码，能创建TCP连接的只有两个地方，一是 Producer 实例化的时候；一是 producer.send 的时候；\n\n- **Kafka 是在 Producer 实例化的时候与 Broker 建立的 TCP 连接**\n- 所以，当 producer.send 的时候，其实已经有TCP连接了\n\n\n\n扩展：除了在 Producer 实例化的时候与 Broker 建立的 TCP 连接之外，还有没有其他情况？\n\n- 有，有两个情况，也会创建TCP连接\n- **元数据更新时，会与元数据中没有连接的 Broker 建立 TCP 连接；**\n  - 每隔5分钟， Producer 会定期从 Broker 中获取元数据信息\n  - Producer 尝试给一个不存在的 Topic 发送消息时，Broker 会说这个 Topic 不存在，然后 Producer 会请求 Broker 更新元数据信息\n- **在消息发送时，如果 Producer 发现与要发送消息的 Topic 所在的 Broker 没有 TCP 连接，就会创建连接；**\n\n\n\n### 是怎么创建TCP连接的\n\n在创建 KafkaProducer 实例时，生产者应用会在后台创建并启动一个名为 Sender 的线程，该 Sender 线程开始运行时首先会创建与 Broker 的TCP连接。\n\nBroker有1000个，bootstrap.servers 要配置1000个嘛？ 不需要，因为 Producer 一旦连接到集群中的任一台 Broker，就能拿到整个集群的 Broker 信息。\n\n\n\n\n\n### TCP连接是什么时候被关闭的\n\n- 用户主动关闭，调用 producer.close\n- Kafka自动关闭（虽然是producer端设置的参数，但实际上，是broker关闭的TCP连接）：与 Producer 端参数 connections.max.idle.ms 的值有关。默认情况下该参数值是 9 分钟，即如果在 9 分钟内没有任何请求“流过”某个 TCP 连接，那么 Kafka 会主动帮你把该 TCP 连接关闭。用户可以在 Producer 端设置 connections.max.idle.ms=-1 禁掉这种机制。一旦被设置成 -1，TCP 连接将成为永久长连接。\n\n\n\n\n\n### 会存在的一些问题\n\n- producer每5分钟获取一些元数据，然后与元数据中没有连接的broker建立TCP连接，然后9分钟后，broker会中断空闲的连接，然后5分钟后，在此建立连接；9分钟后，再次中断连接；\n\n\n\n\n\n## 消费者与TCP连接\n\n\n\n### 消费者是什么时候创建TCP连接的\n\n- 生产者是在new KakfaProducer的时候，后台开启一个Sender的线程用来创建TCP连接的；\n- 消费者**不是**在实例化的时候创建的，而是在开始消费消息的时候（consumer.poll）才主动创建TCP连接，准确的说有三个时机\n  - 发起 FindCoordinator 请求时（连接的brokerId是-1，因为不知道连哪一个）\n  - 连接协调者的时候（连接的brokerId是 `Interger.Max - 协调者所在broker的Id号` ，为什么这么设计，为了防止连接重用）\n  - 真正消费消息的时候（连接某个topic的某个分区的leader副本所在的broker）\n\n\n\n### 创建多少个TCP连接\n\n会创建三类TCP连接\n\n- FindCoordinator 请求与任意一个 Broker 的 TCP 连接\n- 与 Coordinator 的连接，此时消费者才能真正的开始工作\n- 与 Partition 所在leader副本的TCP连接，拉取消息，真正开始处理\n\n\n\n其中第一类（FIndCoordinator请求与任意一个Broker的连接）会在消费者真正开始处理消息的时候，也就是后面两类TCP连接建立好之后，第一类连接会被关闭掉；\n\n\n\n\n\n### 消费者是什么时候关闭TCP连接的\n\n\n\n上面说的三类连接，其中第一类连接会在二，三类连接创建好之后，被关闭掉；\n\n二，三类连接的关闭场景有两种：\n\n- 主动关闭，这个不说了\n- kafka自动关闭，由 消费者端参数connection.max.idle.ms控制。当超过指定时间，该消费者没有消息消费时，就会被关闭连接（但是如果我们的消费逻辑是while循环的情况，则永远不会被关闭，因为一直与broker保持通信，实现了“长链接”的效果）\n\n\n\n### 可能存在的问题\n\n第一类 TCP 连接仅仅是为了首次获取元数据而创建的，后面就会被废弃掉。最根本的原因是，消费者在启动时还不知道 Kafka 集群的信息，只能使用“-1” 去注册，即使消费者获取了真实的 Broker ID，它依旧无法区分这个“-1”对应的是哪台 Broker，因此也就无法重用这个 Socket 连接，只能再重新创建一个新的连接。\n\n\n\n为什么会出现这种情况呢？主要是因为目前 Kafka 仅仅使用 ID 这一个维度的数据来表征 Socket 连接信息。这点信息明显不足以确定连接的是哪台 Broker，也许在未来，社区应该考虑使用 < 主机名、端口、ID> 三元组的方式来定位 Socket 资源，这样或许能够让消费者程序少创建一些 TCP 连接。\n\n\n\n也许你会问，反正 Kafka 有定时关闭机制，这算多大点事呢？其实，在实际场景中，我见过很多将 connection.max.idle.ms 设置成 -1，即禁用定时关闭的案例，如果是这样的话，这些 TCP 连接将不会被定期清除，只会成为永久的“僵尸”连接。基于这个原因，社区应该考虑更好的解决方案。\n\n\n\n\n\n\n\n## 幂等和事务生产者\n\n\n\n### 消息交付可靠性\n\n所谓的消息交付可靠性，是指`kafka`对`Producer`和`Consumer`要处理的消息，提供什么样的承诺：\n\n- `最多一次`：消息只会被发送一次，可能会丢失，绝不会重复\n- `至少一次`（默认）：发送消息的时候，至少要有一次broker明确告知已经提交的callback，消息可能重复，但不会丢失\n\n- `精确一次`：消息不会丢失，也不会重复\n\n\n\n### 幂等和事务的概念\n\n略\n\n\n\n### 幂等生产者\n\n在 Kafka 中，`Producer `默认不是幂等性的，但我们可以创建幂等性 `Producer`。\n\n在没有幂等之前，`Producer`向一个`Partition`发送消息，可能会出现同一条消息被多次发送的情况，导致消息重复\n\n在有了幂等之后，`Producer`向一个`Partition`发送消息，发送一次和发送多次，由于幂等存在，在当前这一个`Partition`内消息不会重复\n\n\n\n#### 幂等生产者的使用方式\n\n```java\nprops.put(“enable.idempotence”, true)\n或\nprops.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG， true)\n```\n\n设置 `enable.idempotence = true` 后，`Producer `自动升级成幂等性 `Producer`\n\n如果把 `enable.idempotence = true` ，则一定要设置`ack = all`，否则会报错：Must set acks to all in order to use the idempotent producer. Otherwise we cannot guarantee idempotence\n\n其他所有的代码逻辑都不需要改变。\n\nKafka 自动做消息的重复去重\n\n公司并没有使用幂等生产者，以下是公司的代码\n\n```java\n    public KafkaProducer24(String brokers, ProduceOptionalConfig extraConfig) {\n        this.extraConfig = extraConfig == null ? ProduceOptionalConfig.defaultConfig : extraConfig;\n        Properties props = new Properties();\n        props.put(\"bootstrap.servers\", brokers);\n        props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n        props.put(\"acks\", \"默认是Leader Replica收到就行\");\n        props.put(\"request.timeout.ms\", \"默认是30000\");\n        props.put(\"compression.type\", \"snappy\");\n        props.put(\"batch.size\", \"默认是16384\");\n        props.put(\"linger.ms\", \"默认是5\");\n        this.producer = new KafkaProducer(props);\n    }\n```\n\n\n\n\n\n#### 幂等生产者的实现原理\n\n- Producer 在每次启动后，都会向 Broker 申请一个全局一个唯一的 pid ，用来标识本次会话\n- V2版本的消息格式增加了 sequence number 字段， producer 每发一批消息， seq 就加1\n- broker 在内存中会维护 <pid, topic, partition, seq> 映射，收到消息后检查 seq ，如果：\n  - new_seq = old_seq + 1 ：正常消息\n  - new_seq <= old_seq ：重复消息\n  - new_seq > old_seq + 1 ： 消息丢失\n\n\n\n#### 幂等生产者的作用范围\n\n只能实现单Session上的幂等性\n\n- 因为`Producer`的每次重启，都会向`Broker`申请一个新的全局唯一的`pid`，用来标识本次会话\n- `Producer`在不同`Session`上的`pid`不一样，是幂等不能跨`Session`的主要原因。\n\n只能保证单分区上的幂等性\n\n- 因为`Broker`端维护的映射是`Partiton`粒度的，所以只能保证单分区上的幂等性\n\n\n\n那么你可能会问，如果我想实现多分区以及多会话上的消息无重复，应该怎么做呢？答案就是事务（transaction）或者依赖事务型 Producer。\n\n\n\n### 事务生产者\n\n如果我想实现多分区以及多会话上的消息无重复，应该怎么做呢？答案就是事务（transaction）或者依赖事务型 Producer。\n\n这也是幂等性 Producer 和事务型 Producer 的最大区别！\n\n在 Kafka 中，`Producer `默认不是幂等性的，同时默认也不是事务的。\n\nKafka 自 0.11 版本开始也提供了对事务的支持，它能保证多条消息原子性地写入到目标分区，同时也能保证 Consumer 只能看到事务成功提交的消息。\n\n\n\n\n\n#### 事务生产者的使用方式\n\n要想使用kafka的事务，需要同时设置`Producer`和`Consumer`\n\n**设置Producer**\n\n设置事务型 Producer 的方法也很简单，满足两个要求即可：\n\n- 和幂等性 Producer 一样，开启 `enable.idempotence = true`\n- 设置 Producer 端参数` transactional.id`。最好为其设置一个有意义的名字。\n- 在发送消息的时候，需要显示的开启`beginTransaction`和提交`commitTransaction`事务\n- 消息1和消息2，要么全部成功，要么全部失败\n\n```java\n//创建事务生产者\nprops.put(“enable.idempotence”, true);\nprops.put(“transactional.id”, \"my-transcation-id-zs\")\n\n\nproducer.initTransactions();//初始化事务\ntry {\n    producer.beginTransaction();//开启事务\n    producer.send(record1);//发送消息1\n    producer.send(record2);//发送消息2\n    producer.commitTransaction();//提交事务\n} catch (KafkaException e) {\n    producer.abortTransaction();//终止事务\n}\n```\n\n\n\n**设置Consumer**\n\n- 为什么要设置，因为事务型`Producer`即使发送失败了，也会写到kakfa日志中，会被`Consumer`消费到；\n- 设置`Consumer`的 `isolation.level`参数\n  - `read_uncommitted`：读未提交，这是默认值，表明 Consumer 能够读取到 Kafka 写入的任何消息，不论事务型 Producer 提交事务还是终止事务，其写入的消息都可以读取。\n  - `read_committed`：读已提交，表明 Consumer 只会读取事务型 Producer 成功提交事务写入的消息。当然了，它也能看到非事务型 Producer 写入的所有消息。\n\n\n\n#### 事务生产者的实现原理\n\n待补充\n\n\n\n## 消费者组和独立消费者\n\n\n\nKafka 为了实现点对点（同一个消息只能被下游的一个 Consumer 消费），使用了`Consumer Group`的概念；\n\n那么什么是`Consumer Group`呢，我们具体看一下\n\n`Consumer Group`：多个 Consumer 实例组成一组消费某一个 Topic，这个 Topic 下的一条消息只能被组中的一个 Consumer 实例消费；\n\n\n\n### 什么是消费者组\n\n`Consumer Group` 是 Kafka 提供的可扩展且具有容错性的消费者机制。\n\n组内有多个消费者实例（Consumer Instance），它们共享一个公共的 Group ID。\n\n组内的所有消费者实例（Consumer Instance）一起消费订阅的主题（Subscribed Topics）的所有分区（Partition）。\n\n当然，该 Topic 的每个 Partition 只能由同一个消费者组内的一个 Consumer 实例来消费。\n\n\n\n### 为什么要引入消费者组\n\n为了提升吞吐量，假设 Topic 的消息的生产速率不变，增加消费者实例，就可以提升吞吐量；\n\n\n\n### 消费者的重平衡\n\n当组内的某一个消费者实例挂了，kafka会自动重平衡；将这个死亡的消费者实例原先消费的分区，转移给存活的消费者实例；\n\n后面会详细介绍：[重平衡](#重平衡)\n\n\n\n### 消费者组的特性是什么\n\n- `Consumer Group` 下可以有一个或多个 Consumer 实例。这里的实例可以是一个单独的进程，也可以是同一进程下的线程。在实际场景中，使用进程更为常见一些。\n- `Group ID` 是一个字符串，在一个 Kafka 集群中，它标识唯一的一个 Consumer Group。\n- `Consumer Group`所订阅的 Topic ，该 Topic 下的某**一个** Partition ，只能分配给组内的某**一个** Consumer 实例消费。当然，这个 Partition 也可以被其他的 Group 消费。\n\n\n\n### 传统的消息引擎模型\n\n- 点对点：一个消息只能被一个消费者消费到\n- 发布订阅：一个 Topic 下的消息，可以被订阅该 Topic 的所有消费者都消费到\n- kafka使用消费者组，实现了两种消息引擎模型； \n  - 如果所有的消费者属于一个消费者组，那就是点对点\n  - 如果所有的消费者属于不同的消费者组，那就是发布订阅\n\n\n\n\n\n### 消费者组的使用方式\n\n待补充，补充一个代码\n\n\n\n### 消费者组是如何维护offset的\n\n对于一个单独的消费者来说，offset就是一个数值；\n\n但是对于一个消费者组来说，因为组内有多个消费者，那么消费者组维护offset是通过一个map来维护的，这个map简单的可以理解为是：Map<TopicPartition,Long>\n\n\n\n对于老版本的kafka来说，offset是保存在zk中的，但是后来kafka的开发者发现，offset的更新太过于频繁，频繁的封信会拖慢zk的性能，所以在新版本的kafka中，offset是保存在broker内部的一个特殊的topic中的(__consumer_offset)。\n\n下面我们来看看这个特殊的 Topic：位移主题（__consumer_offset）\n\n\n\n### 独立消费者\n\n在 Kafka 中，消费消息除了使用 消费者组 Consumer Group 外，还有一种消费者会被使用，但是在业务场景中，使用的不多，一般是从在 Kafka 的流处理中。\n\n它是：独立消费者 Standalone Consumer\n\n1、请问Standalone Consumer 的独立消费者一般什么情况会用到 \n\n- 很多流处理框架的Kafka connector都没有使用consumer group，而是直接使用standalone consumer，因为group机制不好把控 \n\n2、Standalone Consumer 的独立消费者 使用跟普通消费者组有什么区别的。\n\n- standalone consumer没有rebalance，也没有group提供的负载均衡，你需要自己实现。其他方面（比如位移提交）和group没有太大的不同\n\n3、如果使用 Standalone Consumer，是不是也不会发生 rebalance 了？\n\n- standalone consumer就没有rebalance一说了。 它的特点主要是灵活。虽然社区一直在改进rebalance的性能，但大数据量下consumer group机制依然有很多弊病（比如rebalance太慢等），所以很多大数据框架(Spark /Flink)的kafka connector并不使用group机制，而是使用standalone consumer\n\n\n\n## 位移和位移主题\n\n位移在 Kafka 中是一个很重要的概念，分为：消费者位移（Consumer Offset）和分区位移（Offset）：[名词术语](#名词术语)\n\n\n\n### 消费者位移和分区位移\n\n消费者位移（Consumer Offset）：消费者位移是随时变化的，毕竟它是消费者消费进度的指示器嘛。\n\n分区位移（Offset）：表示的是分区内的消息位置，它是不变的，即一旦消息被成功写入到一个分区上，它的位移值就是固定的了。\n\n举个例子：\n\n一个消息发送到kafka集群，kafka就会给这个消息并一个编号，这个编号就是“分区位移”；而且这个“分区位移”是固定不变的；\n\n当有消费者消费的时候，消费者会记录我自己消费到了哪里，这个就是消费者位移；（消息者位移其实并不是记录在消费者端的，而是记录在zk或者kafka中的）；\n\n分区位移是一个常量，在消息写入到 Partition 中之后，就不变了。所以分区位移没什么好研究的。\n\n我们主要看一看：消费者位移\n\n\n\n### 消费者位移\n\n之前介绍过，消费者组是怎么维护 消费者位移（Consumer Offset） 的，在低版本中，Consumer Offset 是维护在 ZK 中的，在后续版本中，是记录在 Broker 中的一个特殊的 Topic 中，这个 Topic 叫做：位移主题（__consumer_offset）\n\n\n\n__consumer_offsets 在 Kafka 源码中有个更为正式的名字，叫位移主题，即 Offsets Topic。\n\n\n\n### 位移主题\n\n\n\n#### 为什么会有位移主题\n\n对于老版本的 Kafka 来说，Consumer Offset 是保存在 ZK 中的，但是后来 Kafka 的开发者发现，Consumer Offset 的更新太过于频繁，频繁的更新会拖慢 ZK 的性能，所以在新版本的 Kafka 中，Consumer Offset 是保存在 Broker 内部的一个特殊的 Topic 中的：__consumer_offset\n\n\n\n#### 位移主题是什么\n\n是 Kafka 中的一个内部 Topic\n\n这个 Topic 的主要作用是用来管理 Consumer Offset\n\nConsumer Offset 管理机制其实很简单，就是将 Consumer 的位移数据作为一条条普通的 Kafka 消息，发送到 __consumer_offsets 中。\n\n可以这么说，__consumer_offsets 的主要作用是保存 Kafka 消费者的位移信息。\n\n\n\n#### 位移主题什么时候创建\n\n当 Kafka 集群中的第一个 Consumer 程序启动时，Kafka 会自动创建位移主题。\n\n\n\n#### 位移主题的分区和副本\n\n我们知道 __consumer_offset 虽然是内部 Topic，但是它仍然是一个 Topic ，既然是 Topic ，那么它的分区数和副本是多少呢？\n\n- 分区数：50；由Broker 端参数 offsets.topic.num.partitions指定\n- 副本数：3；由Broker 端参数 offsets.topic.replication.factor指定\n\n\n\n#### 位移主题中存了什么\n\n存了三类消息\n\n- 位移消息：表示当前消费者组消费的位移信息\n- 用于保存 Consumer Group 信息的消息：比较神秘，几乎无法在搜索引擎中搜到。不过，你只需要记住它是用来注册 Consumer Group 的就可以了。\n- 用于删除 Group 过期位移甚至是删除 Group 的消息：tombstone 消息，即墓碑消息，也称 delete mark\n\n**位移消息**\n\n之前说过，Kafka 中有两种消息格式，[kafka的消息格式](#kafka的消息格式)，那么位移主题的消息格式，其实是 Kafka 自定义的特殊消息格式。\n\n既然是自定义的消息格式，也就说明：开发者不能随意的向这个主题发送消息，因为一旦你写入的消息不满足 Kafka 规定的格式，那么 Kafka 内部无法成功解析，就会造成 Broker 的崩溃。\n\n那么这个主题存的到底是什么格式的消息呢？\n\n事实上， Kafka 自定义的位移主题消息格式，其实是一个 KV 结构\n\nK：保存 3 部分内容：<Group ID，主题名，分区号>  （即使是单个消费者（Standalone Consumer），也是会有groupid的）\n\nV：Offset\n\n\n\n**墓碑消息**\n\n墓碑消息只出现在源码中而不暴露给你。\n\n它的主要特点是它的消息体是 null，即空消息体。\n\n那么，何时会写入这类消息呢？\n\n一旦某个 Consumer Group 下的所有 Consumer 实例都停止了，而且它们的 Consumer Offset 数据都已被删除时，Kafka 会向位移主题的对应分区写入 tombstone 消息，表明要彻底删除这个 Consumer Group 的信息。\n\n\n\n#### 怎么提交offset到位移主题\n\n\n\nKafka Consumer 提交 Offset 时会写入  __consumer_offset 这个 Topic\n\n那 Consumer 是怎么提交位移的呢？\n\n目前 Kafka Consumer 提交位移的方式有两种：自动提交位移和手动提交位移。\n\n\n\n##### 自动提交位移\n\n- 设置 Consumer 端参数：enable.auto.commit\n- 设置 Consumer 端参数：auto.commit.interval.ms\n\n```java\n\nProperties props = new Properties();\n     props.put(\"bootstrap.servers\", \"localhost:9092\");\n     props.put(\"group.id\", \"test\");\n     props.put(\"enable.auto.commit\", \"true\");\n     props.put(\"auto.commit.interval.ms\", \"2000\");\n     props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n     props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n     KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);\n     consumer.subscribe(Arrays.asList(\"foo\", \"bar\"));\n     while (true) {\n         ConsumerRecords<String, String> records = consumer.poll(100);\n         for (ConsumerRecord<String, String> record : records)\n             System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value());\n     }\n```\n\n\n\n\n\n##### 手动提交位移\n\n- 设置 Consumer 端参数：enable.auto.commit\n- 然后我们就需要手动提交位移了，手动提交位移，Kafka提供了两种方式：同步提交方式和异步提交方式\n\n\n\n###### 同步提交方式\n\n- KafkaConsumer#commitSync()\n- 提交失败了，会自动重试，再次提交，所以会影响消费性能\n\n```java\n\nwhile (true) {\n    ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));\n    process(records); // 处理消息\n    try {\n        consumer.commitSync();\n    } catch (CommitFailedException e) {\n        handle(e); // 处理提交失败异常\n    }\n}\n```\n\n\n\n###### 异步提交方式\n\n- KafkaConsumer#commitAsync()\n\n```java\n\nwhile (true) {\n    ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));\n    process(records); // 处理消息\n    consumer.commitAsync((offsets, exception) -> {\n        if (exception != null)\n            handle(exception);\n    });\n}\n```\n\n\n\n###### 同步提交方式+异步提交方式\n\n- 同步会出现的问题：是阻塞的，会降低 Consumer 的 TPS ；好处是会自动重试，提交不成功的话，不会拉取新的消息；\n- 异步会出现的问题：提交异常的话，不会重试；会导致消息重复消费\n- 怎么办呢？结合两者，先使用异步提交一次，如果失败了，finally里使用同步方式\n\n```java\n\ntry {\n    while(true) {\n        ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));\n        process(records); // 处理消息\n        commitAysnc(); // 使用异步提交规避阻塞\n    }\n} catch(Exception e) {\n    handle(e); // 处理异常\n} finally {\n    try {\n        consumer.commitSync(); // 最后一次提交使用同步阻塞式提交\n    } finally {\n        consumer.close();\n    }\n}\n```\n\n\n\n##### 公司内部使用\n\n在公司内部默认是使用：手动提交位移\n\n```java\nif (this.optionalConfig.isTransactional()) {\n    properties.put(\"enable.auto.commit\", \"false\");\n} else {\n    properties.put(\"enable.auto.commit\", \"true\");\n}\n\n\n\npublic class ConsumeOptionalConfig {\n    //....\n    private boolean transactional = true;\n    //....\n}\n\n```\n\n既然公司内部使用的手动提交，那么在哪里提交的位移呢？\n\n```java\nprotected void commitInit() {\n    //公司是为：每一个Consumer开了一个线程，后台手动提交位移\n    this.commitOffsetThread = new TransactionalConsumer.CommitOffsetThread();\n    this.commitOffsetThread.start();\n}\n\n//这是线程真正提交位移的方式，提交到 ZK 的\nlong nextOffset = currentOffset + 1L;\n((ZookeeperConsumerConnector)TransactionalConsumer.this.connector).commitOffsetToZooKeeper(new TopicAndPartition(TransactionalConsumer.this.topic, partition), nextOffset);\nthis.logger.info(String.format(\"commit offset: topic:%s, partition:%d, nextOffset: %s\", TransactionalConsumer.this.topic, partition, nextOffset));\n```\n\n\n\n##### Offset提交导致的问题\n\n上面说了 Offset 的提交，有两种方式：自动提交和手动提交，手动提交又分为：同步提交和异步提交\n\n那么它们会导致什么问题呢？会不会导致消息的丢失消费和消息的重复消费？\n\n**自动提交**\n\n- 自动提交会导致消息的丢失\n- 自动提交会导致消息的重复消费\n\n为什么会导致丢失？是因为消息消费的流程是：Concumer 先 Pull 一部分消息到内存中，然后开始消费，但是需要提交给 Broker 的 Offset 是Pull下来的最大的Offset，由于提交 Offset 是自动的，有可能 Pull 下来的消息还没有消费完，Offset 已经提交了；然后如果从内存中消费出现了问题，那么会导致内存中剩余没消费就永远不会在消费了。\n\n为什么会导致重复？是因为自动提交，默认周期是5秒，如果在第3秒的时候，Broker发生了 Rebalance，那么 Offset 就提交不上去，当 Rebalance 完成之后，这部分数据，还会被在消费一次。\n\n**手动提交**\n\n- 手动提交方式，可以解决消息的重复消费和丢失问题，因为我消费一个，提交一次Offset\n- 同步提交方式，会影响消费端的性能\n- 异步提交方式，解决了性能问题，但是消费成功之后，位移提交失败，不会自动重试提交，如果我们在 callback 中手动重试，又有可能导致提交上去的是一个老的 Offset\n- 异步+同步方式：每次提交Offset都是异步的，然后在 finally 中同步提交一次，可以完美避免以上问题\n\n怎么避免的呢？消费者 poll 方法内部有维护一个不可见的指针，commitAysnc 方法异步提交不管是否成功，poll 仍然能根据自己维护的指针位移消费数据，最后在finally内用同步方法， 同步最新的 Offset。 这样提交上去的就不是老的 Offset\n\n\n\n如果你选择的是自动提交位移，那么就可能存在一个问题：只要 Consumer 一直启动着，它就会无限期地向位移主题写入消息。导致磁盘爆满；因为自动提交位移是后台定时提交的（auto.commit.interval.ms默认是5s）；\n\n那么满了怎么办？满了就删除，怎么删除呢？\n\n\n\n#### 位移主题中的过期数据（过期位移）\n\n我们知道所有的位移数据都是保存在 位移主题 中的，如果不删除的话，位移主题就会无限的膨胀\n\n为了避免该主题无限期膨胀。Kafka 会定期的清理位移主题中的数据。\n\n\n\n**那么什么样的数据被称为过期数据呢？**\n\n我们知道位移主题中存了三类消息，这里以 位移消息 为例；\n\n位移消息的消息格式是Map格式，key是 groupid+topic+partition ；value是位移数据\n\n举个例子说：\n\n一个消费者组（假设groupid为：consumer_group_1），这个消费者组消费一个 Topic（假设消费：topic_a）；然后这个 Topic 有3个 Partition；\n\n生产者 源源不断的向 Topic 中写数据，消费者组不停地消费数据，消费一个数据，就向 位移主题 中发一个位移消息；\n\n那么这里的位移消息可能就是下面这样的：\n\nconsumer_group_1+topic_a+partition_1  ：  2345\n\nconsumer_group_1+topic_a+partition_1  ：  2346\n\nconsumer_group_1+topic_a+partition_1  ：  2347\n\n。。。。。\n\n最终，我们就会发现，同一个key就会存在很多数据，而且只有最后一条数据，才是有效的。那么之前的数据，都**被称为过期数据**；\n\n\n\n再次之外，还有一种情况：\n\n在 Broker 端有一个参数：`offsets.retention.minutes`,这个参数表明了 offset 的保留时间，什么意思呢？\n\n就是说：我们提交到 位移主题 中的消息，并不会永远的保存，在超过了这个配置时间后，Kafka后台有一个线程，就会把这个Offset删掉\n\n这个值一般是 7 天。\n\n也就是说：如果你的消费者7天都没有上线了，或者7天都没有提交 offset 了，Kafka就会把这个消费者组的 位移数据 判定为过期数据。并删除\n\n\n\n#### 位移主题中的过期数据（过期位移）清理\n\n在上面我们知道了 位移主题 中的过期数据有两类：\n\n- 一类是：同一个`key`的过期数据\n- 一类是：超过了`offsets.retention.minutes`的过期数据\n\n\n\n**第一类过期数据，Kafka是怎么清理的呢？**\n\n答案就是 Compaction。\n\n国内很多文献都将其翻译成压缩，我个人是有一点保留意见的。\n\n在英语中，压缩的专有术语是 Compression，它的原理和 Compaction 很不相同，我更倾向于翻译成压实，或干脆采用 JVM 垃圾回收中的术语：整理。\n\n它的原理很简单：就是将：同一个 <Group ID，主题名，分区号> 的 Offset 进行压实整理，只保留最新的\n\n<img src=\"kafka从入门到入土.assets/image-20220828155424662.png\" alt=\"image-20220828155424662\" style=\"zoom: 50%;\" />\n\n图中位移为 0、2 和 3 的消息的 Key 都是 K1。Compact 之后，分区只需要保存位移为 3 的消息，因为它是最新发送的。\n\nKafka 提供了专门的后台线程定期地巡检待 Compact 的主题，看看是否存在满足条件的可压缩数据。这个后台线程叫 Log Cleaner。\n\n\n\n**第二类过期数据，Kafka是怎么清理的呢？**\n\n上面说到，Kafka有一个后台线程：Log Cleaner。\n\n这个线程除了会清理第一类过期数据之外，还会清理第二类过期数据。\n\n很多实际生产环境中都出现过位移主题无限膨胀占用过多磁盘空间的问题，如果你的环境中也有这个问题，我建议你去检查一下 Log Cleaner 线程的状态，通常都是这个线程挂掉了导致的。\n\nKafka 定期自动删除过期位移的条件就是，组要处于 Empty 状态（消费者组的状态机）。因此，如果你的消费者组停掉了很长时间（超过 7 天），那么 Kafka 很可能就把该组的位移数据删除了\n\n\n\n#### 位移提交失败怎么办\n\n一般的失败，API会自动重试；\n\n但是有一个异常叫做 CommitFailedException，这个异常抛出，说明位移的提交出现了大问题，需要人工介入了\n\n那么这个异常是啥意思呢？什么时候会产生呢？产生了之后要怎么处理呢？\n\n**什么是 CommitFailedException**\n\n> Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. \n>\n> This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. \n>\n> You can address this either by increasing max.poll.interval.ms or by reducing the maximum size of batches returned in poll() with max.poll.records.\n\n翻译过来就是：\n\n> 本次提交位移失败了，原因是消费者组已经开启了 Rebalance 过程，并且将要提交位移的分区分配给了另一个消费者实例。\n>\n> 出现这个情况的原因是：你的消费者实例连续两次调用 poll 方法的时间间隔超过了期望的 max.poll.interval.ms 参数值。这通常表明，你的消费者实例花费了太长的时间进行消息处理，耽误了调用 poll 方法。\n>\n> 你可以通过：增加期望的时间间隔 max.poll.interval.ms 参数值 或者 减少 poll 方法一次性返回的消息数量，即减少 max.poll.records 参数值。\n\n**那么什么时候会抛出这个异常呢？**\n\n从源代码方面来说，CommitFailedException 异常通常发生在手动提交位移时，即用户显式调用 KafkaConsumer.commitSync() 方法时。\n\n从使用场景来说，有两种典型的场景可能遭遇该异常。\n\n场景一\n\n- 当消息处理的总时间超过预设的 max.poll.interval.ms 参数值时，Kafka Consumer 端会抛出 CommitFailedException 异常。\n\n- 模拟异常产生：\n\n```java\n…\nProperties props = new Properties();\n…\nprops.put(\"max.poll.interval.ms\", 5000);\nconsumer.subscribe(Arrays.asList(\"test-topic\"));\n \nwhile (true) {\n    ConsumerRecords<String, String> records = \n    consumer.poll(Duration.ofSeconds(1));\n    // 使用Thread.sleep模拟真实的消息处理逻辑\n    Thread.sleep(6000L);\n    consumer.commitSync();\n}\n```\n\n场景二\n\n- 消费者组 Consumer Group  和 独立消费者 Standalone Consumer 拥有了相同的 group.id，当独立消费者程序手动提交位移时，Kafka 就会立即抛出 CommitFailedException 异常，因为 Kafka 无法识别这个具有相同 group.id 的消费者实例，于是就向它返回一个错误，表明它不是消费者组内合法的成员。\n- 这种情况一般出现在很多部门消费同一个 Kafka 集群导致的，各个部门的消费者命名重复了，导致相同的 group.id\n\n\n\n**当前当异常出现了，我们应该怎么办呢？**\n\n怎么解决问题，要先知道问题是怎么出现的。\n\n比如在 场景一 中，我们知道产生问题的原因是：两次 poll() 方法调用的间隔超过了 max.poll.interval.ms\n\n那么就简单了，针对 场景一 ，我们可以：\n\n- **调大 max.poll.interval.ms 这个间隔时间**，默认时间是 5分钟\n- **减少每次 poll() 拉取的消息数量**：我们知道一次 poll() 方法，默认拉 500 条，因为拉取的太多了，消费的慢，所以导致两次 poll() 时间间隔太长\n- 接第二条，‘消费的慢’，那我们就提到消息速率，**优化代码，减少每条消息的处理时间**，提高TPS\n- 除了优化代码，提交消费速度，还可以**使用多线程，提高消费速度**，但是要注意多线程下的位移提交问题\n\n针对场景二呢，上面四个办法就不能用了，不过一般大公司下，消费者都是需要申请的，如果重复了，一般是申请不了的。\n\n\n\n\n\n## 多线程消费\n\n\n\n### Kafka Java Consumer 的单线程设计\n\n为什么 Kafka Java Consumer 要设计成单线程，看一下发展历史就明白了了\n\n在目前的 KafkaConsumer 的API出现之前，有一个 Scala 版本的 Consumer 的API，这组 Scale 的API 被称为老版本 Consumer\n\n在老版本 Consumer 中，Consumer 的设计是多线程的架构：\n\n- 每个 Consumer 实例在内部为所有订阅的 Topic 分区，创建对应的消息获取线程（就是一个分区一个线程），称为 Fetcher 线程\n- 老版本的 Consumer 同时也是阻塞的，Consumer 实例启动后，内部会创建阻塞式的消息获取迭代器\n\n那么为什么后来变成单线程的了呢？\n\n- 主要是因为老版本的 Consumer 是阻塞的\n- 而在大部分业务场景下，比如对数据的过滤，连接，分组，就不能是阻塞式的。\n- 所以在新版的 Consumer 下，Kafka 设计了 单线程+轮训 的机制\n\n采用单线程还有另外一个考虑\n\n- 就是单线程可以简化 Consumer 的设计，在任何编程语言中，单线程都比多线程更方便维护\n\n不过，虽然 Consumer 的设计是单线程的，但是并不意味着我们就不能多线程了。\n\n虽然 KafkaConsumer 的类的设计是单线程的，而且**不是线程安全**的。但是只是说明 拉取消息 的逻辑是单线程的\n\n但是消息拉取之后，怎么处理消息，完全是由开发者决定的，此时可以**手动开发多线程**进行消费\n\n### 多线程方案\n\n总体来说有两种方案。\n\n#### **方案一**\n\n消费者程序启动多个线程，每个线程维护专属的 KafkaConsumer 实例，负责完整的消息获取、消息处理流程。\n\n简单地说：一个线程负责一个分区\n\n<img src=\"kafka从入门到入土.assets/image-20230214112054515.png\" alt=\"image-20230214112054515\" style=\"zoom: 33%;\" />\n\n#### **方案二**\n\n消费者程序使用单或多线程获取消息，同时创建多个消费线程执行消息处理逻辑\n\n简单的说：一个或多个线程负责拉取消息，多个线程负责处理消息\n\n<img src=\"kafka从入门到入土.assets/image-20230214112223276.png\" alt=\"image-20230214112223276\" style=\"zoom:33%;\" />\n\n\n\n#### **方案对比**\n\n| 方案   | 优点                                     | 缺点                                                         |\n| ------ | ---------------------------------------- | ------------------------------------------------------------ |\n| 方案一 | 方便实现                                 | 占用更多的系统资源                                           |\n| 方案一 | 速度快，没有线程间的交互开销             | 线程数受限于 Topic 的分区数（最多一个分区一个线程），扩展性差 |\n| 方案一 | 易于维护分区间的消息顺序                 | 线程自己拉取消息，自己处理消息，可能导致超时，引发Rebalance  |\n| 方案二 | 可独立扩展获取消息线程数和处理消息线程数 | 实现难度高                                                   |\n| 方案二 | 伸缩性好                                 | 难以维护分区内的消息消费顺序                                 |\n| 方案二 |                                          | 处理链路长，不利于 Offset 的提交管理                         |\n\n\n\n#### **代码实现**\n\n方案一\n\n- 这段代码创建了一个 Runnable 类，表示执行消费获取和消费处理的逻辑。每个 KafkaConsumerRunner 类都会创建一个专属的 KafkaConsumer 实例。在实际应用中，你可以创建多个 KafkaConsumerRunner 实例，并依次执行启动它们，以实现方案 1 的多线程架构。\n\n```java\n\npublic class KafkaConsumerRunner implements Runnable {\n     private final AtomicBoolean closed = new AtomicBoolean(false);\n     private final KafkaConsumer consumer;\n\n\n     public void run() {\n         try {\n             consumer.subscribe(Arrays.asList(\"topic\"));\n             while (!closed.get()) {\n      ConsumerRecords records = \n        consumer.poll(Duration.ofMillis(10000));\n                 //  执行消息处理逻辑\n             }\n         } catch (WakeupException e) {\n             // Ignore exception if closing\n             if (!closed.get()) throw e;\n         } finally {\n             consumer.close();\n         }\n     }\n\n\n     // Shutdown hook which can be called from a separate thread\n     public void shutdown() {\n         closed.set(true);\n         consumer.wakeup();\n     }\n```\n\n\n\n方案二\n\n- 这段代码最重要的地方是最后一行：当 Consumer 的 poll 方法返回消息后，由专门的线程池来负责处理具体的消息。调用 poll 方法的主线程不负责消息处理逻辑，这样就实现了方案 2 的多线程架构。\n\n```java\n\nprivate final KafkaConsumer<String, String> consumer;\nprivate ExecutorService executors;\n...\n\n\nprivate int workerNum = ...;\nexecutors = new ThreadPoolExecutor(\n  workerNum, workerNum, 0L, TimeUnit.MILLISECONDS,\n  new ArrayBlockingQueue<>(1000), \n  new ThreadPoolExecutor.CallerRunsPolicy());\n\n\n...\nwhile (true)  {\n  ConsumerRecords<String, String> records = \n    consumer.poll(Duration.ofSeconds(1));\n  for (final ConsumerRecord record : records) {\n    executors.submit(new Worker(record));\n  }\n}\n..\n```\n\n\n\n## 重平衡与协调者\n\n\n\n### 什么是重平衡\n\nRebalance 本质上是一种协议，规定了一个 Consumer Group 下的所有 Consumer 如何达成一致，来分配订阅 Topic 的每个分区。\n\n比如某个 Group 下有 20 个 Consumer 实例，它订阅了一个具有 100 个分区的 Topic。正常情况下，Kafka 平均会为每个 Consumer 分配 5 个分区。这个分配的过程就叫 Rebalance。\n\n\n\n### 什么时候会重平衡\n\n- Consumer Group 内 Consumer实例 数量发生变化（新增或减少）；\n- Consumer Group 订阅的 Topic 数发生变化；\n- Consumer Group 订阅的 Topic 的 Partition 数量发生变化\n\n\n\n### 重平衡策略\n\n- 举例：比如组内有2个消费者，这个组消费 TopicA 和 TopicB ,其中 Consumer-1 消费 TopicA，Consumer-2 消费 TopicB，当该 Consumer Group 新订阅一个 TopicC 的时候，会不会 Consumer-1 消费到 TopicB，Consumer-2 消费到 TopicA\n\nKafka 有三种策略保证重平衡后的公平\n\n**Rnage 分配策略**\n\nRange分配策略是面向每个 Topic 的，首先会对同一个 Topic 里面的 Partition 按照序号进行排序，并把消费者线程按照字母顺序进行排序。然后用分区数除以消费者线程数量来判断每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。 \n\n**RoundRobin策略**\n\nRoundRobin策略的原理是将 Consumer Group 内所有 Consumer 以及订阅的所有 Topic 的 Partition 按照字典序排序，然后通过轮询算法逐个将分区以此分配给每个消费者。 使用RoundRobin分配策略时会出现两种情况： \n\n- 如果同一消费组内，所有的消费者订阅的消息都是相同的，那么 RoundRobin 策略的分区分配会是均匀的。\n\n-  如果同一消费者组内，所订阅的消息是不相同的，那么在执行分区分配的时候，就不是完全的轮询分配，有可能会导致分区分配的不均匀。如果某个消费者没有订阅消费组内的某个 topic，那么在分配分区的时候，此消费者将不会分配到这个 topic 的任何分区。 \n\n**Sticky分配策略**\n\nSticky分配策略，这种分配策略是在 Kafka 的 0.11.X 版本才开始引入的，是目前最复杂也是最优秀的分配策略。 Sticky分配策略的原理比较复杂，它的设计主要实现了两个目的： \n\n- 分区的分配要尽可能的均匀；\n\n-  分区的分配尽可能的与上次分配的保持相同。 如果这两个目的发生了冲突，优先实现第一个目的。\n\n\n\n### 什么是协调者Coordinator\n\n所谓协调者，在 Kafka 中对应的术语是 Coordinator，它专门为 Consumer Group 服务，负责为 Group 执行 Rebalance 以及提供位移管理和组成员管理等。\n\n- 协调者 coordinators 是协调管理 Consumer Group 的一个程序，运行在broker上的\n\n- 每一个broker在启动时都会启动 coordinator 组件（coordinator程序），也就是说每个 Broker 都有具备称为 Coordinator 的能力\n\n\n\n\n具体来讲，Consumer 端应用程序在提交位移时，其实是向 Coordinator 所在的 Broker 提交位移。\n\n同样地，当 Consumer 应用启动时，也是向 Coordinator 所在的 Broker 发送各种请求，然后由 Coordinator 负责执行消费者组的注册、成员管理记录等元数据管理操作。\n\n\n\n### 消费者组是怎么找到自己的coordinator的\n\n既然 Coordinator 是运行在 Broker上 的一个程序，那么一个消费者组，是怎么找到自己的 Coordinator 的呢？\n\n在[消费者与TCP连接](#消费者与TCP连接)这一节中，我们知道消费者在启动的时候，会创建三类 TCP 连接\n\n- 第一类：与负载最小的 Broker 创建连接，发送 FIndCoordinator 请求，希望该 Broker 告诉自己谁是我的协调者；\n- 第二类：与 Coordinator 创建TCP连接，加入组，分配方案，位移获取和提交等\n- 第三类：与 要消费的分区的副本所在 Broker 创建 TCP 连接，真正开始消费数据\n\n所以，消费者组找到自己的 Coordinator 是在第一类 TCP 请求中找到的\n\n那么 具体是怎么找到的呢？是通过之前说过的 __consumer_offset 这个主题来找的\n\n分为两步\n\n- 获取当前 Consumer Group 的 groupid，然后hash得到hash值；\n- 获取__consumer_offset 的分区数，默认是50\n- 计算 ：abs ( hash % 50 ) = 分区号\n- 然后，找到这个分区号的 leader 副本所在的 broker ；这个broker就是这个消费者的coordinator\n\n\n\n\n\n\n\n### 重平衡的缺点\n\n- 会STW（stop the world）：消费者会全部停止消费\n- 时间太慢了，几百个消费者重平衡一次，要几个小时\n- Rebalance 的设计是要求所有 Consumer 实例共同参与，全部重新分配所有分区\n- 在 Rebalance 过程中，所有 Consumer 实例都要参与，所以在整个过程中都不能消费任何消息，因此它对 Consumer 的 TPS 影响很大\n\n\n\n### 避免消费者组重平衡\n\n\n\n首先，明确一个概念，目前Rebalance的弊端（慢，STW）这2个弊端，社区是没有办法解决的；\n\n针对Rebalance的效率低的情况，社区采用了StickyAssignor策略来提升性能；\n\n既然无法解决，那我们只能尽量避免，怎么避免呢？就要从导致Rebalance发生的三种情况来看\n\n- 组成员数量发生变化（99%的Rebalance都是这个原因）\n- 订阅主题数量发生变化（一般是程序开发者主动操作，无法避免）\n- 订阅主题的分区数发生变化（一般是程序开发者主动操作，无法避免）\n\n\n\n组成员数量发生变化，变化分为两种，一种是增加，一种是减少\n\n- 增加：一般都是程序开发者主动操作，比如为了提升topic的消费速率，无法避免\n- 减少：如果是主动停掉的，那自不必说，无法避免；\n- 减少：不是主动停掉的，是被 Coordinator 错误地认为“已停止”从而被“踢出”Group。如果是这个原因导致的 Rebalance，我们就不能不管了。\n\n\n\n**什么时候coordinator会认为consumer实例已停止**\n\n- Coordinator 没有收到 Consumer 的心跳，就会让 Consumer 离组，重新 Rebalance\n  - Consumer 端有个参数，叫 session.timeout.ms，默认10秒；\n  - Coordinator 在10s内没有收到 Consumer 的心跳，就Rebalance\n  - 心跳是consumer主动给coordinator的，那么多久一次呢？是由参数：heartbeat.interval.ms控制的；\n  - 推荐配置：session.timeout.ms=6s，heartbeat.interval.ms=2s：要保证 Consumer 实例在被判定为“dead”之前，能够发送至少 3 轮的心跳请求\n- Consumer 实例在一定时间内消费不完已经 pull() 下来的消息，就会主动离组，重新Rebalance\n  - Consumer 端有个参数，max.poll.interval.ms 参数，默认5分钟\n  - 一个consumer在5分钟内，没有消费完拉取的数据，就Rebalance\n- consumer端的GC情况\n\n\n\nstandalone consumer 就没有 rebalance 一说了。 它的特点主要是灵活。\n\n虽然社区一直在改进rebalance的性能，但大数据量下consumer group机制依然有很多弊病（比如rebalance太慢等）\n\n所以很多大数据框架(Spark /Flink)的kafka connector并不使用group机制，而是使用standalone consumer\n\n\n\n### 怎么排查生产是否重平衡过多\n\n主动去排查：去找Coordinator所在的broker日志，如果经常发生rebalance，会有类似于\"(Re)join group\" 之类的日志\n\n被动排查：一般 Rebalance 过多，会降低消费者能力，间接的就会出现消息堵，可以配置相关告警\n\n\n\n### 重平衡核心全流程\n\n\n\n重平衡是怎么做到的？\n\n在 Kafka 中，每个 Consumer 都会通过**心跳线程**，定期的向 Coordinator 汇报自己的状态；\n\n同时 Coordinator 也会通过**心跳线程**，告诉 Consumer ：我收到了你的汇报；\n\n如果 Consumer 超时没有汇报；说明这个 Cnnsumer 有问题了，此时 Coordinator 就会开启重平衡\n\nCoordinator 会通过**心跳线程**，向这个 Consumner 所在的 Group 下的所有 Consumer，发送消息：**REBALANCE_IN_PROGRESS**\n\n当 Consumer 收到这种消息之后，就知道要开启重平衡了\n\n\n\n而 Coordinator 具体是怎么实现重平衡的呢？\n\n其实是通过控制 Consumer Group 的状态来完成重平衡的。这是**理解重平衡的基础**。\n\n下面我们就来看看 消费者组的状态机，这是**理解重平衡的基础**；\n\n\n\n#### 消费者组的状态机\n\n\n\n消费者组的状态主要有以下 五个：\n\n| 状态                | 说明                                                         |\n| ------------------- | ------------------------------------------------------------ |\n| Empty               | 组内没有任何成员，但消费者可能存在已经提交的数据，并且未过期：[位移主题中的过期数据（过期位移）](#位移主题中的过期数据（过期位移）) |\n| Dead                | 组内没有任何成员，Coordinator 已经把这个消费者组的元数据信息删除了 |\n| PreparingRebalance  | 消费者组准备开启重平衡，此时所有成员都要重新申请加入组       |\n| CompletingRebalance | 消费者组下的所有成员都已经加入，等待Leader分配方案，老版本中这个状态叫：AwatingSync |\n| Stable              | 消费者组的稳定状态，表示重平衡已经完成，可以正常开启消费了   |\n\n```mermaid\nstateDiagram\n\tdirection LR\n    Empty-->Dead: 组信息过期被删除\n    Empty-->PreparingRebalance:准备开启rebalance\n    PreparingRebalance-->Dead:位移主题分区Leader发生变化\n    PreparingRebalance-->Empty:组内所有成员离组\n    PreparingRebalance-->CompletingRebalance:有成员入组\n    CompletingRebalance-->Dead:位移主题分区Leader发生变化\n    CompletingRebalance-->PreparingRebalance:成员加入或离开\n    CompletingRebalance-->Stable:Leader完成分配\n    Stable-->Dead:位移主题分区Leader发生变化\n    Stable-->PreparingRebalance:心跳过期/成员离组/新成员加入\n    \n    %% 给状态添加样式\n    classDef badBadEvent fill:green,color:white,font-weight:bold,stroke-width:2px,stroke:yellow\n\tclass Empty badBadEvent\n\tclass PreparingRebalance badBadEvent\n\tclass CompletingRebalance badBadEvent\n\tclass Stable badBadEvent\n\tclass Dead badBadEvent\n\n```\n\n\n\n一个消费者组最开始是 Empty 状态\n\n当重平衡过程开启后，它会被置于 PreparingRebalance 状态等待成员加入\n\n之后变更到 CompletingRebalance 状态等待分配方案\n\n最后流转到 Stable 状态完成重平衡\n\n当有新成员加入或已有成员退出时，消费者组的状态从 Stable 直接跳到 PreparingRebalance 状态，此时，所有现存成员就必须重新申请加入组。\n\n当所有成员都退出组后，消费者组状态变更为 Empty\n\n\n\n\n\n#### 重平衡流程\n\n##### 场景一：新成员入组\n\n```mermaid\nsequenceDiagram\n\t成员1->>协调者: 心跳请求：你好协调者，我是组内的成员1，我还活着\n\t协调者-->>成员1: 心跳响应：你好成员1，已收到\n\t成员2->>协调者: joinGroup请求：你好协调者，我请求加入组，我要消费的是主题是 B\n\t成员1->>协调者: 心跳请求：你好协调者，我是组内的成员1，我还活着\n\t协调者-->>成员1: 心跳响应：你好成员1，REBALANCE_IN_PROGRESS,重平衡，你需要重新入组\n\t成员1->>协调者: joinGroup请求：你好协调者，我请求加入组，我要消费的是主题是 A\n\t协调者-->>成员2: joinGroup响应：你好成员2，你已成功入组，你是这组的Leader<br/>这组的订阅信息有：{成员1->主题A，成员2->主题B}\n\t协调者-->>成员1: joinGroup响应：你好成员1，你已成功入组，当前组的Leader是成员2，请等待分配方案\n\t成员1->>协调者: syncGroup请求：你好协调者，我是成员1，等待分配方案\n\t成员2->>协调者: syncGroup请求：你好协调者，我是成员2，也是这个组的Leader<br/>以下是我的分配方案：{成员1->主题A分区0，成员2->主题B分区0}\n\t协调者-->>成员1: syncGroup响应：你好成员1，你负责消费主题A的0分区\n\t协调者-->>成员2: syncGroup响应：你好成员2，你负责消费主题B的0分区\n\t成员1->>协调者: 心跳请求：你好协调者，我是组内的成员1，我还活着\n\t协调者-->>成员1: 心跳响应：你好成员1，已收到\n\t成员2->>协调者: 心跳请求：你好协调者，我是组内的成员2，我还活着\n\t协调者-->>成员2: 心跳响应：你好成员2，已收到\n\n\n```\n\n\n\n##### 场景二：组成员主动离组\n\n```mermaid\nsequenceDiagram\n\t成员1->>协调者: 心跳请求：你好协调者，我是组内的成员1，我还活着\n\t协调者-->>成员1: 心跳响应：你好成员1，已收到\n\t成员2->>协调者: 心跳请求：你好协调者，我是组内的成员2，我还活着\n\t协调者-->>成员2: 心跳响应：你好成员2，已收到\n\t成员1->>协调者: leaveGroup请求：你好协调者，我是组内的成员1，申请主动离组\n\t协调者-->>成员1: leaveGroup响应：你好成员1，已收到\n\t成员2->>协调者: 心跳请求：你好协调者，我是组内的成员2，我还活着\n\t协调者-->>成员2: 心跳响应：你好成员2，REBALANCE_IN_PROGRESS,重平衡，你需要重新入组\n\t成员2->>协调者: joinGroup请求：你好协调者，我请求加入组，我要消费的是主题是 B\n\t协调者-->>成员2: joinGroup响应：你好成员2，你已成功入组，你是这组的Leader<br/>这组的订阅信息有：{成员2->主题B}\n\t成员2->>协调者: syncGroup请求：你好协调者，我是成员2，也是这个组的Leader<br/>以下是我的分配方案：{成员2->主题B分区0}\n\t协调者-->>成员2: syncGroup响应：你好成员2，你负责消费主题B的0分区\n\t成员2->>协调者: 心跳请求：你好协调者，我是组内的成员2，我还活着\n\t协调者-->>成员2: 心跳响应：你好成员2，已收到\n```\n\n\n\n##### 场景三：组成员崩溃离组\n\n```mermaid\nsequenceDiagram\n\t成员1->>协调者: 心跳请求：你好协调者，我是组内的成员1，我还活着\n\t协调者-->>成员1: 心跳响应：你好成员1，已收到\n\t成员2->>协调者: 心跳请求：你好协调者，我是组内的成员2，我还活着\n\t协调者-->>成员2: 心跳响应：你好成员2，已收到\n\tnote left of 成员2: 成员2此时已离线\n\t协调者->>协调者: 发现成员2已经很长时间没有汇报了\n\t成员1->>协调者: 心跳请求：你好协调者，我是组内的成员1，我还活着\n\t协调者-->>成员1: 心跳响应：你好成员1，REBALANCE_IN_PROGRESS,重平衡，你需要重新入组\n\t成员1->>协调者: joinGroup请求：你好协调者，我请求加入组，我要消费的是主题是 A\n\t协调者-->>成员1: joinGroup响应：你好成员1，你已成功入组，你是这组的Leader<br/>这组的订阅信息有：{成员1->主题A}\n\t成员1->>协调者: syncGroup请求：你好协调者，我是成员1，也是这个组的Leader<br/>以下是我的分配方案：{成员1->主题A分区0}\n\t协调者-->>成员1: syncGroup响应：你好成员1，你负责消费主题A的0分区\n\t成员1->>协调者: 心跳请求：你好协调者，我是组内的成员1，我还活着\n\t协调者-->>成员1: 心跳响应：你好成员1，已收到\n```\n\n\n\n##### 场景四：重平衡时协调者对组内成员提交位移的处理\n\n```mermaid\nsequenceDiagram\n\t成员1->>协调者: 心跳请求：你好协调者，我是组内的成员1，我还活着\n  \t协调者-->>成员1: 心跳响应：你好成员1，已收到\n  \t协调者->>协调者: 此时发现需要重平衡\n\t成员1->>协调者: 心跳请求：你好协调者，我是组内的成员1，我还活着\n\t协调者->>成员1: 心跳响应：你好成员1，REBALANCE_IN_PROGRESS,重平衡，你需要重新入组\n\t成员1->>成员1: 必须赶在超时时间内提交位移\n\tnote right of 成员1 : 赶在超时时间内提交位移\n\t成员1->>协调者: 提交位移请求：你好协调者，我要提交的位移是：{....}\n\t协调者->>成员1: 提交位移响应：你好成员1，位移数据已收到\n\tnote right of 成员1 : 如果提交失败了，这部分数据在重平衡之后就会被重新消费\n```\n\n\n\n\n\n#### 重平衡的一些问题\n\njoingroup时等待所有消费者上报订阅信息，协调者通过什么判断所有消费者都已经上报了？\n\n- join group时也是有一个总的超时时间的（取所有member最大的rebalance超时时间），靠这个作为判断是否进入到下一阶段的阈值。\n- 如果在这次 Rebalance 期间，有 消费者 超时没有上报信息，那么这个消费者会被排除在这轮 Rebalance 之外\n\n如果在超时时间之后，排除在外的 消费者 此时上报了信息，怎么办？\n\n- 相当于 新成员入组，重新 Rebalance\n\n\n\n\n\n\n## 副本机制\n\n\n\n我们之前谈到过，Kafka 是有 Topic 概念的，而每个 Topic 又进一步划分成若干个 Partition。\n\n每个 Partition 配置有若干个 Replica，Replica 的概念实际上是在 Partition 层级下定义的\n\n\n\n### 什么是副本\n\n所谓副本（Replica），本质就是一个只能追加写消息的提交日志。\n\n同一个 Partition 下的所有 Replica 保存有相同的消息序列，这些 Replica 分散保存在不同的 Broker 上，从而能够对抗部分 Broker 宕机带来的数据不可用。\n\n在实际生产环境中，每台 Broker 都可能保存有各个 Topic 下不同 Partition 的不同 Replica，因此，单个 Broker 上存有成百上千个 Replica 的现象是非常正常的。\n\n下图展示的是一个有 3 台 Broker 的 Kafka 集群上的副本分布情况。\n\n从这张图中，我们可以看到，主题 1 分区 0 的 3 个副本分散在 3 台 Broker 上，其他主题分区的副本也都散落在不同的 Broker 上，从而实现数据冗余。\n\n<img src=\"kafka从入门到入土.assets/image-20230214160607518.png\" alt=\"image-20230214160607518\" style=\"zoom:33%;\" />\n\n\n\n### 副本之间数据是怎么同步的\n\n我们知道 Replica 是用来冗余数据的，同一个 Partiton 下的所有 Replica 的数据都应该是一模一样的，顺序都是一样的\n\n那么这么多的 Replica，是怎么进行 Replica 之间的数据同步的呢？\n\nKafka使用的解决方案：就是采用**基于领导者（Leader-based）的副本机制**\n\n<img src=\"kafka从入门到入土.assets/image-20230214161048286.png\" alt=\"image-20230214161048286\" style=\"zoom:33%;\" />\n\n- 在 Kafka 中，副本分成两类：领导者副本（Leader Replica）和追随者副本（Follower Replica）。每个分区在创建时都要选举一个 Leader Replica，剩余的是 Follower Replica\n- Kafka 的副本机制比其他分布式系统要更严格一些。在 Kafka 中，Follower Replica 是不对外提供服务的。所有的读写请求都必须由 Leader Replica所在的 Broker负责处理。而 Follower 的任务只有一个：就是从 Leader 异步拉取消息，并写入到自己的提交日志中，从而实现与 Leader 的同步。\n- 当 Leader Replica 挂掉了，或者说 Leader Replica 所在的 Broker 宕机时，Kafka 依托于 ZK 进行新的 Leader Replica 的选举\n\n你一定要特别注意上面的第二点，即追随者副本是不对外提供服务的。\n\n原因归咎于两点：方便 Read-your-writes ，同时方便实现单调读（Monotonic Reads）\n\n\n\n### Follower不提供服务的优点\n\n有两个好处\n\n#### 方便实现“Read-your-writes”\n\n所谓 Read-your-writes，顾名思义就是，当你使用生产者 API 向 Kafka 成功写入消息后，马上使用消费者 API 去读取刚才生产的消息。\n\n举个例子，比如你平时发微博时，你发完一条微博，肯定是希望能立即看到的，这就是典型的 Read-your-writes 场景。如果允许追随者副本对外提供服务，由于副本同步是异步的，因此有可能出现追随者副本还没有从领导者副本那里拉取到最新的消息，从而使得客户端看不到最新写入的消息。\n\n#### 方便实现单调读（Monotonic Reads）\n\n什么是单调读呢？就是对于一个消费者用户而言，在多次消费消息时，它不会看到某条消息一会儿存在一会儿不存在。\n\n如果允许追随者副本提供读服务，那么假设当前有 2 个追随者副本 F1 和 F2，它们异步地拉取领导者副本数据。倘若 F1 拉取了 Leader 的最新消息而 F2 还未及时拉取，那么，此时如果有一个消费者先从 F1 读取消息之后又从 F2 拉取消息，它可能会看到这样的现象：第一次消费时看到的最新消息在第二次消费时不见了，这就不是单调读一致性。但是，如果所有的读请求都是由 Leader 来处理，那么 Kafka 就很容易实现单调读一致性。\n\n\n\n\n\n现在我们知道了 Replica 的同步机制，和 Follower Replica 不对外提供服务的原因，接下来还有两个问题，我们一一来看：\n\nKafka 是怎么保证 Replica 的数据一致性的\n\n当 Leader Replica 挂掉之后，Kafka 是怎么进行选举新的 Leader Replica 的\n\n\n\n### 怎么保证 副本数据一致性\n\n我们知道 Kafka 的 Partition 有很多个 Replica\n\nReplica 分为 Leader Replica 和 Follower Replica\n\nLeader Replica 对外提供读写服务，Follower Replica 只是从 Leader Replica 异步同步数据，不对外提供任何服务；\n\n那么，Kakfa是怎么保证这些 Replica 内的数据是一致的呢？\n\n\n\n#### **In-Sync Replicas（ISR）**\n\n既然 Follower Replica 是异步的方式，从Leader Replica 同步数据的，那么就一定会存在延迟；\n\n就像 Mysql 的主从一样，主要涉及到数据同步，就一定会有延迟，无外乎：延迟的大小是多少。\n\n\n\nKafka 知道这个延迟是无法避免的，所以，Kafka 维护了一个集合，这个集合中保存的是：与Leader同步的Follower；\n\n什么是与Leader同步的Follower？有同步的Follower，难道还有不同步的Follower？\n\n- 是的，有同步的，就有不同步的。\n- Kafka 有自己一套判定条件，只要 Follower Replica 满足了这个判定条件，Kafka 就认为这个 Follower 是同步的。就会放进 ISR 集合；\n- 这个条件就是：replica.lag.time.max.ms，表示 Follower 副本能够落后 Leader 副本的最长时间间隔，默认是10S\n- 也就是说：当 Follower 与 Leader 的同步时间差，在10秒内，Kafka就认为这个 Follower 是同步的。否则就是不同步的\n- ISR 集合有什么用的，主要是用来选举新的 Leader Replica 的，后面会说\n\n\n\nISR 是一个动态调整的集合，当 Follower 落后于 Leader ，并且落后时间大于`replica.lag.time.max.ms`，Kafka 就会将这个 Follower 踢出 ISR；\n\n同样的，当一个落后的 Follower 最终追上了 Leader ，Kafka 会将这个 Follower 在加入 ISR；\n\n\n\n#### ISR是怎么变化的\n\nKafka 在启动的时候会开启两个任务\n\n一个任务用来定期地检查是否需要调整 ISR 集合，这个周期是replica.lag.time.max.ms的一半，默认5秒；\n\n当检测到 ISR 集合中有失效副本时，就会收缩 ISR 集合，当检查到有 Follower 的 HighWatermark （高水位）追赶上 Leader 时，就会扩充ISR。 \n\n除此之外，当 ISR 集合发生变更的时候。还会将变更后的记录缓存到 isrChangeSet 中\n\n另一个任务会周期性地检查 isrChangeSet，如果发现这个 isrChangeSet 有新的变更记录，那么它会在 ZK 中持久化一个节点。\n\n然后因为 Controller（Kafka 控制器） 在这个 ZK 节点的路径上注册了一个Watcher，所以它就能够感知到 ISR 的变化，并向它所管理的broker发送更新元数据的请求。最后删除该ZK节点。 \n\n\n\nLeader 副本天然就在 ISR 中\n\n极端的情况：ISR 包含全部的 Replica，也有可能 ISR 中一个 Replica 都没有，如果一个都没有的话，说明 Leader 都挂了，此时就需要选举新的 Leader了。\n\n\n\n### 怎么进行选举新的 Leader Replica\n\n选举的时候，是通过 Controller（Kafka控制器）来处理的。 Coordinator（协调者）只是消费者组用来重平衡的；这两个不是一个概念；\n\n当 ISR 不为空的时候，则选择其中一个作为新Leader，新的ISR则包含当前 ISR 中所有幸存的 Replica。\n\n当 ISR 为空的时候，此时幸存的 Replica 都是非同步副本，也就是说：都是和 老的 Leader Replica 差距比较大的 Replica，如果此时从这些 非同步副本 中选举一个作为 Leader 的话，就会有消息丢失的风险；如果不选举，那就是 Kafka 服务不可用了。\n\n当 ISR 为空的时候，如果进行选举，则这个选举叫做： Unclean 领导者选举，Broker 端参数 unclean.leader.election.enable 控制是否允许 Unclean 领导者选举\n\n如果开启了  Unclean 领导者选举 ，相当于选择了可用性，牺牲了一致性；如果不选举，相当于选择了一致性，牺牲了可用性\n\n建议不要开启，毕竟我们还可以通过其他的方式来提升高可用性。如果为了这点儿高可用性的改善，牺牲了数据一致性，那就非常不值当了。\n\n\n\n\n\n## 请求是怎么被处理的\n\n\n\n### kafka的请求分类\n\n之前，我们了解到 Kafka 是使用 TCP 进行通信，在TCP的基础上，Kafka定义了属于自己的请求协议：\n\n比如常见的 PRODUCE 请求是用于生产消息的，FETCH 请求是用于消费消息的，METADATA 请求是用于请求 Kafka 集群元数据信息的等等\n\n截止到 2.3 版本，总共有 45 种，在这 45 种请求中，可以分为两类：\n\n**数据类请求**：Kafka 社区把 PRODUCE 和 FETCH 这类请求称为数据类请求。\n\n**控制类请求**：Kafka 社区把 LeaderAndIsr、StopReplica 这类请求称为控制类请求。\n\n\n\n### 处理请求的方式\n\n在传统的开发设计中，对一个请求的处理，很自然的就可以想到下面两种方式\n\n#### 同步处理\n\n服务端收到一个消息，立即开始处理，处理完成后返回\n\n```java\n//伪代码\nwhile (true) {\n    Request request = accept(connection);\n    handle(request);\n}\n```\n\n这个方法实现简单，但是有个致命的缺陷，那就是吞吐量太差。由于只能顺序处理每个请求，因此，每个请求都必须等待前一个请求处理完毕才能得到处理。这种方式只适用于请求发送非常不频繁的系统。\n\n#### 异步处理\n\n既然同步的方式效率差， 那就用异步的方式\n\n```java\n//伪代码\nwhile (true) {\n    Request = request = accept(connection);\n    Thread thread = new Thread(() -> {\n        handle(request);});\n    thread.start();\n}\n```\n\n这个方法反其道而行之，完全采用异步的方式。系统会为每个入站请求都创建单独的线程来处理。这个方法的好处是，它是完全异步的，每个请求的处理都不会阻塞下一个请求。但缺陷也同样明显。为每个请求都创建线程的做法开销极大，在某些场景下甚至会压垮整个服务。\n\n\n\n### Kafka 是如何处理请求的\n\nKafka 使用 Reactor 模式来处理请求\n\n#### 什么是Reactor模式\n\nReactor 模式是 JUC 包的作者 Doug Lea 的作品，真不愧是大神。\n\n简单来说，Reactor 模式是事件驱动架构的一种实现方式，特别适合应用于处理多个客户端并发向服务器端发送请求的场景。\n\nReactor 模式的架构如下图所示，图来自 Doug Lea 的PPT：[https://gee.cs.oswego.edu/dl/cpjslides/nio.pdf](#https://gee.cs.oswego.edu/dl/cpjslides/nio.pdf)：\n\n<img src=\"kafka从入门到入土.assets/image-20230215172615962.png\" alt=\"image-20230215172615962\" style=\"zoom: 33%;\" />\n\n多个 Client 客户端会发送请求给到 Reactor。\n\nReactor 有个请求分发线程 Dispatcher ，也就是图中的 Acceptor 线程，它会将不同的请求下发到多个工作线程中处理。\n\nDispatcher 是 Reactor 模式的一个概念，它的实现是：Acceptor 线程，所以它俩是指同一个东西。\n\nAcceptor 线程只是用于请求分发，不涉及具体的逻辑处理，非常得轻量级，因此有很高的吞吐量表现。\n\n而工作线程可以根据实际业务处理需要任意增减，从而动态调节系统负载能力。\n\n\n\n#### Kafka 的Reactor模式\n\n<img src=\"kafka从入门到入土.assets/image-20230215173456675.png\" alt=\"image-20230215173456675\" style=\"zoom:33%;\" />\n\nKafka 的 Broker 端有个 SocketServer 组件，类似于 Reactor 模式中的 Dispatcher。\n\n它也有对应的 Acceptor 线程和一个工作线程池，只不过在 Kafka 中，这个工作线程池有个专属的名字，叫网络线程池。\n\nKafka 提供了 Broker 端参数 num.network.threads，用于调整该网络线程池的线程数。\n\nnum.network.threads 的默认值是 3，表示每台 Broker 启动时会创建 3 个网络线程，专门处理客户端发送的请求。\n\n\n\n\n\n\n\n##### 在Reactor模式下怎么处理请求\n\n<img src=\"kafka从入门到入土.assets/image-20230215192742666.png\" alt=\"image-20230215192742666\" style=\"zoom:50%;\" />\n\n\n\n上图，其中 1-7 步骤是处理请求， 7-10 是响应请求\n\n1、客户端或者其他Broker发起请求，这里的请求可能是 数据类请求，也可能是 控制类请求\n\n2、请求发送到 Broker，会由 SocketServer 组件开始处理\n\n3、SocketServer 组件（Acceptor线程）开始处理\n\n4、SocketServer 组件（Acceptor线程）会将请求分发到网络线程池，这是一个很轻量级的工作\n\n5、网络线程池中的某个线程接收到请求，但是这个线程并不会开始处理，而是将当前请求发送到共享请求队列\n\n6、Broker 端还有一个IO线程池，会不停的从共享请求队列中获取请求，这才是真正的开始处理请求\n\n- Broker 端参数 num.io.threads 控制了这个线程池中的线程数。\n- 目前该参数默认值是 8，表示每台 Broker 启动后自动创建 8 个 IO 线程处理请求\n- 你可以根据实际硬件条件设置此线程池的个数\n\n7、这个请求如果是 PRODUCE 请求，就写入日志；如果是 FETCH 请求，就从磁盘或者页缓存中读取数据\n\n\n\n##### 在Reactor模式下怎么响应请求\n\n<img src=\"kafka从入门到入土.assets/image-20230215192742666.png\" alt=\"image-20230215192742666\" style=\"zoom:50%;\" />\n\n\n\n上图，其中 1-7 步骤是处理请求， 7-10 是响应请求\n\n\n\n7、这个请求如果是 PRODUCE 请求，就写入日志；如果是 FETCH 请求，就从磁盘或者页缓存中读取数据\n\n8、如果当前这个请求**可以直接返回**，就会找到当时发送这个请求的线程，然后返回到这个线程的响应队列中\n\n- 什么是可以直接返回的请求？ \n\n- > 再讲什么是可以直接返回的请求之前，先了解一个什么是不可以直接返回的请求？\n  >\n  > 不可以直接返回的请求，比如设置了 acks=all 的 PRODUCE 请求\n  >\n  > 一旦设置了 acks=all，那么该请求就必须等待 ISR 中所有副本都接收了消息后才能返回\n  >\n  > 此时处理该请求的 IO 线程就必须等待其他 Broker 的写入结果\n  >\n  > 这就是不能直接返回的请求。\n  >\n  > 相反的，就是可以直接返回的请求\n\n- 响应队列是网络线程池中每个线程专属的吗？\n\n- > 是的。\n  >\n  > 请求队列是所有网络线程共享的，而响应队列则是每个网络线程专属的。\n  >\n  > 这么设计的原因就在于，Dispatcher 只是用于请求分发而不负责响应回传，因此只能让每个网络线程自己发送 Response 给客户端，所以这些 Response 也就没必要放在一个公共的地方。\n\n- 怎么找到当时发送这个请求的线程呢？\n\n- > 在源码中，有这部分代码逻辑：RequestChannel 类的 sendResponse 方法\n  >\n  > // 找出response对应的Processor线程，即request当初是由哪个Processor线程处理的 \n  >\n  > val processor = processors.get(response.processor) \n  >\n  > // 将response对象放置到对应Processor线程的Response队列中 \n  >\n  > if (processor != null) { \n  >\n  >   processor.enqueueResponse(response) \n  >\n  > }\n\n8、如果当前这个请求是**不可以直接返回的**，就会将当前这个请求暂存到 Purgatory\n\n- 什么是不可以直接返回的请求？ \n\n- > 不可以直接返回的请求，比如设置了 acks=all 的 PRODUCE 请求\n  >\n  > 一旦设置了 acks=all，那么该请求就必须等待 ISR 中所有副本都接收了消息后才能返回\n  >\n  > 此时处理该请求的 IO 线程就必须等待其他 Broker 的写入结果\n  >\n  > 这就是不能直接返回的请求。\n  >\n  > 相反的，就是可以直接返回的请求\n\n- Purgatory 是什么？\n\n- > Purgatory 的组件，这是 Kafka 中著名的“炼狱”组件。\n  >\n  > 它是用来缓存延时请求（Delayed Request）的。\n  >\n  > 所谓延时请求，就是那些一时未满足条件，不能立刻处理的请求。\n\n9、等 Purgatory 中暂存的请求，可以返回的时候，会找到当时发送这个请求的线程，返回到这个线程的响应队列中\n\n- 怎么知道是可以返回的时候？\n\n- > 举个例子：比如设置了 acks=all 的 PRODUCE 请求\n  >\n  > 一旦设置了 acks=all，那么该请求就必须等待 ISR 中所有副本都接收了消息后才能返回\n  >\n  > 此时处理该请求的 IO 线程就必须等待其他 Broker 的写入结果\n  >\n  > 此时才可以返回\n\n10、网络线程池的线程，会将自己响应队列中的响应数据，通过网络传输回去。\n\n\n\n\n\n### 控制类和数据类请求分离\n\n在本小节开头，就介绍过：Kafka 的请求分类两类\n\n**数据类请求**：Kafka 社区把 PRODUCE 和 FETCH 这类请求称为数据类请求。\n\n**控制类请求**：Kafka 社区把 LeaderAndIsr、StopReplica 这类请求称为控制类请求。\n\n在了解了 Kafka 是怎么处理请求的流程之后，思考这么一个问题：\n\n如果当前 共享请求队列 中，已经积压了很多的数据，IO线程正在马不停蹄的处理，此时我们发送一个请求：要求一个 Replica 下线。\n\n此时：这个要求 Replica 下线的请求是优先处理，还是顺序处理？\n\n如果是优先处理，那共享请求队列中积压的数据，怎么办？\n\n如果是顺序处理，如果等待的时间很长很长，Replica 一直无法下线怎么办？\n\n\n\n所以我们需要把 控制类请求 和 数据类请求 **分开处理**。\n\n这就是 控制类和数据类请求分离\n\n\n\n#### 场景\n\n举一个具体的场景：\n\n假设我们有个主题只有 1 个分区，该分区配置了 2 个副本\n\n其中 Leader 副本保存在 Broker 0 上，Follower 副本保存在 Broker 1 上\n\n假设 Broker 0 这台机器积压了很多的 PRODUCE 请求\n\n此时你如果使用 Kafka 命令强制将该主题分区的 Leader、Follower 角色互换\n\n那么 Kafka 内部的控制器组件（Controller）会发送 LeaderAndIsr 请求给 Broker 0，显式地告诉它，当前它不再是 Leader，而是 Follower 了\n\n而 Broker 1 上的 Follower 副本因为被选为新的 Leader，因此停止向 Broker 0 拉取消息\n\n\n\n#### 不分离的现象\n\n如果 控制类请求 和 数据类请求 不分离\n\nLeaderAndIsr 请求（ Leader、Follower 角色互换）就会放在 共享请求队列 的后面\n\n如果 共享请求队列 积压了很多，我们就要等很长很长时间，才能处理到这个 控制类请求\n\n很显然，这不是我们想要的结果\n\n\n\n#### 分离的现象\n\n如果 控制类请求 和 数据类请求 分离\n\n那么在 LeaderAndIsr 发送之前积压的 PRODUCE 请求就都无法正常完成了。\n\n\n\n这是我们想要的结果吗？\n\n我不知道这是不是我们想要的结果，但这是目前 Kafka 的处理方式\n\nKafka 会优先处理 LeaderAndIsr 请求，Broker 0 就会立刻抛出 NOT_LEADER_FOR_PARTITION 异常，快速地标识这些积压 PRODUCE 请求已失败\n\n\n\n#### 怎么设计分离\n\n现在我们知道 Kafka 会优先处理 控制类请求，如果是你来设计，你会怎么让 Kafka 优先处理 控制类请求呢？\n\n方案一：\n\n在 Broker 中实现一个优先级队列，并赋予控制类请求更高的优先级。\n\n这是很自然的想法，所以我本以为社区也会这么实现的，但后来我这个方案被清晰地记录在“已拒绝方案”列表中。\n\n拒绝的原因在于，它无法处理请求队列已满的情形。当请求队列已经无法容纳任何新的请求时，纵然有优先级之分，它也无法处理新的控制类请求了\n\n\n\n方案二：\n\n直接将 控制类请求 替换 共享请求队列中 的最前面的数据，这样就可以优先处理控制类请求了，处理完控制类请求，再将这个数据类请求插队到队头；\n\n这个方案是网友分享的，当然，Kafka 社区也没有采用这个方案\n\n\n\n那么 Kafka 是怎么做的呢？\n\n\n\n#### Kafka是怎么分离的\n\n那么，社区是如何解决的呢？\n\n很简单，Kafka 社区实现了两套一模一样的 Reactor模型\n\n一个用来处理 数据类型请求，一个用来处理 控制类请求；实现了两类请求的分离。\n\n也就是说，Kafka Broker 启动后，会在后台分别创建两套网络线程池和 IO 线程池的组合，它们分别处理数据类请求和控制类请求。\n\n至于所用的 Socket 端口，自然是使用不同的端口了，你需要提供不同的 listeners 配置，显式地指定哪套端口用于处理哪类请求。\n\n\n\n\n\n## kafka控制器\n\n\n\n### 什么是控制器组件\n\n控制器组件（Controller），是 Apache Kafka 的核心组件\n\n它的主要作用是在 ZK 的帮助下管理和协调整个 Kafka 集群\n\n集群中任意一台 Broker 都能充当控制器的角色，只能有一个 Broker 成为控制器\n\n每个正常运转的 Kafka 集群，在任意时刻都有且只有一个控制器\n\n控制器是重度依赖 ZK 的，因此，我们有必要花一些时间学习下 ZK 是做什么的。\n\n\n\n### Kafka依赖ZK\n\n#### 什么是ZK\n\nZK 是一个分布式协调服务框架，它使用的数据模型类似于文件系统的树形结构，根目录也是以“/”开始\n\nZK结构上的每个节点被称为 znode，用来保存一些元数据协调信息，可分为持久性 znode 和临时 znode\n\n- 持久性 znode 不会因为 ZooKeeper 集群重启而消失\n\n- 临时 znode 则与创建该 znode 的 ZooKeeper 会话绑定，一旦会话结束，该节点会被自动删除\n\nZK 赋予客户端监控 znode 变更的能力，即所谓的 Watch 通知功能。\n\n- 一旦 znode 节点被创建、删除，子节点数量发生变化，或是 znode 所存的数据本身变更\n- ZK 会通过节点变更监听器 (ChangeHandler) 的方式显式通知客户端。\n\n\n\n#### Kafka使用ZK做了什么\n\nKafka 大量使用了 ZK 的 Watch 机制对集群进行管理，如下图\n\n不用了解每个 znode 的作用，但可以大致体会下 Kafka 对 ZooKeeper 的依赖。\n\n```mermaid\ngraph LR;\n \tKafka的ZK节点 --> /consumers\n \t/consumers --> consumer_group_name\n \tconsumer_group_name --> offsets\n \tconsumer_group_name --> ids\n \tconsumer_group_name --> owners\n\tKafka的ZK节点 --> /controller_epoch\n\tKafka的ZK节点 --> /brokers/ids\n\tKafka的ZK节点 --> /controller\n\tKafka的ZK节点 --> /admin/delete_topics\n\tKafka的ZK节点 --> /admin/preferred_replica_election\n\tKafka的ZK节点 --> /brokers/seqid\n\tKafka的ZK节点 --> /isr_change_notifaction\n\tKafka的ZK节点 --> /config\n\t/config --> clients\n\t/config --> changes\n\t/config --> topics\n\tKafka的ZK节点 --> /config/changes\n\tKafka的ZK节点 --> /brokers/topics\n\tKafka的ZK节点 --> /admin/reassign_partitions\n\tKafka的ZK节点 --> /admin\n\t/admin --> delete_topics\n```\n\n在之前我们说过，每个 Broker 都有成为 控制器（Controller）的能力，但是控制器有且只有一个，那么控制器是怎么被选出来的\n\n\n\n### 控制器是怎么选出来的\n\n在之前我们说过，每个 Broker 都有成为 控制器（Controller）的能力\n\n但是控制器有且只有一个，那么控制器是怎么被选出来的\n\n\n\n实际上，Broker 在启动时，会尝试去 ZK 中创建 /controller 节点\n\nKafka 当前选举控制器的规则是：第一个成功创建 /controller 节点的 Broker 会被指定为控制器。\n\n\n\n### 控制器用来做什么的\n\n之前说过，控制器（Controller）起到了管理整个集群的作用，那么它具体都做了什么呢？\n\n```mermaid\ngraph LR;\n\tKafka控制器 --> 主题管理\n\tKafka控制器 --> 分区重分配\n\tKafka控制器 --> Preferred领导者选举\n\tKafka控制器 --> 集群成员管理\n\tKafka控制器 --> 数据服务\n\t\n```\n\n### 控制器中存了什么数据\n\n接下来，我们就详细看看，控制器中到底保存了哪些数据\n\n控制器既然作为整个 Kafka 集群的管理者，里面主要存了三部分数据\n\n- 所有 Topic 信息\n  - 某个 Topic 下的所有副本\n  - 某个 Topic 的所有分区\n  - 所有的 Topic 列表\n  - 移除某个 Topic 的所有信息\n- 所有 Broker 信息\n  - 该 Broker 下的所有分区\n  - 某组 Broker 的所有副本\n  - 正在关闭的 Broker 列表\n  - 当前存活的 Broker 列表\n- 所有涉及运维的 Partiton 信息\n  - 当前存活的所有 Partiton 副本\n  - 正在进行重分配的 Partiton 列表\n  - 某组 Partiton 下的所有副本\n  - 正在进行 preferred leader 选举的Partition\n  - 分配给每个 Partition 的副本列表\n  - 每个 Partition 的 Leader 和 ISR 信息\n\n```mermaid\ngraph LR;\n\t控制器数据 --> 所有Topic信息\n\t所有Topic信息 --> 某个Topic下的所有副本\n\t所有Topic信息 --> 某个Topic的所有分区\n\t所有Topic信息 --> 所有的Topic列表\n\t所有Topic信息 --> 移除某个Topic的所有信息\n\t控制器数据 --> 所有Broker信息\n\t所有Broker信息 --> 该Broker下的所有分区\n\t所有Broker信息 --> 某组Broker的所有副本\n\t所有Broker信息 --> 正在关闭的Broker列表\n\t所有Broker信息 --> 当前存活的Broker列表\n\t控制器数据 --> 所有涉及运维的Partiton信息\n\t所有涉及运维的Partiton信息 --> 当前存活的所有Partiton副本\n\t所有涉及运维的Partiton信息 --> 正在进行重分配的Partiton列表\n\t所有涉及运维的Partiton信息 --> 某组Partiton下的所有副本\n\t所有涉及运维的Partiton信息 --> 正在进行preferredleader选举的Partition\n\t所有涉及运维的Partiton信息 --> 分配给每个Partition的副本列表\n\t所有涉及运维的Partiton信息 --> 每个Partition的Leader和ISR信息\n```\n\n值得注意的是，这些数据其实在 ZooKeeper 中也保存了一份\n\n每当控制器初始化时，它都会从 ZooKeeper 上读取对应的元数据并填充到自己的缓存中\n\n\n\n\n\n\n\n### 控制器故障转移\n\n我们在前面强调过，在 Kafka 集群运行过程中，只能有一台 Broker 充当控制器的角色\n\n那么这就存在单点失效（Single Point of Failure）的风险\n\nKafka 是如何应对单点失效的呢？\n\n\n\n当运行中的控制器突然宕机或意外终止时，Kafka 能够快速地感知到，并立即启用备用控制器来代替之前失败的控制器\n\n\n\n<img src=\"kafka从入门到入土.assets/image-20230218120514451.png\" alt=\"image-20230218120514451\" style=\"zoom: 23%;\" />\n\n\n\n最开始时，Broker 0 是控制器。\n\n当 Broker 0 宕机后，此时 ZK 就会通过 Watch 机制感知到并删除了 /controller 临时节点。\n\n之后所有存活的 Broker 就会收到通知，此时所有存活的 Broker 开始竞选新的控制器身份。\n\nBroker 3 最终赢得了选举，成功地在 ZK 上重建了 /controller 节点。\n\n之后，Broker 3 会从 ZK 中读取集群元数据信息，并初始化到自己的缓存中。\n\n至此，控制器的 Failover 完成，可以行使正常的工作职责了。\n\n\n\n### 控制器内部设计原理\n\n<font color='red'>**这部分只是根据部分文章，自己总结出来的，有些逻辑点说不通，后续还需要通过看 Kafka 的源码再补充完善这部分**</font>\n\n<font color='red'>**这部分只是根据部分文章，自己总结出来的，有些逻辑点说不通，后续还需要通过看 Kafka 的源码再补充完善这部分**</font>\n\n<font color='red'>**这部分只是根据部分文章，自己总结出来的，有些逻辑点说不通，后续还需要通过看 Kafka 的源码再补充完善这部分**</font>\n\n\n\n#### 老版本多线程设计\n\n在 Kafka 0.11 版本之前，Kafka 控制器是一个**模拟状态机的多线程控制器**。 \n\n<img src=\"kafka从入门到入土.assets/image-20230219143821049.png\" alt=\"image-20230219143821049\" style=\"zoom:50%;\" />\n\n\n\n它以下列方式工作：\n\n既然是模拟状态机，那么都有哪些状态呢？\n\n**需要维持的状态**：这些状态是存储在 <font color='purple'>**Controller Context**</font> 中的\n\n- 每台机器上的分区副本。\n- 分区的领导者。\n\n**什么会导致这些状态变化呢（状态变化源）**\n\n- <font color='#FFF2CC'>注册到 Zookeeper 的监听器线程。</font>\n  - <font color='#FFF2CC'>AddPartitionsListener</font>>\n  - <font color='#FFF2CC'>BrokerChangeListener</font>\n  - <font color='#FFF2CC'>DeleteTopicListener</font>\n  - <font color='#FFF2CC'>PartitionReassignedListener（admin）</font>\n  - <font color='#FFF2CC'>PreferredReplicaElectionListener（admin）</font>\n  - <font color='#FFF2CC'>ReassignedPartitionsIsrChangeListener</font>\n  - <font color='#FFF2CC'>TopicChangeListener</font>\n- <font color='#FFF2CC'>controller与broker之间的socket连接 (controlled shutdown)</font>\n- <font color='#FFE6CC'>内部定时任务 **Schedule Task**线程（比如：preferred leader 选举）</font>\n\n\n\n**状态是怎么变化的呢**\n\n- ZK的监听线程，Kafka的API线程，内部定时任务的线程 都会**同时**改变状态\n- 就是这些线程会同时修改 <font color='purple'>**Controller Context**</font> \n\n\n\n**状态是怎么传播的**\n\n- 通过controller与broker之间的socket连接，进行状态的传输\n- 通过controller与broker之间的消息队列\n- 发送给broker的异步消息\n- 不需要回调的消息（主题删除的除外）\n\n\n\n**故障转移**\n\n- 基于 Zookeeper 的领导者选举\n- Zookeeper 作为容错的持久状态存储。\n\n\n\n**这个设计的缺陷**\n\n控制器是多线程的设计，会在内部创建很多个线程。比如：\n\n- 控制器需要为每个 Broker 都创建一个对应的 Socket 连接\n- 控制器向这些 Broker 发送特定请求，会创建专属的列表\n- 控制器连接 ZooKeeper 的会话，也会创建单独的线程来处理 Watch 机制的通知回调\n- 控制器还会为主题删除创建额外的 I/O 线程\n- 多线程访问共享可变数据（控制器缓存）是维持线程安全最大的难题\n- 为了保护数据安全性，控制器不得不在代码中大量使用 ReentrantLock 同步机制，这就进一步拖慢了整个控制器的处理速度\n\n\n\n\n\n#### 新版本单线程加时间队列设计\n\nkafka 设计原文：https://cwiki.apache.org/confluence/display/kafka/kafka+controller+redesign\n\n鉴于这些原因，社区于 0.11 版本重构了控制器的底层设计，主要改进了两点\n\n- 把多线程的方案改成了单线程加事件队列的方案。\n- 将 Broker 与 Zookeeper 的连接从同步改成了异步\n\n<img src=\"kafka从入门到入土.assets/image-20230219144633226.png\" alt=\"image-20230219144633226\" style=\"zoom:35%;\" />\n\n\n\n将之前的 **状态变化源** 所涉及到的线程，抽象成 一个一个的**事件 Event**\n\n之前状态的变化，是通过线程直接操作控制器缓存\n\n现在状态发生了变化，会将这个变化抽象成事件，放在事件队列中\n\n由一个 <font color='green'>Event Execotor Thread</font> 单独处理，并操作 <font color='purple'>**Controller Context**</font> \n\n此时的 zk 线程只负责 <font color='purple'>**Controller Context**</font> 更新而不负责事件 Event 执行。\n\n\n\n针对控制器的第二个改进就是，将之前同步操作 ZooKeeper 全部改为异步操作。\n\nZooKeeper 本身的 API 提供了同步写和异步写两种方式。\n\n之前控制器操作 ZooKeeper 使用的是同步的 API，性能很差，集中表现为，当有大量主题分区发生变更时，ZooKeeper 容易成为系统的瓶颈。\n\n新版本 Kafka 修改了这部分设计，完全摒弃了之前的同步 API 调用，转而采用异步 API 写入 ZooKeeper，性能有了很大的提升。\n\n根据社区的测试，改成异步之后，ZooKeeper 写入提升了 10 倍！\n\n\n\n\n\n### 如何处理脑裂\n\n如果 Controller 挂掉了，Kafka集群必须找到可以替代的 controller，否则集群将不能正常运转。\n\n这里面存在一个问题，很难确定 Broker 是挂掉了，还是仅仅只是短暂性的故障。\n\n但是，不管是哪种情况，集群为了正常运转，必须选出新的controller。\n\n但是如果老的 controller 又正常了，他并不知道自己已经被取代了，那么此时集群中会出现两台controller。\n\n\n\n#### 什么是脑裂\n\n比如，某个 controlle r由于 GC 时间比较久，而被认为已经挂掉，并选择了一个新的controller。\n\n在 GC 的情况下，在老的 Controller眼中，并没有改变任何东西，该 Broker 甚至不知道它已经暂停了。\n\n因此，它将继续充当当前 Controller，但是此时系统中已经选择了另外一个新的 Controller\n\n对于当前 Kafka 集群来说，就有了两个 Controller，有了两个大脑，就是脑裂了。\n\n这是分布式系统中的常见情况，称为脑裂。\n\n\n\n#### 如何解决脑裂\n\nKafka是通过使用**epoch number**（纪元编号，也称为隔离令牌）来完成的。\n\nepoch number只是单调递增的数字，第一次选出Controller时，epoch number值为1\n\n如果再次选出新的Controller，则epoch number将为2，依次单调递增\n\n\n\n简单的说，就是老的 controller 复活之后，它的 epoch 还是老的值，它所下发的命令携带的 epoch 还是老的值\n\n当它的命令到达 broker 端之后，broker 发现有另一个 controller 发来的消息的epoch是新的值\n\n就不会执行老的 controller 的命令。\n\n\n\n\n\n## 关于高水位和Leader Epoch的讨论\n\n\n\n高水位和低水位分别是什么\n\nHW\n\nLEO（Log End Offet）\n\nLSO（Log Stable Offset）：事务生产者\n\n\n\n每个 Replica 都有自己的 HW 和 LEO；\n\nPartiton 的 HW 就是 Leader Replica 的HW；\n\nLeader Replica 所在的 Broker 除了保存当前 Replica 的 HW 和 LEO 之外，还保存了 它的 Follower Replica 的 HW 和 LEO\n\n\n\n\n\n\n\n\n\n\n\n## 问题\n\n\n\n为什么kafka不像mysql那样允许追随者副本（follower replica）对外提供只读服务？\n\nkafka是怎么做到 提供一套 API 实现生产者和消费者？\n\nkafka是怎么做到 降低网络传输和磁盘存储开销；\n\nkafka是怎么做到 实现高伸缩性架构。\n\nkafka为什么快，为什么高吞吐？\n\n- 消息日志（Log）只能追加写，避免了随机IO，改成了顺序IO，大大提高了写能力；\n\n你觉得 Kafka 未来的演进路线是怎么样的？如果你是 Kafka 社区的“掌舵人”，你准备带领整个社区奔向什么方向呢？\n\n想你是一家创业公司的架构师，公司最近准备改造现有系统，引入 Kafka 作为消息中间件衔接上下游业务。作为架构师的你会怎么选择合适的 Kafka 发行版呢\n\nkafka每天 1 亿条 1KB 大小的消息，保存两份且留存两周的时间，需要多大的磁盘空间？\n\n如果需要kafka1小时内处理1TB的业务数据，在千兆网络下，需要多少台kafka机器？\n\nkafka怎么实现的故障转移？\n\nkafka是怎么保障大数据量均匀的分布在各个Broker上的？\n\nkafka的零拷贝技术是什么？\n\n- https://blog.csdn.net/ljheee/article/details/99652448\n- https://www.jianshu.com/p/835ec2d4c170\n\nbroker端收到消息也会解压缩，进行消息校验，那么零拷贝还有用嘛？\n\nconsumer可以先提交offset，在处理消息嘛？\n\nkafka的producer是在producer实例化的时候，创建的TCP连接，那么这个时候，producer都不知道要往那个topic发消息，那么就不知道要连接到哪个broker？kafka是怎么做的呢？\n\nkafka在建立TCP连接的步骤中，有没有可以优化的地方，目前社区做的不好的地方？\n\n丰网的kafka的消费者重复注册是怎么做的？是同一个消费者实例的多个线程，还是同一个消费者类，注册了多个bean；\n\n重试机制会导致消息乱序吗？\n\n- 重试机制不会重新计算Partition信息\n- 重试机制会导致消息乱序，但是可以通过 max.in.flight.requests.per.connection=1 来避免，但是会导致吞吐量下降\n- max.in.flight.requests.per.connection：表示限制客户端在单个连接上能够发送的未响应请求的个数；\n- 设置为 1 表示：broker收到一个请求之后，在响应之前，是不会接收别的请求的\n\n消息的分区位移是什么时候写入的？\n\n如果一个消息写入失败了，Producer 有重试，它的Offset是新的，还是老的？\n\nConsumer设置自动提交位移，有一个提交频率，具体的流程是怎么提交的，如果消费到了Producer重试的消息，Offset会怎么样？\n\n","tags":["kafka","生产者","消费者","消息","中间件"],"categories":["JAVA","消息中间件","KAFKA"]},{"title":"spring注解AOP开发和源码解读及实践","url":"/note/JAVA/SSM三大框架/【spring】spring注解AOP开发和源码解读及实践/","content":"\n\n\n# spring注解AOP开发和源码解读及实践\n\n本文主要介绍spring的aop，基于注解和XML的简单使用和源码解读, 本文涉及的所有图片，如果不清晰，可以下载PDF文件进行查看：\n\n## AOP的使用\n\n\n\n在了解使用之前，我们需要先了解一下：execution表达式\n\n### execution表达式\n\n```java\n// 任意公共方法的执行：\nexecution(public * *(..))\n\n//任何一个以“set”开始的方法的执行：\nexecution(* set*(..))\n\n//AccountService 接口的任意方法的执行：\nexecution(* com.xyz.service.AccountService.*(..))\n\n//定义在service包里的任意方法的执行：\nexecution(* com.xyz.service.*.*(..))\n\n//定义在service包和所有子包里的任意类的任意方法的执行：\nexecution(* com.xyz.service..*.*(..))\n\n//定义在pointcutexp包和所有子包里的JoinPointObjP2类的任意方法的执行：\nexecution(* com.test.spring.aop.pointcutexp..JoinPointObjP2.*(..)))\n\n\n```\n\n\n\n### 基于注解的使用\n\n目前最火的使用方式就是基于注解的使用方式，避免了大量的配置文件，而且易于管理和维护。简单明了，推荐使用。\n\n#### 导入aop所必须的最小maven依赖\n\n- 1.spring的aop依赖于spring的ioc容器，所以需要导入spring-context,同时spirng-context中已经引入了spring-aop，所以就不需要单独的引入spring-aop了。\n\n- 2.spring的aop依赖于强大的AspectJ，所以需要引入aspectjweaver的依赖，但是spring-aspects已经加入了这个依赖，所以，只需要再引入spring-aspects就可以了。\n\n- 3.关于为什么只引入这两个依赖，请移步：[spring-aop和aspectJ的关系](#spring-aop和aspectJ的关系)\n\n- 4.最后，我们自己会编写了一些测试方法，所以需要引入Junit的依赖。\n\n```xml\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-context</artifactId>\n            <version>4.3.12.RELEASE</version>\n        </dependency>\n\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-aspects</artifactId>\n            <version>4.3.12.RELEASE</version>\n        </dependency>\n\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.12</version>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n```\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/image-20221222163808764.png\" alt=\"image-20221222163808764\" style=\"zoom:80%;\" />\n\n\n\n#### 实现业务逻辑类\n\n- 在这个实例中，具体的业务逻辑类是： com.zspc.core.spring.aop.service.Calculator\n\n- 我们想要的目的是：在业务逻辑运行的时候将日志进行打印（方法之前、方法运行结束、方法出现异常，等等等）\n\n```java\npublic class Calculator {\n    /**\n     * 计算两个数的除法\n     */\n    public int div(int a, int b){\n        System.out.println(\"开始计算-->除数:\"+a+\",被除数:\"+b+\".\");\n        return a/b;\n    }\n}\n```\n\n#### 实现具体的日志切面类\n\n- 之前说到，我们的目的是：在业务逻辑类运行的时候将日志进行打印（方法之前、方法运行结束、方法出现异常，等等等）\n\n- 所以我们需要一个切面类，切面类里面的方法需要动态感知Calculator.div(int,int)方法运行到哪里然后执行通知方法；\n\n- 环绕通知：目标方法运行前后都运行，需要手动运行joinPoint.proceed()，才能推进目标方法的执行，对应切面类中的logAround()\n\n- 前置通知：目标方法运行之前运行，对应切面类中的logStart()\n\n- 后置通知：目标方法运行之后，结束之前（无论方法正常结束(return)还是异常结束(exception)）运行，对应切面类中的logAfter()\n\n- 返回通知：目标方法返回之后运行，对应切面类中logReturn()\n\n- 异常通知：目标方法发生异常的时候运行，该异常运行后，返回通知不会运行，对应切面类中的logException()\n\n- 执行流程：环绕通知开始-->前置通知-->环绕通知joinPoint.proceed()-->环绕通知结束-->后置通知-->返回通知/异常通知\n\n```java\n\npublic class LogAspect {\n\n    public void logStart(JoinPoint joinPoint) {\n        //...\n    }\n\n    public void logEnd(JoinPoint joinPoint) {\n        //...\n    }\n\n    public void logReturn(JoinPoint joinPoint, Object result) {\n        //...\n    }\n\n    public void logException(JoinPoint joinPoint, Exception exception) {\n        //...\n    }\n\n\n    public Object logAround(ProceedingJoinPoint joinPoint){\n        //...\n    }\n}\n```\n\n#### 对切面类的方法添加注解，标注执行时机\n\n- 对切面类添加注解,并指定切面\n\n  - @Around\n\n  - @Before\n\n  - @After\n\n  - @AfterReturning\n\n  - @AfterThrowing\n\n- 指定切面有两种方法\n\n  - 定义一个公共的切面方法，@Pointcut(\"execution (xxxxx)\")，并在切面类注解中引入\n\n  - 直接在切面类注解中指定切面：@Before(\"com.xxx.xxx.xxx()\")\n\n```java\npublic class LogAspect {\n\n    //抽取公共的切入点表达式\n    @Pointcut(\"execution(* com.zspc.core.spring.aop.service.Calculator.*(..))\")\n    public void pointCut() {}\n\n    @Before(\"pointCut()\")\n    public void logStart(JoinPoint joinPoint) {\n        //...\n    }\n\n    @After(\"pointCut()\")\n    public void logEnd(JoinPoint joinPoint) {\n        //...\n    }\n\n    @AfterReturning(value = \"pointCut()\", returning = \"result\")\n    public void logReturn(JoinPoint joinPoint, Object result) {\n        //....\n    }\n\n    @AfterThrowing(value = \"pointCut()\", throwing = \"exception\")\n    public void logException(JoinPoint joinPoint, Exception exception) {\n        //...\n    }\n\n    @Around(value = \"pointCut()\")\n    public Object logAround(ProceedingJoinPoint joinPoint){\n        //...\n    }\n\n}\n\n```\n\n#### 告诉spring哪个类是切面类\n\n- 就是给切面类加上 @Aspect 注解，让spirng容器知道这是一个切面类。\n\n```java\n@Aspect\npublic class LogAspect {\n\n    //....省略...\n\n}\n\n```\n\n#### 将切面类和业务逻辑类纳入spirng管理\n\n- 就是在配置类中添加@Bean\n\n```sql\n@Configuration\npublic class MainConfig {\n\n\n    @Bean\n    public LogAspect logAspect() {\n        return new LogAspect();\n    }\n\n\n    @Bean\n    public Calculator calculator() {\n        return new Calculator();\n    }\n\n}\n\n```\n\n#### 开启基于注解的aop模式\n\n- 给配置类添加@EnableAspectJAutoProxy,，这样spring才能识别所有的aop注解。\n\n```java\n@Configuration\n@EnableAspectJAutoProxy\npublic class MainConfig {\n\n\n    @Bean\n    public LogAspect logAspect() {\n        return new LogAspect();\n    }\n\n\n    @Bean\n    public Calculator calculator() {\n        return new Calculator();\n    }\n\n}\n```\n\n\n\n#### 最后一步，编写测试类，进行测试\n\n- 测试类\n\n```java\npublic class AOPTest {\n    @Test\n    public void testAop() {\n        ApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfig.class);\n        Calculator calculator = (Calculator) applicationContext.getBean(\"calculator\");\n        int div = calculator.div(2, 1);\n        System.out.println(div);\n    }\n}\n```\n\n- 测试结果\n\n```java\n环绕通知开始\n前置通知运行。。。参数列表是：{[12, 5]}\n开始计算-->除数:12,被除数:5.\n环绕通知结束\n后置通知运行。。。@After\n返回通知运行。。。@AfterReturning:运行结果：{2}\n2\n```\n\n### 基于XML的使用\n\n#### 略\n\n这个就不说了，和上面差不多\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \n       xmlns:aop=\"http://www.springframework.org/schema/aop\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/beans \n       http://www.springframework.org/schema/beans/spring-beans.xsd \n       http://www.springframework.org/schema/aop \n       http://www.springframework.org/schema/aop/spring-aop.xsd\">\n\n\n    <bean id=\"logAspect\" class=\"com.zspc.core.spring.aop.config.LogAspect\"/>\n\n    <bean id=\"calculator\" class=\"com.zspc.core.spring.aop.service.Calculator\"/>\n\n    <aop:config>\n        <aop:aspect ref=\"logAspect\">\n            <aop:pointcut expression=\"execution (* com.zspc.core.spring.aop.service.Calculator.*(..))\" id=\"pointCut\"/>\n            <aop:before pointcut-ref=\"pointCut\" method=\"logStart\"/>\n            <aop:after-returning pointcut-ref=\"pointCut\" method=\"logReturn\"/>\n            <aop:after-throwing pointcut-ref=\"pointCut\" method=\"logException\"/>\n            <!--<aop:after pointcut-ref=\"pointCut\" method=\"doAfter\"/>-->\n            <!--<aop:around pointcut-ref=\"pointCut\" method=\"doAround\"/>-->\n        </aop:aspect>\n    </aop:config>\n\n\n</beans>\n```\n\n\n\n## AOP的源码流程分析\n\n对于任何的源码分析，都做到三步分析，就可以非常明确了：\n\n- 看给容器中注入了什么组件\n\n- 这个组件什么时候工作\n\n- 这个组件的功能是什么\n\n\n\n从 `@EnableAspectJAutoProxy ` 开始\n\n### @EnableAspectJAutoProxy 是什么，干啥用的\n\n结论先行：`@EnableAspectJAutoProxy`的作用就是为了给我们的容器中注入一个：`AnnotationAwareAspectJAutoProxyCreator`\n\n先看看这个注解类`EnableAspectJAutoProxy.java`的源码:\n\n```java\npackage org.springframework.context.annotation;\n\nimport java.lang.annotation.Documented;\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\n\n@Target({ElementType.TYPE})\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\n@Import({AspectJAutoProxyRegistrar.class})\npublic @interface EnableAspectJAutoProxy {\n    boolean proxyTargetClass() default false;\n\n    boolean exposeProxy() default false;\n}\n\n```\n\n- `EnableAspectJAutoProxy.java`类上有一个注解： `@Import({AspectJAutoProxyRegistrar.class})`： \n\n    - `@Import`这个注解给容器中导入了一个组件 `AspectJAutoProxyRegistrar`\n\n- `AspectJAutoProxyRegistrar`这个组件是干嘛呢？ 我们点进去看他的继承关系，源码如下：\n\n    - ```java\n        package org.springframework.context.annotation;\n        \n        import org.springframework.aop.config.AopConfigUtils;\n        import org.springframework.beans.factory.support.BeanDefinitionRegistry;\n        import org.springframework.core.annotation.AnnotationAttributes;\n        import org.springframework.core.type.AnnotationMetadata;\n        \n        class AspectJAutoProxyRegistrar implements ImportBeanDefinitionRegistrar {\n            AspectJAutoProxyRegistrar() {\n            }\n        \n            public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) {\n                AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(registry);\n                AnnotationAttributes enableAspectJAutoProxy = AnnotationConfigUtils.attributesFor(importingClassMetadata, EnableAspectJAutoProxy.class);\n                if (enableAspectJAutoProxy != null) {\n                    if (enableAspectJAutoProxy.getBoolean(\"proxyTargetClass\")) {\n                        AopConfigUtils.forceAutoProxyCreatorToUseClassProxying(registry);\n                    }\n        \n                    if (enableAspectJAutoProxy.getBoolean(\"exposeProxy\")) {\n                        AopConfigUtils.forceAutoProxyCreatorToExposeProxy(registry);\n                    }\n                }\n            }\n        }\n        \n        ```\n\n    - `class AspectJAutoProxyRegistrar implements ImportBeanDefinitionRegistrar ` 这个类继承自 `ImportBeanDefinitionRegistrar`\n\n    - 我们发现他是一个 `ImportBeanDefinitionRegistrar `，通过之前的学习，我们知道`ImportBeanDefinitionRegistrar`的作用是：\n        - 使用`@Import`的时候，可以指定`ImportBeanDefinationRegitrar.`\n        - 自定义一个类实现`ImportBeanDefinationRegistrar`接口,并实现`resisterBeanDefinatons`方法，在这个方法里面，可以指定需要注册的组件。\n        - 使用`ImportBeanDefinationRegistrar`,可以指定bean名，以及作用域之类的，比之前两种方式拥有更多的定制性\n        - 关于这些作用，看不懂，没关系，可以参考：\n\n    - 所以，我们要看看`AspectJAutoProxyRegistrar`这个到底给我们容器中注入了什么东西？\n        - `AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(registry);` 通过这行代码，我们看到他给我们注册了一个`AspectJAnnotationAutoProxyCreator`如果需要的话。\n        - 一直点进去这个方法，最后会看到给我们的容器中注入了一个bean：\n        - 注入的bean的名字叫做：`org.springframework.aop.config.internalAutoProxyCreator`\n        - 注入的bena的实际对象是：`org.springframework.aop.aspectj.annotation.AnnotationAwareAspectJAutoProxyCreator`\n\n简单的说就是：EnableAspectJAutoProxy 使用了 @Import，@Import导入了一个AspectJAutoProxyRegistrar ，这个Register继承自ImportBeanDefinitionRegister，并实现了registerBeanDifinitions方法，向容器中注册了一个：AnnotationAwareAspectJAutoProxyCreator\n\n总结：`@EnableAspectJAutoProxy`的作用就是为了给我们的容器中注入一个：`AnnotationAwareAspectJAutoProxyCreator`\n\n以下是示意图（图中也有步骤说明）\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/image-20221222170945180.png\" alt=\"image-20221222170945180\" style=\"zoom: 33%;\" />\n\n\n\n\n\n### AnnotationAwareAspectJAutoProxyCreator 是什么？干啥用的？\n\n- 我们看看这个类`AnnotationAwareAspectJAutoProxyCreator.java`的继承关系：\n    - class AnnotationAwareAspectJAutoProxyCreator extends AspectJAwareAdvisorAutoProxyCreator\n    - class AspectJAwareAdvisorAutoProxyCreator extends AbstractAdvisorAutoProxyCreator\n    - class AbstractAdvisorAutoProxyCreator extends AbstractAutoProxyCreator\n    - class AbstractAutoProxyCreator extends ProxyProcessorSupport implements SmartInstantiationAwareBeanPostProcessor, BeanFactoryAware\n    - 到这里就到底了，我们发现了两个重要的接口：\n        - `SmartInstantiationAwareBeanPostProcessor`：是一个后置处理器xxxxBeanPostProcessor，我们知道在spirng中，后置处理器是一个非常重要的概念，他会在bean的初始化前后做一些工作。所以，我们要看这个`SmartInstantiationAwareBeanPostProcessor`到底做了什么，实现了我们的aop的强大功能\n        - `BeanFactoryAware`：实现了这个接口的bean，可以直接访问 Spring 容器，该bean被容器创建以后，它会拥有一个指向 Spring 容器（也就是BeanFactory）的引用，可以利用该bean根据传入参数动态获取被spring工厂加载的其他的所有的bean。 eg：这部分是IOC的内容，我们不扯那么多\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/znInKzNU3RP5vLmcaq7ij3QsxgXDADDfgxV76ZhS-58.png\" alt=\"znInKzNU3RP5vLmcaq7ij3QsxgXDADDfgxV76ZhS-58\" style=\"zoom: 25%;\" />\n\n\n\n- AnnotationAwareAspectJAutoProxyCreator 作为 xxxBeanPostProcessor 做了什么工作\n\n- AnnotationAwareAspectJAutoProxyCreator 作为 BeanFactoryAware 做了什么工作\n\n- 在分析上面两个问题之前，我们先来看看AnnotationAwareAspectJAutoProxyCreator是什么时候被创建的。\n\n- 在之前，我们知道@EnableAspectJAutoProxy的给我们的容器中注入一个：AnnotationAwareAspectJAutoProxyCreator\n\n- 同时，我们知道AnnotationAwareAspectJAutoProxyCreator的作用主要是作为一个后置处理器，在bean的创建前后做一些工作，以及实现了BeanFactoryAware接口，可以直接与spring容器进行操作。\n\n- 那么，AnnotationAwareAspectJAutoProxyCreator是什么时候被创建的呢？\n\n\n\n### AnnotationAwareAspectJAutoProxyCreator 是什么时候被创建的\n\n- 从程序的入口开始看，这里的入口是指我们的测试类，也就是下面这段代码\n\n```java\npublic class AOPTest {\n    @Test\n    public void testAop() {\n        ApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfig.class);\n        Calculator calculator = (Calculator) applicationContext.getBean(\"calculator\");\n        int div = calculator.div(2, 1);\n        System.out.println(div);\n    }\n}\n\n```\n\n- new AnnotationConfigApplicationContext(MainConfig.class)： 传入配置类，创建Spring容器\n\n- 点击进去构造方法，在创建容器的时候，有一个非常重要的方法叫做：refresh();\n\n- refresh()方式是整个IOC容器创建的关键，对于他的解释，看下面的说明，关于refresh()方法，在本文中不是重点，可以略过...\n\n```java\n@Override\npublic void refresh() throws BeansException, IllegalStateException {\n   // 来个锁，不然 refresh() 还没结束，你又来个启动或销毁容器的操作，那不就乱套了嘛\n   synchronized (this.startupShutdownMonitor) {\n\n      // 准备工作，记录下容器的启动时间、标记“已启动”状态、处理配置文件中的占位符\n      prepareRefresh();\n\n      // 这步比较关键，这步完成后，配置文件就会解析成一个个 Bean 定义，注册到 BeanFactory 中，\n      // 当然，这里说的 Bean 还没有初始化，只是配置信息都提取出来了，\n      // 注册也只是将这些信息都保存到了注册中心(说到底核心是一个 beanName-> beanDefinition 的 map)\n      ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();\n\n      // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean\n      // 这块待会会展开说\n      prepareBeanFactory(beanFactory);\n\n      try {\n         // 【这里需要知道 BeanFactoryPostProcessor 这个知识点，Bean 如果实现了此接口，\n         // 那么在容器初始化以后，Spring 会负责调用里面的 postProcessBeanFactory 方法。】\n\n         // 这里是提供给子类的扩展点，到这里的时候，所有的 Bean 都加载、注册完成了，但是都还没有初始化\n         // 具体的子类可以在这步的时候添加一些特殊的 BeanFactoryPostProcessor 的实现类或做点什么事\n         postProcessBeanFactory(beanFactory);\n         // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 方法\n         invokeBeanFactoryPostProcessors(beanFactory);\n\n         // 注册 BeanPostProcessor 的实现类，注意看和 BeanFactoryPostProcessor 的区别\n         // 此接口两个方法: postProcessBeforeInitialization 和 postProcessAfterInitialization\n         // 两个方法分别在 Bean 初始化之前和初始化之后得到执行。注意，到这里 Bean 还没初始化\n         registerBeanPostProcessors(beanFactory);\n\n         // 初始化当前 ApplicationContext 的 MessageSource，国际化这里就不展开说了，不然没完没了了\n         initMessageSource();\n\n         // 初始化当前 ApplicationContext 的事件广播器，这里也不展开了\n         initApplicationEventMulticaster();\n\n         // 从方法名就可以知道，典型的模板方法(钩子方法)，\n         // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前）\n         onRefresh();\n\n         // 注册事件监听器，监听器需要实现 ApplicationListener 接口。这也不是我们的重点，过\n         registerListeners();\n\n         // 重点，重点，重点\n         // 初始化所有的 singleton beans\n         //（lazy-init 的除外）\n         finishBeanFactoryInitialization(beanFactory);\n\n         // 最后，广播事件，ApplicationContext 初始化完成\n         finishRefresh();\n      }\n\n      catch (BeansException ex) {\n         if (logger.isWarnEnabled()) {\n            logger.warn(\"Exception encountered during context initialization - \" +\n                  \"cancelling refresh attempt: \" + ex);\n         }\n\n         // Destroy already created singletons to avoid dangling resources.\n         // 销毁已经初始化的 singleton 的 Beans，以免有些 bean 会一直占用资源\n         destroyBeans();\n\n         // Reset 'active' flag.\n         cancelRefresh(ex);\n\n         // 把异常往外抛\n         throw ex;\n      }\n\n      finally {\n         // Reset common introspection caches in Spring's core, since we\n         // might not ever need metadata for singleton beans anymore...\n         resetCommonCaches();\n      }\n   }\n}\n```\n\n- 在refresh()方法中，调用了registerBeanPostProcessors(beanFactory);用来注册xxxBeanPostProcessor后置处理器。\n\n- 正如我们的标题：AnnotationAwareAspectJAutoProxyCreator类，他其实就是一个继承了SmartInstantiationAwareBeanPostProcessor的一个后置处理器。\n\n- 所以这个方法里面，其实就创建了我们的AnnotationAwareAspectJAutoProxyCreator类。\n\n  - 作者注：registerBeanPostProcessors(beanFactory)是用来注册xxxBeanPostProcessor的，但是我们的AnnotationAwareAspectJAutoProxyCreator不是以BeanPostProcessor结尾的，能创建它吗？\n\n  - 作者注：当然是能的，在这里，一开始没转过弯，我们要知道我们的AnnotationAwareAspectJAutoProxyCreator虽然不是BeanPostProcessor结尾的，但是他可是继承了xxxBeanPostProcessor的，所以他也是一个BeanPostProcessor。\n\n- 知道了registerBeanPostProcessors(beanFactory)是用来注册xxxBeanPostProcessor，所以我们知道我们的AnnotationAwareAspectJAutoProxyCreator类也是在这里创建的，那么我们进去看看。\n\n- 首先获取所有等待注册的xxxBeanPostProcessor的定义，注意这里只是定义！并不是真正的bean。：String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false);\n\n- 怎么理解这里所说的“定义”呢？\n\n  - 就相当于我出门买东西，要买牙刷，牙膏，洗衣液，衣架. 我列了一个清单。\n\n  - 这个清单，是所有要买的东西的定义，但是它不是真正的东西！\n\n- 下一步，对所有的xxxBeanPostProcessor进行归类，并且按类分别生成Bean,这里就是生成真正的Bean了。\n\n- 归类，共分为三类：继承了PriorityOrdered的为一类，继承了Ordered的为一类，剩下的为一类。\n\n- 然后对这三类，分别进行注册。\n  - 优先注册实现了PriorityOrdered接口的BeanPostProcessor； \n\n  - 再给容器中注册实现了Ordered接口的BeanPostProcessor；\n\n  - 最后注册没实现优先级接口的BeanPostProcessor；\n\n- 所谓的注册，实际上就是创建BeanPostProcessor的具体Bean实例，放在容器里。\n\n- 现在，我们知道了，所谓的注册，实际上就是创建BeanPostProcessor的具体Bean实例，并且我们知道了在哪里注册我们的BeanPostProcessor。下面我们具体看看怎么注册的。\n\n- 在看怎么注册的之前，明确一点：在spring启动的时候，会注册很多xxxBeanPostProcessor，我们现在先不需要关注其他的，我们关注的是\n\n- bean 的定义为：InternalAutoProxyCreator\n\n- 创建的bean实例为：AnnonationAwareAspectJAutoProxyCreator\n\n- 主要是关注这个，其他的我们先不看\n\n- 之前我们说了，在refresh()方法中，会注册BeanPostProcessor，而且是按照分类进行注册的。\n\n- 下面这个图，是上面这部分逻辑的图示（图中有说明）\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/image-20221222174926824.png\" alt=\"image-20221222174926824\" style=\"zoom: 67%;\" />\n\n\n\n- 我们的关注点：AnnontationAwareAspectJAutpProxyCreator是实现了了Ordered接口的，所以我们关注怎么注册实现了Ordered的接口的BeanPostProcessor\n- 主要是在源码：BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); 这一行，通过我们的bean定义和要获取的bean实例类型-->来获取我们的bean实例\n\n```java\n    // Next, register the BeanPostProcessors that implement Ordered.\n    List<BeanPostProcessor> orderedPostProcessors = new ArrayList<BeanPostProcessor>();\n    for (String ppName : orderedPostProcessorNames) {\n        BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class);\n        orderedPostProcessors.add(pp);\n        if (pp instanceof MergedBeanDefinitionPostProcessor) {\n            internalPostProcessors.add(pp);\n        }\n    }\n```\n\n- getBean() --调用了--> doGetBean()，doGetBean的主要逻辑如下\n\n```java\nprotected <T> T doGetBean(final String name, final Class<T> requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException {\n    // 尝试从缓存中获取我们的目标Bean对象\n    Object sharedInstance = getSingleton(beanName);\n    if (sharedInstance != null && args == null) {\n        //获取到了，直接拿到目标bean对象\n        bean = getObjectForBeanInstance(sharedInstance, name, beanName, null);\n    }else {\n        // 缓存中获取不到，那么就去生成\n        // 生成之前会做一些检查\n            // Create bean instance. 开始生成目标bean\n            if (mbd.isSingleton()) {\n                //目标类是单例\n                sharedInstance = getSingleton(beanName, new ObjectFactory<Object>() {\n                    @Override\n                    public Object getObject() throws BeansException {\n                        try {\n                            return createBean(beanName, mbd, args);\n                        }catch (BeansException ex) {\n                            //异常\n                        }\n                    }\n                });\n                bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd);\n            }else if (mbd.isPrototype()) {\n                //目标类是多例--省略了\n                bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd);\n            }else {\n                //其他\n            }\n        }\n    }\n    // 检查并返回--省略了\n    return (T) bean;\n}\n```\n\n- 我们是第一次运行，缓存中肯定没有，所以肯定会去生成createBean()，我们进去createBean()看一下，createBean里面 --调用了--> doCreateBean()\n\n- 创建bean实例，都是在 doCreateBean() 中完成的，doCreateBean()主要完成了下面几个工作\n  - 创建bean实例，但是没有任何属性： instanceWrapper = createBeanInstance(beanName, mbd, args);\n\n  - 对bean进行属性复制：populateBean(beanName, mbd, instanceWrapper); \n  - 初始化bean：exposedObject = initializeBean(beanName, exposedObject, mbd); 这个初始化，才是重点中的重点\n\n- 返回初始化之后的bean，就是真真正正的bean了，也就是我们苦思冥想的：AnnotationAwareAspectJAutoProxyCreator的实例。\n\n\n\n- **初始化bean：exposedObject = initializeBean(beanName, exposedObject, mbd); 这个初始化，才是重点中的重点**\n\n- 我们来单独看看这部分重点内容，初始化bean里面会在bean的初始化之前和之后分别执行BeanPostProcessor\n\n- 处理Aware接口的方法回调：invokeAwareMethods(beanName, bean);\n\n- 只有实现了Aware接口的bean才会调用\n\n- 这里主要是做了一步：就是把BeanFactory交给当前的bean，换句话说：就是当前bean里面保存了一个对beanFactory的一个引用。\n\n- 应用后置处理器的postProcessBeforeInitialization（）：wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName);\n\n- 执行自定义的初始化方法：invokeInitMethods(beanName, wrappedBean, mbd);\n\n- 这里所说的自定义的初始化方法，是我们自己配置的 init 方法，会在这里执行\n\n  - 什么是自己配置的init方法，就是下面这种，指定的 initMethod \n  -  > @Bean(initMethod=\"\",destoryMethod=\"\")\n  -  > <bean id=\"\", class=\"\", init-method=\"\", destory-method=\"\">\n\n- 执行后置处理器的postProcessAfterInitialization（）：wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName);\n- 返回wrappedBean，就是我们的目标结果了。\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/image-20221222175654720.png\" alt=\"image-20221222175654720\" style=\"zoom: 50%;\" />\n\n\n\n- 最后创建完之后，会将我们的BeanPostProcessor放在BeanFoctory中。beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(applicationContext))\n  - 作者注：BeanPostProcessor也是一个Bean，既然是Bean，就要满足Bean的生成步骤，每一个Bean的生成都会判断是否有对应的BeanPostProcessor需要执行！这也是为什么我们的 AnnotationAwareAspectJAutoProxyCreator明明是一个BeanPostProcessor，为什么还要执行applyBeanPostProcessorsBeforeInitialization和applyBeanPostProcessorsAfterInitialization\n\n\n- 总结：至此，我们本小节的标题：AnnotationAwareAspectJAutoProxyCreator 是什么时候被创建的呢？就已经完成了。我们总体回顾一下\n  - 首先是，我们知道@EnableAspectJAutoProxy的给我们的容器中注入一个：AnnotationAwareAspectJAutoProxyCreator\n  - 同时，我们知道AnnotationAwareAspectJAutoProxyCreator的作用主要是作为一个后置处理器，在bean的创建前后做一些工作，以及实现了BeanFactoryAware接口，可以直接与spring容器进行操作。\n  - 那么，我们现在又知道了AnnotationAwareAspectJAutoProxyCreator的是什么时候被创建的，怎么被创建的，以及创建完之后是加入到了BeanFacory中。\n- 下面，我们就要看看，这个 AnnotationAwareAspectJAutoProxyCreator 是怎么具体影响我们的业务的，是怎么把aop功能添加进来的。\n- 在看这个问题之前，我们要先看一下，我们具体的业务类是怎么创建的。包括：MainConfig, LogAspect, Calculator这三个类\n- MainConfig是一个配置类\n- LogAspect是一个切面类\n- Calculator是一个普通类\n\n\n\n### 具体的业务类(MainConfig, LogAspect, Calculator)是怎么创建的\n\n\n\n- 首先我们明确一点，在spirng中，所有bean的生成走的代码都是同一个，只不过根据接口的不同，走的逻辑不同\n- 对于这三个类bean的生成，因为三个类所代表的含义都是不同的，所以他们分别生成的逻辑是不同的。\n- 在区分这三个类的生成逻辑之前，我们总体看一下，bean的通用生成规则：\n- 对于一个Bean来说，不管这个Bean是BeanPostProcessor，还是config类，还是切面类，或者是普通类，在spirng中，都是通过getBean()作为统一入口\n- 比如对于前面说的BeanPostProcessor，他的创建入口在：\n  - 开始：refresh() \n  - 紧接着：registerBeanPostProcessors(beanFactory);\n  - 紧接着：registerBeanPostProcessors(beanFactory, this);\n  - 这一行调用了getBean()：BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class);\n\n- 比如我们的普通Bean 的创建\n  - 开始：refresh() \n  - 紧接着：finishBeanFactoryInitialization(beanFactory);\n  - 紧接着：beanFactory.preInstantiateSingletons();\n  - 最后调用了 getBean()：getBean(beanName); \n\n- 所以，我们从getBean开始，看一下spring是怎么创建Bean的，以及怎么兼容所有的bean类型的（BeanPostProcessor，Config，切面类等）\n- 下面的代码是getBean的主要逻辑流程，我是把所有的逻辑汇总在了一起，实际的代码中，是涉及到多个类的多个方法，比较复杂。我们只看主逻辑。\n\n```java\n\n//遍历所有的bean定义\nfor(String beanName : 所有的BeanName){\n    getBean(){\n        doGetBean(){\n            // 尝试从缓存中获取我们的目标Bean对象\n            Object sharedInstance = getSingleton(beanName);\n            if (sharedInstance != null && args == null) {\n                //获取到了，直接拿到目标bean对象\n                bean = getObjectForBeanInstance(sharedInstance, name, beanName, null);\n            }else {\n                // 缓存中获取不到，那么就去生成\n                // 生成之前会做一些检查\n                // Create bean instance. 开始生成目标bean\n                if (mbd.isSingleton()) {\n                    //生成单例bean\n                    createBean(){\n\n                        // Give BeanPostProcessors a chance to return a proxy instead of the target bean instance.\n                        // 给 BeanPostProcessors 一个机会：返回代理类替代目标类(这里的代理类并不是说从缓存中取出代理类，而是用另一种方式生成代理类)\n                        resolveBeforeInstantiation(){\n                            1.applyBeanPostProcessorsBeforeInstantiation(){\n                                for (BeanPostProcessor bp : getBeanPostProcessors()) {\n                                    if (bp instanceof InstantiationAwareBeanPostProcessor) {\n                                        postProcessBeforeInstantiation(){\n                                            //我们可以看到，这里也有创建代理的逻辑，以至于很多人会搞错。\n                                            //确实，这里是有可能创建代理的，但前提是对于相应的 bean 我们有自定义的 TargetSource 实现，\n                                            //进到 getCustomTargetSource(...) 方法就清楚了，我们需要配置一个 customTargetSourceCreators，它是一个 TargetSourceCreator 数组。\n                                            //这里就不再展开说 TargetSource 了\n                                        }\n                                    }\n                                }\n                            }\n\n                            2.如果before返回的bean是个null，after不会执行\n                            3.applyBeanPostProcessorsAfterInitialization(){\n                                for (BeanPostProcessor bp : getBeanPostProcessors()) {\n                                    postProcessAfterInitialization    \n                                }\n                            }\n                        }\n                        //开始创建Bean\n                        doCreateBean(){\n                            doCreateBean(){\n                                //生成bean对象\n                                createBeanInstance(beanName, mbd, args);\n                                //给bean对象赋属性值\n                                populateBean(beanName, mbd, instanceWrapper);\n                                //初始化bean对象\n                                initializeBean(){\n                                    //执行后置处理器\n                                    1.applyBeanPostProcessorsBeforeInitialization(){\n                                        postProcessBeforeInitialization\n                                    }\n                                    2.invokeInitMethods\n                                    3.applyBeanPostProcessorsAfterInitialization(){\n                                        postProcessAfterInitialization(){\n                                            wrapIfNecessary(){\n                                                //Create proxy if we have advice.\n                                                //如果有切面的话，就创建代理\n                                                //Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null);\n                                                createProxy(bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean));\n                                            }\n                                        }\n                                    }\n                                }\n                            }\n                        }\n\n                    }\n                    bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd);\n                }else if (mbd.isPrototype()) {\n                    //生成多例bean--省略了\n                    bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd);\n                }else {\n                    //其他--省略\n                }\n            }\n        }\n        // 检查并返回--省略了\n        return (T) bean;\n    }\n}\n\n```\n\n- 要想把上面那部分代码看懂，至少需要debug调试十几遍，反正我是不知道调试了几十遍，才看懂的。现在对上面的代码几个注意点说下：\n\n- applyBeanPostProcessorsBeforeInstantiation 和 applyBeanPostProcessorsBeforeInitialization 这是两个不一样的方法,一定不要看错，否则会很迷惑\n  - 一个结尾是：Instantiation（实例化）\n  - 一个结尾是：Initialization（初始）\n\n- 下面我们就来看一下，我们关注的三个类的具体创建步骤，结合上面的代码流程，通过三个具体类的创建，来温故知新一下。\n\n\n\n\n\n#### MainConfig\n\n- 我们从refresh()-->finishBeanFactoryInitialization()-->preInstantiateSingletons()-->这些就不说了，我们从遍历Bean定义开始\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/hNUBsLKhDYiqy_0t9sgVmMs6XhnbS9UO2ETVGIxlHBE.png\" alt=\"hNUBsLKhDYiqy_0t9sgVmMs6XhnbS9UO2ETVGIxlHBE\" style=\"zoom: 50%;\" />\n\n\n\n#### LogAspect\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/n64jjFWSA8kCbc9AgoYEMtaVPtku_Nu8LSTDjyp8QsU.png\" alt=\"n64jjFWSA8kCbc9AgoYEMtaVPtku_Nu8LSTDjyp8QsU\" style=\"zoom: 80%;\" />\n\n\n\n#### Calculator\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/WkX2HY8q-7QE94zE6_AGpMWC9AxxIOOW0M6DZ3mCDCA.png\" alt=\"WkX2HY8q-7QE94zE6_AGpMWC9AxxIOOW0M6DZ3mCDCA\" style=\"zoom:80%;\" />\n\n\n\n- 总结：通过以上的分析，我们收获了什么呢？\n\n- 我们知道了BeanPostProcessor有两种，一种是在创建Bean之前给一个机会返回代理，一种是在创建bean之后进行一些操作。\n\n- 前者是继承了SmartInstantiationAwareBeanPostProcessor的才有的功能。后者是所有的BeanPostProcessor都有的功能（包括SmartInstantiationAwareBeanPostProcessor）。\n\n- 同时，我们知道了三个类的具体创建流程：mainConfig，LogAspect，Calculator的创建流程。\n\n- 最后，我们仍然有一个疑问：\n\n- @EnableAspectJAutoProxy给我们的容器中注入一个：AnnotationAwareAspectJAutoProxyCreator\n\n- AnnotationAwareAspectJAutoProxyCreator是一个SmartInstantiationAwareBeanPostProcessor，我们目前只知道他是在bean创建之前给一个返回代理的机会。\n\n- 但是同时我们知道，我们生成的所有这些类（仅限于当前这个demo里的所有类），貌似都没有把握这个机会，在bean创建之前生成了代理。\n\n- 那么，这么AnnotationAwareAspectJAutoProxyCreator到底是有什么作用呢？？我还不知道！！！\n\n- 除了上面那个问题，我们不知道之外，接下来，再来具体看看我们的代理对象，就是Calculator代理对象具体是怎么生成！\n\n\n\n### Calculator代理对象具体是怎么生成的？\n\n\n\n- 话接上回，我们知道了spinrg的aop会对需要增强的bean的创建代理对象。在这里，被切的Calculator类就是一个增强的类，所以spirng会对他创建代理。\n\n- 同样的，我们知道，spring在对切面增强类创建代理，是在wrapIfNessary()这个方法里面创建代理的。那么我们就来看看是什么创建的，切面方法是怎么注入进来的。\n\n- 首先是获取当前bean可用的所有通知方法，Object[] specificInterceptors\n\n- 找到候选的所有的增强器（找哪些通知方法是需要切入当前bean方法的）\n\n- 获取到能在bean使用的增强器。\n\n- 给增强器排序\n\n- 这个获取当前类的所有通知方法的代码，就不看了，我们只需要知道运行完这个方法之后，就会拿到当前类的所有通知方法就可以啦。\n\n- 然后，当我们获取到当前类的所有通知方法之后，保存当前bean在advisedBeans中，并设置为true，表示它是一个增强bean\n\n- 紧接着就是创建代理：Object proxy = createProxy(bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean));\n\n- 将代理保存到proxyFactory，然后创建代理对象：代理有两种，有Spring自动决定创建哪一个代理\n\n- JdkDynamicAopProxy(config);jdk动态代理；\n\n- ObjenesisCglibAopProxy(config);cglib的动态代理；\n\n- 代理创建成功后，给容器中返回当前组件使用cglib增强了的代理对象；\n\n- 以后容器中获取到的就是这个组件的代理对象，执行目标方法的时候，代理对象就会执行通知方法。\n\n- 这部分我们不深究究竟是怎么获取通知方法的，已经具体是怎么选择创建什么代理的，以及代理最终是怎么创建的。所以这里就不贴图了。\n\n- 总结：\n\n- 我们知道spring的aop会对需要增强的bean的创建代理对象\n\n- 需要增强的bean会被保存在advisedBeans中，创建后的代理对象也会保存在proxyFactory中，最终创建的proxy代理对象会返回，并保存在IOC容器中，供以后使用。\n\n- 那么，接下来，我们就看看，当我们调用目标方法的时候，代理对象是怎么具体执行的？\n\n\n\n### 调用目标方法，代理对象是怎么执行的？\n\n- 通过以上所有的步骤，我们现在终于走到了最后一步，在开始执行目标方法之前，我们先来简单的整体回顾一下。\n\n- @EnableAspectJAutoProxy 开启AOP功能，并给容器中注册一个组件 AnnotationAwareAspectJAutoProxyCreator\n\n- AnnotationAwareAspectJAutoProxyCreator这个组件通过refresh()中的registerBeanPostProcessors(beanFactory);这个方法被注册进来\n\n- 然后开始生成所有的bean（包括我们的mainConfig，LogAspect, Calculator）等的创建，在这类的创建步骤中，之前注册的组件：AnnotationAwareAspectJAutoProxyCreator会产生作用\n\n- 什么作用呢？就是在bean的创建之前执行BeanPostProcessor，在before中给一个返回代理对象的机会。\n\n- 如果没有返回代理对象，那么就创建bean，创建bean之后，会再次执行BeanPostProcessor，在after中会判断是否是增强bean，会是需要创建代理\n\n- 如果不需要创建代理，那么就直接返回bean（比如MainConfig，LogAspect这两个就不需要创建代理）\n\n- 如果需要创建代理，那么就获取所有的通知方法，然后spirng决定创建cglib代理还是jdk代理，并返回代理对象。\n\n- 最后，就到了我们这一小节的主题：代理对象是怎么替代目标方法执行的？\n\n- 代理对象创建成功之后，执行目标方法，其实就是通过代理对象来执行目标方法了。\n\n- 执行目标的方法的入口是在我们的测试类中\n\n```java\npublic class AOPTest {\n    @Test\n    public void testAop() {\n        ApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfig.class);\n        Calculator calculator = (Calculator) applicationContext.getBean(\"calculator\");\n        //这里是是入口\n        int div = calculator.div(2, 1);\n        System.out.println(div);\n    }\n}\n\n```\n\n- 意思就是当我们执行int div = calculator.div(2, 1);，实际上就是我们的代理对象执行的。所以他会进入到代理对象的执行流程里。\n\n- 下一步，就是进入到代理对象的执行，执行目标方法进入了CglibAopProxy.intercept()方法中\n\n- intercept()方法的作用就是在目标方法执行前后进行拦截，这也是我们aop代理对象的核心，就是通过拦截器执行切面。\n\n- 在intercept()方法中主要做了两件事：\n\n- getInterceptorsAndDynamicInterceptionAdvice() 获取所有的拦截器链\n\n- proceed() 执行拦截器链\n\n- 当拦截器链执行完之后，所有的切面也就执行完了。同时会进行返回 return retVal;这个retVal就是我们目标方法的返回值。\n\n- 这就是调用目标方法，代理对象的大致执行流程。\n\n```java\n@Override\npublic Object intercept(Object proxy, Method method, Object[] args, MethodProxy methodProxy) throws Throwable {\n    //一些变量的定义\n    try {\n        //获取拦截器链\n        List<Object> chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass);\n\n        // 对拦截器链进行检查\n        if (chain.isEmpty() && Modifier.isPublic(method.getModifiers())) {\n            //如果拦截器链为空，就直接执行目标方法\n            Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args);\n            retVal = methodProxy.invoke(target, argsToUse);\n        }else {\n            // 否则就执行拦截器链\n            retVal = new CglibMethodInvocation(proxy, target, method, args, targetClass, chain, methodProxy).proceed();\n        }\n        //最后得到返回值，进行返回\n        retVal = processReturnType(proxy, target, method, retVal);\n        return retVal;\n    }finally {\n        //一些处理\n    }\n}\n\n```\n\n- 总结：我们知道了代理对象执行的大致流程，无非是两个关键的步骤：\n\n- 拦截器链的获取\n\n- 拦截器链的执行\n\n### 目标方法执行之拦截器链的获取\n\n\n\n- 通过上面我们知道，目标方法的执行，其实就是代理对象的执行。代理对象在之前之前， 会获取到所有的拦截器（这里的拦截器，实际上就是我们之前说的通知方法，也叫切面方法）\n\n- 那么，现在我们来看一看，拦截器链是怎么获取的。\n\n- 首先，进入拦截器链的获取方法中： List chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass);\n\n- 会先从缓存中拿一下，缓存中没有的话，再去获取拦截器链。会将获取的结果放在缓存中，以便于下次可以直接使用\n\n- 获取拦截器链：this.advisorChainFactory.getInterceptorsAndDynamicInterceptionAdvice(this, method, targetClass)\n\n- 获取拦截器链的步骤比较简单，我们直接通过简化后的代码进行查看（省略了很多代码，建议跟着源码看）\n\n```java\npublic List<Object> getInterceptorsAndDynamicInterceptionAdvice() {\n    // 定义一个List，从来存放所有的拦截器链：看看人家List的定义，会传入list的大小，不浪费一点内存空间，真好！\n    List<Object> interceptorList = new ArrayList<Object>(config.getAdvisors().length);\n    //遍历所有的Advisors，Advisors里面都是我们的通知方式，通过断点我们看到。包含一个系统默认的通知方法和我们定义的所有拦截方法\n    for (Advisor advisor : config.getAdvisors()) {\n        //会根据不同的类型，分别走不同的逻辑，但是都会调用同一个方法，就是getInterceptors()\n        if (advisor instanceof PointcutAdvisor) {\n            //从advisor中获取MInterceptor\n            MethodInterceptor[] interceptors = registry.getInterceptors(advisor);\n        }else if (advisor instanceof IntroductionAdvisor) {\n            Interceptor[] interceptors = registry.getInterceptors(advisor);\n        }else {\n            Interceptor[] interceptors = registry.getInterceptors(advisor);\n        }\n    }\n    //最后返回所有的拦截器\n    return interceptorList;\n}\n```\n\n- 明白了拦截器链的获取流程之后，我们在进入看看怎么从从advisor中获取MInterceptor，这部分代码更简单，直接贴上源码\n\n```java\n@Override\npublic MethodInterceptor[] getInterceptors(Advisor advisor) throws UnknownAdviceTypeException {\n    //创建list用于保存\n    List<MethodInterceptor> interceptors = new ArrayList<MethodInterceptor>(3);\n    //获取具体的通知方法，advice其实就是我们具体的通知方法。 \n    //advice和advisor的作用是：advisor是一个大集合，里面包含了很多很多东西，advice就是advisor包含的内容之一，就是具体的通知方法\n    Advice advice = advisor.getAdvice();\n    //然后对通知方法进行判断，我们知道通知方法有很多种：前置通知，后置通知等等\n    //如果当前通知方法是MethodInterceptor类型的，就直接放进去\n    if (advice instanceof MethodInterceptor) {\n        interceptors.add((MethodInterceptor) advice);\n    }\n    //如果不是MethodInterceptor类型的，就会通过一个适配器，将通知方法转换成MethodInterceptor类型的。 \n    //可以进去看看这个适配器，其实就是装饰模式，进行了一次包装，包装成MethodInterceotor\n    for (AdvisorAdapter adapter : this.adapters) {\n        if (adapter.supportsAdvice(advice)) {\n            interceptors.add(adapter.getInterceptor(advisor));\n        }\n    }\n    //最后进行一下校验，然后返回\n    if (interceptors.isEmpty()) {\n        throw new UnknownAdviceTypeException(advisor.getAdvice());\n    }\n    return interceptors.toArray(new MethodInterceptor[interceptors.size()]);\n}\n```\n\n\n\n- 执行完以上的步骤，我们就获取到了当前目标类的所有的拦截器。下一步就是执行拦截器了。\n\n\n\n### 目标方法执行之拦截器链的执行\n\n- 到这一步，才是我们切面的真真正正的执行，前面做的都是准备。什么是真真正正的执行呢？就是我们可以在控制台，看到输出。\n\n- 明确一个概念：所谓的spring的aop，就是一个代理类，这个代理类内有很多拦截器，在真正的方法执行前后，会执行这些拦截器，这就是aop的本质。\n\n- 好了，下面我们看看拦截器链的执行吧。这是重点！！\n\n- retVal = new CglibMethodInvocation(proxy, target, method, args, targetClass, chain, methodProxy).proceed();\n\n- 其中proceed()方法是重点，他是一个递归调用的方法。\n\n- 在方法的一开始，保存了一个变量，这个变量从-1开始，每一次process()的执行，都会++，直到所有的拦截器都执行完了，才会开始返回。\n\n- 我们直接看源码，这个方法并不是很长。\n\n```java\n@Override\npublic Object proceed() throws Throwable {\n    //    这个变量从-1开始执行，直到所有的拦截器全都执行完\n    if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) {\n        return invokeJoinpoint();\n    }\n    //变量每次执行++\n    Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex);\n    if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) {\n        // Evaluate dynamic method matcher here: static part will already have\n        // been evaluated and found to match.\n        InterceptorAndDynamicMethodMatcher dm =\n                (InterceptorAndDynamicMethodMatcher) interceptorOrInterceptionAdvice;\n        if (dm.methodMatcher.matches(this.method, this.targetClass, this.arguments)) {\n            return dm.interceptor.invoke(this);\n        }\n        else {\n            // Dynamic matching failed.\n            // Skip this interceptor and invoke the next in the chain.\n            return proceed();\n        }\n    }\n    else {\n        // It's an interceptor, so we just invoke it: The pointcut will have\n        // been evaluated statically before this object was constructed.\n        //这里是拦截器链的递归调用，注意传入的是this，也就是当前的MethodInvocation，因为invoke()方法中会用到\n        return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this);\n    }\n}\n\n\n\n@Override\npublic Object invoke(MethodInvocation mi) throws Throwable {\n    //拦截器链，在invoke的时候，会进入到这个方法\n    //注意注意注意，每一次invoke的时候，其实进入的是不一样的方法。 注意自己打断点看一下，所以，我这里把其他的都删掉了。\n    //只留了一个mi.proceed()方法，因为每次进入的都是不同的类的invoke()方法，但是都会调用proceed()\n    //也可以看看，我提供的方法调用栈信息图\n    return mi.proceed();\n}\n```\n\n\n\n- 我之前说，proceed()是递归调用，其实是不对的！他其实不能算作递归调用。\n\n- 因为他是一个MethodInvocation，内部包含了其他的MethodInvocation，内部的MethodInvocation又包含了其他的MethodInvocation。\n\n- MethodInvocation调用proceed()，并不断压栈，直到所有的MethodInvocation调用完了。\n\n- 然后从最最内部的MethodInvocation开始，一个一个返回。直到返回到最上层的MethonInvocation。\n\n- proceed调用栈图\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/image-20221222183003694.png\" alt=\"image-20221222183003694\" style=\"zoom:80%;\" />\n\n\n\n- 一些注意点：\n\n- 注意点1：around拦截器的执行时机，要了解，是在压栈之后，立即执行，然后我们知道around里面，调用了proceed，然后会再次将around进行压栈。这个一会再说！\n\n- 注意点2：压栈的顺序，这个一会和注意点1一起说。\n\n- 注意点3：每一个拦截器，分别都执行了什么，比如前置通知压栈后，直接开始调用，然后前置通知调用玩，直接调用目标方法。比如后置通知里有一个finally，表示不管是否发生异常，后置通知都执行。比如返回通知，会直接throw异常，throw异常之后，返回通知就不在执行，交给异常通知了。等等之类的。\n\n- 我们来总体看一下调用流程。并解决注意点1和2\n\n\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/4OTL2oICo5fo_CqT86SyAdIp1Txk-x__mt7r3e0KDto.png\" alt=\"4OTL2oICo5fo_CqT86SyAdIp1Txk-x__mt7r3e0KDto\" style=\"zoom:80%;\" />\n\n\n\n### 总结\n\n *     1）、@EnableAspectJAutoProxy 开启AOP功能\n *     2）、@EnableAspectJAutoProxy 会给容器中注册一个组件 AnnotationAwareAspectJAutoProxyCreator\n *     3）、AnnotationAwareAspectJAutoProxyCreator是一个后置处理器；\n *     4）、容器的创建流程：\n       *     1）、registerBeanPostProcessors（）注册后置处理器；创建AnnotationAwareAspectJAutoProxyCreator对象\n       *     2）、finishBeanFactoryInitialization（）初始化剩下的单实例bean\n             *     1）、创建业务逻辑组件和切面组件\n             *     2）、AnnotationAwareAspectJAutoProxyCreator拦截组件的创建过程\n             *     3）、组件创建完之后，判断组件是否需要增强\n                   *     是：切面的通知方法，包装成增强器（Advisor）;给业务逻辑组件创建一个代理对象（cglib）；\n *     5）、执行目标方法：\n       *     1）、代理对象执行目标方法\n       *     2）、CglibAopProxy.intercept()；\n             *     1）、得到目标方法的拦截器链（增强器包装成拦截器MethodInterceptor）\n             *     2）、利用拦截器的链式机制，依次进入每一个拦截器进行执行；\n             *     3）、效果：\n                   *     正常执行：环绕通知开始-》前置通知-》目标方法-》环绕通知结束-》后置通知-》返回通知-》结束\n                   *     出现异常：环绕通知开始-》前置通知-》目标方法-》环绕通知结束-》后置通知-》异常通知-》结束\n\n\n\n## 记录一次AOP不生效的排查心路\n\n结论先行：\n\n- AOP生效的条件就是，当调用`目标类`的`目标方法`的时候，实际上是由`目标类的代理对象`调用`目标方法`的，切面会生效\n- 在一个类内部方法调用的时候，切面是不生效的。\n\n\n\n### 业务场景\n\n要给客户展示出各个维度的指标数据，计算的方法都是一样的，只是源数据的结构不一样，所以\n\n- 计算的方法统一抽象到抽象父类中\n- 不同的指标针对不同的源数据，处理成统一结构，然后调用父类中的计算方法统一返回\n- 很显然，这是策略模式\n\n假设我们有指标A，指标B要展示给客户；\n\n因为在测试环境数据不好造，所以我想到了使用AOP进行MOCK\n\n具体的代码见下面\n\n\n\n### 代码结构\n\n#### 结构图\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/image-20221223144223033.png\" alt=\"image-20221223144223033\" style=\"zoom:80%;\" />\n\n#### 业务代码\n\n##### 抽象父类：AbstractIndicatorStrategy\n\n```java\n\npublic abstract class AbstractIndicatorStrategy {\n\n    @PostConstruct\n    protected abstract void init();\n\n\n    public String process(){\n        String data = getData();\n        return calculate(data);\n    }\n\n    public String calculate(String data){\n        return data+\" = 666\";\n    }\n\n    public abstract String getData();\n\n}\n\n```\n\n\n\n##### 策略类A：IndicatorAStrategyImpl\n\n```java\n@Component\npublic class IndicatorAStrategyImpl extends AbstractIndicatorStrategy {\n    @Override\n    protected void init() {\n        IndicatorStrategyManager.register(\"INDICATOR_A\",this);\n    }\n\n    @Override\n    public String getData() {\n        return \"indicator_a_data\";\n    }\n\n}\n```\n\n\n\n##### 策略类B：IndicatorBStrategyImpl\n\n```java\n\n@Component\npublic class IndicatorBStrategyImpl extends AbstractIndicatorStrategy {\n\n    @Override\n    protected void init() {\n        IndicatorStrategyManager.register(\"INDICATOR_B\",this);\n    }\n\n    @Override\n    public String getData() {\n        return \"indicator_b_data\";\n    }\n\n}\n```\n\n\n\n##### 策略管理类：IndicatorStrategyManager\n\n```java\n\npublic class IndicatorStrategyManager {\n\n    private static Map<String, AbstractIndicatorStrategy> maps = new HashMap<>();\n\n\n    public static void register(String strategyCode, AbstractIndicatorStrategy metricStrategy) {\n        maps.put(strategyCode, metricStrategy);\n    }\n\n    public static AbstractIndicatorStrategy getStrategy(String strategyCode) {\n        return maps.get(strategyCode);\n    }\n\n}\n\n```\n\n\n\n##### 客户端\n\n```java\n//测试策略A\nString indicatorAResult = IndicatorStrategyManager.getStrategy(\"INDICATOR_A\").process();\n//测试策略B\nString indicatorBResult = IndicatorStrategyManager.getStrategy(\"INDICATOR_B\").process();\n```\n\n\n\n以上是业务部分的相关代码，那么AOP是在哪里使用的呢，看下面\n\n\n\n#### AOP代码\n\n上面提到到，我希望通过切面的方式，mock掉`calculate`这个方法\n\n所以我创建了一个切面类；添加了`@Aspect`注解，并且在springboot的启动类上添加了`@EnableAspectJAutoProxy`注解\n\n```java\n\n@Slf4j\n@Aspect\n@Component\npublic class MockAspect {\n\n    @Value(value = \"${remote.mock.indicator:CLOSE}\")\n    private String mockOpen;\n\n    /**\n     * mock calculate\n     */\n    @Pointcut(\"execution(* com.sf.fw.nas.manager.strategy.base.AbstractIndicatorStrategy.calculate(..))\")\n    public void calculate() {}\n\n    /**\n     * 计算指标\n     */\n    @Around(\"calculate()\")\n    public Object aroundCalculate(ProceedingJoinPoint joinPoint) throws Throwable {\n        if (\"OPEN\".equals(mockOpen)) {\n            Object[] args = joinPoint.getArgs();\n            String data = (String) args[0];\n            return data + \" = mock_data\";\n        }\n        return joinPoint.proceed();\n    }\n\n}\n\n```\n\n\n\n```java\n\n@EnableFeignClients(\"com.xxx.remote\")\n@EnableTransactionManagement\n@MapperScan(\"com.xxx.mapper\")\n@SpringBootApplication(exclude = {DruidDataSourceAutoConfigure.class, DataSourceAutoConfiguration.class})\n@EnableAspectJAutoProxy \npublic class AppApplication {\n\tpublic static void main(String[] args) {\n\t\tSpringApplication app = new SpringApplication(AppApplication.class);\n\t\tapp.setBannerMode(Banner.Mode.OFF);\n\t\tapp.run(args);\n\t}\n}\n```\n\n\n\n\n\n### 不生效的场景\n\n最后我们运行代码，发现，我们的切面，压根没有进来\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/image-20221223143934745.png\" alt=\"image-20221223143934745\" style=\"zoom:80%;\" />\n\n\n\n\n\n### 排查思路\n\n#### 第一次排查：解决了切面没有切对方法的问题（其实不是）\n\n在这一次的排查过程中，我以为是因为我的切面切得是抽象父类的方法，但是我实际调用的是策略子类，方法可能切不到。\n\n所以我修改了一下切面的 execution 表达式；\n\n由原来的：\n\n```java\n@Pointcut(\"execution(* com.sf.fw.nas.manager.strategy.base.AbstractIndicatorStrategy.calculate(..))\")\n```\n\n改成了：\n\n```java\n@Pointcut(\"execution(* com.sf.fw.nas.manager.strategy.*.*.calculate(..))\")\n```\n\n含义是：切在`com.sf.fw.nas.manager.strategy`包和所有子包里的任意类的`calculate`方法的执行\n\n如果看不懂这个表达式的，可以看看：[execution表达式](#execution表达式)\n\n\n\n结论：\n\n切面依旧没有生效。所以说：不是这个问题导致的。\n\n题外话：其实不是 这个问题，最终经过所有的排查思路之后，找到问题并解决之后，发现，即使切的是父类的方法，还是能进去切面的。 \n\n\n\n\n\n#### 第二次排查：解决了调用类不是代理类的问题（关键）\n\n然后我又复习了一遍，AOP到底是什么原理。 [AOP源码流程大致分析](#AOP源码流程大致分析)\n\n了解到：\n\nAOP的实现原理，其实就是动态代理，spring会对切面切到的目标类，生成代理类。\n\n然后执行目标类的目标方法的时候，其实是由代理类来执行的。\n\n这就是AOP的原理。\n\n\n\n了解了上面的步骤之后，我们来验证一下，看看调用目标方法的类是不是代理类呢？\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/image-20221223150216763.png\" alt=\"image-20221223150216763\" style=\"zoom:80%;\" />\n\n\n\n果不其然，调用目标方法的类，竟然不是代理类；这样AOP肯定是不会生效的。\n\n那么问题来了？\n\n为什么不是代理类呢？\n\n- 是我的切面配置有错误吗？`@Aspect`和@`EnableAspectJAutoProxy` 这两注解没生效吗？\n- 还是其他的配置有问题呢？\n- 还是别的什么问题呢？\n\n我现在有点迷。\n\n\n\n于是我又复习了一下AOP的原理，我要看一下，这个代理类是怎么生成的？为什么我的类不是代理类。[Calculator代理对象具体是怎么生成的？](#Calculator代理对象具体是怎么生成的？)\n\n经过复习，了解到，在spring中，java类分为几种：\n\n- 有BeanPostProcessor，是一个很重要的概念，用来实现spring的很多强大的功能\n- 有普通的类：这里所说的普通的类包括：切面类，Config类，业务类，其实都算是普通类\n\n那么这些类是怎么生成的？\n\n- 在spring的refresh方法中，有两个方法叫做：\n  - registerBeanPostProcessors：这个是用来注册 BeanPostProcessor 的\n  - finishBeanFactoryInitialization：这个就是用来创建生成普通类的。\n- 所以我们的代理对象，正常应该在 finishBeanFactoryInitialization 这个方法中被生成。\n\n然后我们继续了解 finishBeanFactoryInitialization 这个方法\n\n- preInstantiateSingletons：开始实例化单例bean，调用 getBean\n- getBean：获取bean，调用 doGetBean\n- doGetBean：获取bean，会先从缓存拿，拿不到就调用：createBean\n- createBean：创建bean，调用 doCreateBean\n- doCreateBean：开始真正的创建bean，会调用：createBeanInstance，populateBean，initializeBean\n- createBeanInstance 创建bean，populateBean 给bean赋值，initializeBean 初始化bean， 会调用 applyBeanPostProcessorsAfterInitialization\n- applyBeanPostProcessorsAfterInitialization：应用BeanPostProcessor增强bean\n- postProcessAfterInitialization：会找到很多BeanPostProcessor，循环调用BeanPostProcessor的这个方法进行增强\n- AbstractAutoProxyCreator：AbstractAutoProxyCreator是AOP用来增强类的，进入到这个类的postProcessAfterInitialization方法中，会调用：wrapIfNecessary\n- wrapIfNecessary：开始对类进行代理\n- createProxy：真正的创建代理类\n\n好了，了解了以上的流程之后，我们进入到spring的源码中，打上断点，验证：我们的类到底有没有生成代理类？\n\n\n\n打上断点，开始验证：\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/image-20221223152614060.png\" alt=\"image-20221223152614060\" style=\"zoom:80%;\" />\n\n\n\n然后发现，断点进来了，而且生成了代理类，说明配置是没有问题的。\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/image-20221223152838510.png\" alt=\"image-20221223152838510\" style=\"zoom:80%;\" />\n\n\n\n既然配置没有问题，那就说明我们的代码有问题。\n\n代理类生成了，但是我们没有获取到代理类，然后我们思考，获取策略类的时候，是从`IndicatorStrategyManager`中获取的。\n\n也就是说，从`IndicatorStrategyManager`中获取的类不是代理类。那么为什么呢？\n\n这个时候去看我们的代码，发现我们在往`IndicatorStrategyManager`注册bean的时候，用的是下面这种方式：\n\n```java\n    @Override\n    protected void init() {\n        IndicatorStrategyManager.register(\"INDICATOR_A\",this);\n    }\n```\n\n我们往策略管理类中注册的是`this`\n\n- this 是什么，this是当前这个bean，不是从spring容器中拿到的bean\n- 所以，它当然不是 代理对象啦。\n\n既然找到了问题，就好解决了。我们把 this 改成 从spring容器中获取bean\n\n```java\n@Component\npublic class IndicatorAStrategyImpl extends AbstractIndicatorStrategy {\n    @Override\n    protected void init() {\n        IndicatorStrategyManager.register(\"INDICATOR_A\",\n            ApplicationContextProvider.getBean(IndicatorAStrategyImpl.class));\n    }\n\n    @Override\n    public String getData() {\n        return \"indicator_a_data\";\n    }\n\n}\n```\n\n其中使用到的`ApplicationContextProvider`这个类是自己写的一个工具类。代码如下\n\n```java\n\n@Component\npublic class ApplicationContextProvider implements ApplicationContextAware, BeanPostProcessor {\n    /**\n     * 上下文对象实例\n     */\n    private static ApplicationContext applicationContext;\n\n    @Override\n    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {\n        ApplicationContextProvider.applicationContext = applicationContext;\n    }\n\n    /**\n     * 获取applicationContext\n     */\n    public static ApplicationContext getApplicationContext() {\n        return applicationContext;\n    }\n\n    /**\n     * 通过class获取Bean.\n     */\n    public static <T> T getBean(Class<T> clazz) {\n        try {\n            return getApplicationContext().getBean(clazz);\n        } catch (NoSuchBeanDefinitionException e) {\n            return null;\n        }\n    }\n\n    /**\n     * 通过name,以及Clazz返回指定的Bean\n     */\n    public static <T> T getBean(String name, Class<T> clazz) {\n        try {\n            return getApplicationContext().getBean(name, clazz);\n        } catch (NoSuchBeanDefinitionException e) {\n            return null;\n        }\n    }\n}\n```\n\n\n\n好了，至此，我们在测试一波：\n\n喜大普奔，现在我们获取到的对象就是我们的代理类啦。\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/image-20221223154857074.png\" alt=\"image-20221223154857074\" style=\"zoom:80%;\" />\n\n但是很不幸的是，切面仍然没有生效。\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/image-20221223155053663.png\" alt=\"image-20221223155053663\" style=\"zoom:80%;\" />\n\n\n\n我又迷惑了，为什么呢？ 接着往下看\n\n\n\n#### 第三次排查：找到了cglib内部调用的问题（未解决）\n\n到这里，我已经有点晕了，为什么我的AOP还是不生效呢？\n\n再次回想：AOP生效的条件就是，当调用`目标类`的`目标方法`的时候，实际上是由`目标类的代理对象`调用`目标方法`的。\n\n这句话里面有几个关键词：\n\n- 目标类：当然了，就是我们的策略类A（IndicatorAStrategyImpl）和B（IndicatorBStrategyImpl）这两个东东啦。\n- 目标方法：当然啦，目标方法是我们希望被切面切到的`calculate`方法啦\n- 目标类的代理对象：当然啦，是我们之前看到的 `IndicatorAStrategyImpl$$EnhancerBySpringCGLIB$$c1fb478` 这个东东啦\n- 调用目标方法：就是调用 `calculate`方法。\n\n好，重复一遍最后一句：调用目标方法：就是调用 `calculate`方法。\n\n那么我们看看代码，我们的代理类调用的是谁？\n\n```java\nString indicatorAResult = IndicatorStrategyManager.getStrategy(\"INDICATOR_A\").process();\nString indicatorBResult = IndicatorStrategyManager.getStrategy(\"INDICATOR_B\").process();\n```\n\n是的，没错，我们的代理类调用的是`process`方法，不是`calculate`方法。\n\n那么接下来，就有两个问题了\n\n- 直接用代理类调用calculate方法，AOP真的会生效吗？我已经有点怀疑自己了。\n- 为什么代理类调用process方法，然后process方法调用目标方法calculate的时候，切面不生效呢？\n\n\n\n**直接用代理类调用calculate方法，AOP真的会生效吗**\n\n我们调整一下代码：把调用process的方法改成调用calculate\n\n```java\nString indicatorAResult = IndicatorStrategyManager.getStrategy(\"INDICATOR_A\").calculate(\"test\");\n//String indicatorAResult = IndicatorStrategyManager.getStrategy(\"INDICATOR_A\").process();\n```\n\n发现：切面生效了。\n\n<img src=\"【spring】spring注解AOP开发和源码解读及实践.assets/image-20221223160347614.png\" alt=\"image-20221223160347614\" style=\"zoom:80%;\" />\n\n\n\n**为什么代理类调用process方法，然后process方法调用目标方法calculate的时候，切面不生效呢？**\n\n因为当代理类调用了`process`方法之后，就已经进入了`process`方法内部了；\n\n在`process`方法内部调用`calculate`，其实相当于：`this.calculate`的调用方式；\n\n又是`this`，所以它就已经不是代理类了，所以切面自然也不会生效。\n\n\n\n\n\n## 致谢\n\n- 感谢尚硅谷《spring源码分析》视频教程:https://www.bilibili.com/video/av32102436\n\n- 感谢《Spring AOP 源码解析》一文：https://javadoop.com/post/spring-aop-source\n\n\n\n---\n\n\n\nspring-aop和aspectJ的关系\n\n\n\n\n\nImportBeanDefinitionRegistrar的作用\n\n","tags":["spring","源码解读","AOP"],"categories":["JAVA","SSM三大框架"]},{"title":"我的面试问题","url":"/note/MYSELF/我的面试问题/","content":"\n\n\nredis的基本类型有哪些？\nredis的有哪些使用场景？\nString类型里面有bitmap，了解过吗？\nsorted_set的底层原理是什么？\n\t跳表\nredis的事务\n\t如果A，B两个事务，A事务修改了k1的值，然后查询k1，B事务删了k1，如果B的exec先发送完，A的exec后发送，会出现什么现象\n\t怎么解决这个现象呢？\n缓存穿透？布隆过滤器？\nRedis的持久化？RDB和AOF的区别\n\tRDB的原理：父子进程fork 和 copy on write； 指针指向同一个key，如果这个key修改了怎么办？\nRedis主从中怎么保证数据的一致性（主从复制怎么实现的）\n\t- 通过RDB文件进行同步的，同步有两种方式网络和磁盘\n\tredis的管道pipline用过吗\n\tredis的内存如果用完了，会怎么样？\n\t- Redis有哪几种数据淘汰策略？\n\t- LRU\n\t- 怎么保证都是热点数据\n\tredis的集群有了解吗?\n\t普通哈希，一致性哈希（哈希环），哈希槽\n\n\n\n\n一条查询语句是怎么执行的？\n\n事务的四个特性，Mysql是怎么实现这四个特性的\n事务有哪些隔离级别？\nRR是怎么解决脏读的？\n生产使用的隔离级别是哪种？\nMySQL日志有哪几种？区别呢？\n\n\n\n如何强制使用某个索引？\n如果有一个很长的url存到了库中，我要利用这个字段去精确查询某行记录，怎么创建索引更好？\n前缀索引怎么确定长度？\n\n\nmysql有哪些调优方案？\n\t- 索引，使用覆盖索引，索引下推；\nmysql会不会选错索引？选错了怎么办？\n\t- sql中手动指定索引\n\t- 删除掉走错的索引\nmysql为什么会选错索引？\n\t- 统计不准，可以anlize table；\nanalize table准确吗？\n\t- 不准确，抽样采集的；\n\n\n\n\n\n涉及到财务的系统设计\n- http://confluence.sf-express.com/pages/viewpage.action?pageId=189135181\n- 快递  银行  商家\n- 快递根据快递单生成账单，根据账单的金额，调用银行进行打款申请，打款给商家；\n- 快递单号，发件人，发件人银行账号，收件人，快递金额\n- 用户可能涉及的状态：\n\n\n\nDDD的一些基本概念：\n- 实体和值对象的区别是什么？\n- 一个查询请求过来，打到DDD后台之后，你们的分层是怎么样的，这个请求的链路是什么样子的\n- 防腐层是什么？\n\n\nmysql\n- 对一个字段创建索引，是选择普通索引，还是唯一索引（buffer）？\n- 对一个很长的字段，想精确查询，怎么建立索引？\n- 回表？ 怎么减少回表\n\t- 有没有可能经过索引优化，避免回表过程呢？（覆盖索引）\n- 索引的B+树\n\t- \n- mysql的事务特性\n\t- 日志\n\n\n\n\n\n分布式的基本概念：\n\n二阶段提交流程是什么？有什么问题？\n\nhttps://baijiahao.baidu.com/s?id=1698550212539924249&wfr=spider&for=pc\n\nhttps://baijiahao.baidu.com/s?id=1694171547415193602&wfr=spider&for=pc\n\nhttps://blog.csdn.net/qq_27184497/article/details/103673548\n\nhttps://blog.csdn.net/qq_27184497/article/details/103673548\n\nhttps://zhuanlan.zhihu.com/p/78599954\n\nhttps://www.cnblogs.com/Courage129/p/14528981.html\n\nhttps://www.sdk.cn/details/EmGNy6EYwpyl6WewAX\n\nhttps://www.sdk.cn/details/EmGNy6EYwpyl6WewAX\n\nhttps://zhuanlan.zhihu.com/p/61129707\n\nhttps://blog.csdn.net/qq_38747892/article/details/122326276\n\nhttps://zhuanlan.zhihu.com/p/42056183\n\n\n\n\n\n\nJVM垃圾回收\n\n","tags":["面试"],"categories":["WORKER"]},{"title":"VirtualBox安装CentOS7","url":"/note/LINUX/CENTOS/VirtualBox安装CentOS7/","content":"\n\n\n安装前准备\n\n安装VirtualBox：自行百度安装\n\n准备镜像：可以用 [CentOS官网](https://www.centos.org/)，[CentOS中文官网](http://centos.p2hp.com/) 下载CentOS镜像\n\n我这里下载的是 CentOS 7：http://mirrors.nju.edu.cn/centos/7.9.2009/isos/x86_64/CentOS-7-x86_64-Minimal-2009.iso\n\nCentOS 7提供了三种ISO镜像文件：\n**DVD ISO** 标准安装版，桌面版\n**Everything ISO** 标准安装版的补充，增加了大量的应用软件\n**Minimal ISO** 精简版，自带的应用软件最少，生产环境推荐使用\n\nMinimal版本优点：\n\n- 节省系统资源，磁盘空间占用小\n- 自带软件少，系统纯净运行更稳定\n- 需要的软件包可以自行安装\n\n\n\n\n\n创建虚拟机\n\n1：点击新建\n\n2：输入名称：centos7；文件夹：是保存虚拟机系统的文件；类型：选择Linux；版本：选择Other Linux(64-bit)\n\n3：选择虚拟机的内存大小：根据自己需求选择，我这里选择了2G\n\n4：创建虚拟机的硬盘：根据自己需求选择，可以稍后创建，现在创建，我选择了：现在创建\n\n5：选择虚拟机硬盘的类型：根据自己需求选择，我选择了默认的VDI类型\n\n6：选择虚拟机硬盘的大小：根据自己需求选择，我选择了动态分配\n\n7：选择虚拟机硬盘的位置：根据自己需求选择，我选择了虚拟硬盘放在D盘\n\n8：选择虚拟机硬盘的大小：根据自己需求选择，我选择了硬盘大小最大为20GB\n\n9：创建完成后，可以选中当前虚拟机\n\n10：点击设置\n\n11：可以对创建好的虚拟机再次调整参数，比如我这里将虚拟机的CPU调整为了2核\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215105959953.png\" alt=\"image-20230215105959953\" style=\"zoom:80%;\" />\n\n\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215110204977.png\" alt=\"image-20230215110204977\" style=\"zoom:80%;\" />\n\n\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215110229245.png\" alt=\"image-20230215110229245\" style=\"zoom:80%;\" />\n\n\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215110308994.png\" alt=\"image-20230215110308994\" style=\"zoom:80%;\" />\n\n\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215110325528.png\" alt=\"image-20230215110325528\" style=\"zoom:80%;\" />\n\n\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215110400735.png\" alt=\"image-20230215110400735\" style=\"zoom:80%;\" />\n\n\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215110904381.png\" alt=\"image-20230215110904381\" style=\"zoom:80%;\" />\n\n\n\n启动虚拟机\n\n1：选中虚拟机\n\n2：点击启动，点击之后，稍等片刻，会弹出虚拟机启动窗口\n\n3：在虚拟机窗口，选择启动镜像，点击文件夹图标\n\n4：选择 注册\n\n5：找到我们之前下载的 CentOS7 的镜像，开启启动\n\n6：通过键盘，移动上下左右箭头，选择 Install CentOS 7，回车\n\n7：选择中文，继续\n\n8：选择安装位置\n\n9：将安装位置选中为我们创建的虚拟硬盘\n\n10：完成\n\n11：开始安装\n\n12：创建root密码，我这里将root设置为：root\n\n13：创建用户，根据自己的需求选择是否创建本地用户，我这里就不创建了，不创建的话，默认就只有一个root用户\n\n14：安装完成，开始重启，重启之后，就可以登录了。\n\n\n\n\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215111205370.png\" alt=\"image-20230215111205370\" style=\"zoom:80%;\" />\n\n\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215111944905.png\" alt=\"image-20230215111944905\" style=\"zoom:80%;\" />\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215112021017.png\" alt=\"image-20230215112021017\" style=\"zoom:80%;\" />\n\n\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215112159337.png\" alt=\"image-20230215112159337\" style=\"zoom:80%;\" />\n\n\n\n\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215112256613.png\" alt=\"image-20230215112256613\" style=\"zoom:80%;\" />\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215112320332.png\" alt=\"image-20230215112320332\" style=\"zoom:80%;\" />\n\n\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215112345724.png\" alt=\"image-20230215112345724\" style=\"zoom:80%;\" />\n\n\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215112447552.png\" alt=\"image-20230215112447552\" style=\"zoom:80%;\" />\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215115027821.png\" alt=\"image-20230215115027821\" style=\"zoom:80%;\" />\n\n<img src=\"VirtualBox安装CentOS7.assets/image-20230215115129320.png\" alt=\"image-20230215115129320\" style=\"zoom:80%;\" />\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["centos"],"categories":["LINUX","CENTOS"]},{"url":"/note/JAVA/部署与容器/DOCKER/docker常用的启动脚本/","content":"---\n\n---\n\n\n\n\n\n\n\n```sh\ndocker run -d --name=elasticsearch7.6.2 \\\n -p 9200:9200 -p 9300:9300 \\\n -e \"discovery.type=single-node\" \\\n -v /Users/zhuansun/workspace/docker/elasticsearch7.6.2/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \\\n -v /Users/zhuansun/workspace/docker/elasticsearch7.6.2/data:/usr/share/elasticsearch/data \\\n -v /Users/zhuansun/workspace/docker/elasticsearch7.6.2/logs:/usr/share/elasticsearch/logs \\\n -v /Users/zhuansun/workspace/docker/elasticsearch7.6.2/plugins:/usr/share/elasticsearch/plugins \\\n  elasticsearch:7.6.2\n\n\n\ndocker logs elasticsearch7.6.2\n\ndocker exec -it --user root elasticsearch7.6.2 /bin/bash\n\n\n\n\n\ndocker run -p 3306:3306 --cpus 2 -m 4GB --name mysql5.7.19 \\\n-v /Users/zhuansun/workspace/docker/mysql5.7.19/conf:/etc/mysql \\\n-v /Users/zhuansun/workspace/docker/mysql5.7.19/logs:/var/log/mysql \\\n-v /Users/zhuansun/workspace/docker/mysql5.7.19/data:/var/lib/mysql \\\n-e MYSQL_ROOT_PASSWORD=123456 \\\n-d mysql:5.7.19\n\n\n\n\n\n\n\n\n\ndocker run -p 6379:6379 --name redis5.0.7 \\\n -v /Users/zhuansun/workspace/docker/redis5.0.7/redis.conf:/etc/redis/redis.conf \\\n -v /Users/zhuansun/workspace/docker/redis5.0.7/data:/data \\\n -d redis:5.0.7 redis-server /etc/redis/redis.conf --appendonly yes\n\ndocker exec -it redis5.0.7 /bin/bash\n# -p 6379:6379:把容器内的6379端口映射到宿主机6379端口\n# -v /root/docker/redis/redis.conf:/etc/redis/redis.conf：把宿主机配置好的redis.conf放到容器内的这个位置中\n# -v /root/docker/redis/data:/data：把redis持久化的数据在宿主机内显示，做数据备份\n# redis-server /etc/redis/redis.conf：这个是关键配置，让redis不是无配置启动，而是按照这个redis.conf的配置启动\n# -appendonly yes：redis启动后数据持久化\n\n\n```\n\n"},{"title":"mysql的索引从入门到入土","url":"/note/JAVA/数据库/MYSQL/mysql的索引从入门到入土/","content":"\n\n\n# mysql的索引从入门到入土\n\n\n\n\n\n## 索引的XMIND图\n\n文件位置：[点我打开](./mysql的索引从入门到入土.assets/Mysql索引.xmind)\n\n\n\n## 索引的常见数据结构\n\n\n\n哈希表\n\n- 哈希表这种结构适用于只有等值查询的场景，比如 Memcached 及其他一些 NoSQL 引擎。\n\n有序数组\n\n- 有序数组在等值查询和范围查询场景中的性能就都非常优秀。\n\n- 有序数组索引只适用于静态存储引擎\n\n搜索树\n\n- 二叉搜索树：父节点左子树所有结点的值小于父节点的值，右子树所有结点的值大于父节点的值。\n\n- 平衡二叉树：\n\n- N叉树：\n\n\n\n\n\n## InnoDB为什么选择B+树（对比其他树）\n\n\n\n### 为什么选择树\n\n树的查询效率高，还可以保持有序。\n\n\n\n### 为什么不用二叉搜索树\n\n#### 什么是二叉搜索树（也叫二叉排序树或者二叉查找树）\n\n以下是二叉查找树的结构：\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219173305682.png\" alt=\"image-20221219173305682\" style=\"zoom:50%;\" />\n\n它的**特点**是：\n\n- 父节点左子树所有结点的值小于父节点的值，右子树所有结点的值大于父节点的值。\n- 只有两个叉\n- 查询的时间复杂度是：O(log(N))\n- 从算法逻辑来讲，二叉搜索树的查找速度和比较次数都是最小的。\n\n\n\n#### 为什么不用二叉搜索树（查找耗磁盘IO）\n\n因为考虑到磁盘IO的性能。\n\n为什么这么说：InnoDB在查找的时候，是不能将所有的索引全部加载到内存中的，所以必然涉及到磁盘IO。\n\n比如下面这个，二叉搜索树，我们想获取到10，需要经历几次磁盘IO：\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219173305682.png\" alt=\"image-20221219173305682\" style=\"zoom:50%;\" />\n\n答案是：4次\n\n- 第一次磁盘IO，读取9\n- 第二次磁盘IO，读取13\n- 第三次磁盘IO，读取11\n- 第四次磁盘IO，读取10\n\n这样性能是很低的。\n\n所以InnoDB并没有采用二叉搜索树来作为索引的数据结构。\n\n但是InnoDB的设计者，又不能抛弃树这个数据结构带来的遍历，所以只能将 “瘦高” 的二叉搜索树，让它变得 “矮胖”。以便于节省磁盘IO。这就是`B树`的特征之一\n\n\n\n### 什么是B树\n\n#### 什么是B树\n\n上面说了，InnoDB的设计者为了想保留树带来的便利，只能将： “瘦高” 的二叉搜索树，让它变得 “矮胖”。以便于节省磁盘IO。\n\nB树是一种`多路平衡查找树`，它的每一个节点最多包含`m`个孩子，`m`被称为`B树`的阶，`m`的大小取决于磁盘页的大小。\n\n下图就是一个 `3阶的B树` 示意图\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219180143637.png\" alt=\"image-20221219180143637\" style=\"zoom:50%;\" />\n\n\n\n一颗`m阶的B树`，它的特点是：\n\n- `m阶`表示每一个节点最多拥有`m`个孩子\n- 根节点至少有 `2` 个子女\n- 有`k`个孩子的节点包含有k-1个元素，其中： `ceil(m/2) <= k <= m`，（ceil表示向上取整）\n- 每一个叶子节点都包含`k-1`个元素，其中：`ceil(m/2) <= k <= m`，（ceil表示向上取整）\n- 所有的叶子节点都位于同一层\n- 每个节点的元素从小到大排列，并且当该结点的孩子是非叶子结点时，节点中第`k-1`个元素正好是`k`个孩子包含的元素的值域分划。\n\n\n\n\n\n我们一一来看这些特点，其中\n\n- 一颗`3阶`的树，每个节点最多包含`3`个孩子，m = 3\n- `9`是根节点，有`2`个孩子`（2,6）`和 `12`。(根节点至少`2`个)\n- `（2,6）`这个中间节点包含`2`个元素：`2`和`6`，有`3`个孩子。 `2 <= k <= 3`\n- `12 `这个中间节点包含`1`一个元素：`12`，有`2`个孩子。满足规则\n- 叶子节点`1`,`(3,5)`,`8`,`11`,`(13,15)`位于同一层\n- 每个节点的元素从小到大排列，`(3,5)`在`(2,6)`的值域之间。\n\n\n\n#### B树的等值查找\n\n\n\n在下面的一个3阶B树中，查找5，需要经历几次磁盘IO：\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219180143637.png\" alt=\"image-20221219180143637\" style=\"zoom:50%;\" />\n\n答案是：三次\n\n- 第一次磁盘IO：`9`\n- 第二次磁盘IO：`（2,6）`\n- 第三次磁盘IO：`（3,5）`\n\n\n\n#### B树的范围查找\n\n在下面的一个3阶B树中，查找范围是 3-11 的元素，需要经历几次磁盘IO：\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219180143637.png\" alt=\"image-20221219180143637\" style=\"zoom:50%;\" />\n\n答案是：好多好多次\n\n- 3次磁盘IO后，找到范围下限：3 所在的（3,5）这个节点\n- 中序遍历到 元素6\n- 中序遍历到 元素8\n- 中序遍历到 元素9\n- 中序遍历到 元素11，遍历结束\n\n\n\n由此可见，B树的范围查找，很繁琐。\n\n\n\n\n\n\n\n#### B树的插入（自平衡）\n\n插入比较复杂\n\n以下面的为例子：在一个3阶的B树中，插入 4\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219180143637.png\" alt=\"image-20221219180143637\" style=\"zoom:50%;\" />\n\n由于 4 在 （3,5）之间。\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219185214252.png\" alt=\"image-20221219185214252\" style=\"zoom:50%;\" />\n\n- 这是一颗`3阶`的B树，由于每个节点最多包含 `k-1` 个元素，其中  `2 <= k <= 3`，所以每个节点可以包含`1个`，`2个`元素；\n- 要在`（3,5）`之间插入 `4`，`（3,5）`已经是`2`个元素了，不能在插入了。\n- `（3,5）`的父节点`（2,6）`也是`2`个元素，也不能在插入了\n- 根节点 `9` 是`1`个元素，可以在升级为2个元素。\n- 于是：\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219190355215.png\" alt=\"image-20221219190355215\" style=\"zoom:50%;\" />\n\n总结：\n\n- 仅仅是插入一个元素，就让整个B树发生了连锁反应\n- 虽然麻烦，但是也正因为如此，可以让B树始终保持多路平衡。（**自平衡**）\n\n\n\n#### B树的删除（左旋）\n\n\n\n在如下的一个3阶B树中，删除元素11\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219190937755.png\" alt=\"image-20221219190937755\" style=\"zoom:50%;\" />\n\n\n\n- 首先这是一颗3阶B树，由于B树的规则，每个中间节点都包含`k-1`个元素和`k`个孩子，其中 ` 2<= k <= 3`\n- 所以：11删除之后，父节点12就只剩下1个孩子了，不符合规则，3阶的B树每个中间节点至少有2个孩子。\n- 因为，需要找出删除11后，剩余的三个元素12,13,15的中位数，取代节点12；然后节点12下移成为孩子（**左旋**）\n\n\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219191850585.png\" alt=\"image-20221219191850585\" style=\"zoom:50%;\" />\n\n\n\n#### B树的卫星数据\n\n所谓的卫星数据（Satellite Information），指的是索引元素所指向的数据记录；\n\n在B树中，每一个节点都带有卫星数据。\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219221417559.png\" alt=\"image-20221219221417559\" style=\"zoom:50%;\" />\n\n\n\n### 为什么选择B+树\n\n#### 什么是B+树\n\nB+树是基于B树的一种变体。有着比B树更高的查询性能\n\n下面是一个3阶的B+树的示意图：节点之间含有重复元素，叶子节点还用指针连在一起\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219215758443.png\" alt=\"image-20221219215758443\" style=\"zoom:50%;\" />\n\n\n\n\n\n一颗m阶的B+树有以下特点：\n\n- 有k个子树的中间节点包含有k个元素（B树中是k-1个元素）\n- 每个元素不保存数据，只用来索引，所有的数据保存在叶子节点\n- 所有的叶子节点包含了全部元素的信息，而且每个叶子节点都带有指向下一个节点的指针，形成了一个有序链表\n- 叶子节点本身依关键字大小自小而大顺序链接\n- 每一个父节点的元素都出现在子节点中，而且是子节点中最大（或最小）的元素\n- 无论插入多少元素，都要保持最大元素在根节点中\n\n\n\n\n\n#### B+树的卫星数据\n\n所谓的卫星数据（Satellite Information），指的是索引元素所指向的数据记录；\n\n在B+树中，只有叶子节点带有卫星数据。\n\n在mysql中，表中的一行记录就是一个卫星数据。\n\n不过需要注意的是：\n\n- 在mysql中，主键索引的叶子结点存的是`卫星数据`（就是行记录）\n- 在mysql中，非主键索引的叶子节点存的是`指向卫星数据的指针`（就是行记录的主键id）\n\n\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219221505548.png\" alt=\"image-20221219221505548\" style=\"zoom:50%;\" />\n\n\n\n#### B+树的等值查找\n\n\n\n在下面这颗3阶的B+树上查找元素 3，会经历几次磁盘IO：\n\n\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219215758443.png\" alt=\"image-20221219215758443\" style=\"zoom:50%;\" />\n\n\n\n结果：\n\n- 第一次磁盘IO：（8,15）\n- 第二次磁盘IO：（2,5,8）\n- 第三次磁盘IO：（3,5）\n\n\n\n由于B+树的中间节点并没有存放卫星数据，所以比B树更加“矮胖”，也即是说：在相同数据量的情况下，B+树会比B树访问到更少的磁盘IO。\n\n由于B+树的中间节点没有存放卫星数据，所以要想拿到记录，必须访问到叶子节点。而B树由于每个节点都存放了卫星数据，所以只要匹配到，就可以获取到记录，对B树来说：最好的情况是只访问根节点就能拿到记录，最坏的情况就是访问到叶子结点。换言之：B树的查找性能不稳定，B+树由于每一次都要访问到叶子节点，所以B+树的查找性能是稳定的。\n\n\n\n\n\n#### B+树的范围查找\n\n在下面这颗3阶的B+树上，查找范围是 3-11 的元素，需要经历几次磁盘IO：\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219215758443.png\" alt=\"image-20221219215758443\" style=\"zoom:50%;\" />\n\n答案是：5次\n\n- 3次磁盘IO后，找到范围下限：3 所在的（3,5）这个节点\n- 然后通过叶子结点上的指针，直接遍历叶子节点。（3,5），（6,8），（9,11）\n- 直接确定到范围\n\n\n\n相比于B树范围查找的繁琐，B+树就简单的多了。\n\n\n\n### B+树的B树的比较\n\nB+树相比B树的优势有三个\n\n- IO次数更少\n- 查询性能稳定\n- 范围查询简便\n\n至于插入和删除，两者大同小异。\n\n\n\n\n\n## 索引分类\n\n了解下面列出来的索引概念：\n\n- 主键索引\n- 普通索引\n- 聚簇索引\n- 非聚簇索引\n- 二级索引\n- 唯一索引\n- 全文索引\n- 联合索引\n- 覆盖索引\n- 前缀索引\n\n\n\n### 主键索引和普通索引\n\n主键索引：也叫聚簇索引，它 是一种特殊的唯一索引，不允许有空值。\n\n普通索引：也叫二级索引，最基本的索引，没有任何限制\n\n主键索引在InnoDB中是聚簇索引；普通索引在InnoDB中是非聚簇索引\n\n\n\n\n\n### 聚簇索引和非聚簇索引\n\n聚簇索引并不是单独的索引类型，而是一种数据存储方式，指的是：数据和对应的索引紧凑的存储在一起。\n\n非聚簇索引指的就是：数据和索引分开存储。\n\n举例来说：\n\n对于InnoDB：主键索引的叶子节点存的是主键id和数据，它们是保存在一起的；非主键索引的叶子节点存的是主键id，和真实的数据是分开的。所以主键索引是聚簇索引，非主键索引是非聚簇索引。（InnoDB的索引和数据是放在同一个文件中的：数据即索引，索引即数据）\n\n对于MyISAM：索引和记录本身就是分开存储的，是不同的文件。所以MyISAM中所有的索引都是非聚簇索引。\n\n\n\n### 唯一索引\n\n唯一索引：与\"普通索引\"类似，不同的就是：索引列的值必须唯一，但**允许有空值**。\n\n\n\n### 全文索引\n\n全文索引：仅可用于 MyISAM 表，针对较大的数据，生成全文索引很耗时耗空间。\n\n\n\n### 联合索引\n\n也叫组合索引，为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则。\n\n\n\n### 覆盖索引\n\n当一个SQL查询语句所需要查询的字段，被所使用的的索引可以直接满足需求，不需要回表了，称之为覆盖索引\n\n\n\n### 前缀索引\n\n当需要索引的字段很长，而且该字段后半部分的选择性很低，比如邮箱地址；后面都是一样的。\n\n对于这种我们就可以建立前缀索引：使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。\n\n```sql\n\nmysql> alter table SUser add index index2(email(6));\n```\n\n前缀索引会导致覆盖索引失效，因为前缀索引是截取后的部分做的索引，不得不在回表查一次完整的信息。\n\n\n\n## 索引特点\n\n\n\n### 最左匹配原则\n\n\n\n假设有一个用户表，我们给 （name, age）加上联合索引。\n\n如果我们要查询所有名字第一个字是“张”的人，SQL语句可能会这么写：\n\n> where name like '张%';\n\n在这种情况下，也是可以使用到这个索引的。这就是索引的最左匹配原则。\n\n\n\n- 最左匹配原则可以是联合索引的最左N的字段，也可以是一个字符串的最左M个字符；\n- 在建立联合索引的时候，如何安排索引内的字段顺序，需要仔细考量了。\n\n\n\n### 索引下推\n\n\n\nmysql 5.6 引入了 索引下推优化。可以在索引遍历的过程中，对索引中包含的字段先做判断，直接过滤掉不满足的记录，减少回表次数。\n\n\n\n假如有一个用户表，我们给（name，age）加上联合索引。\n\n如果我们要查询所有名字第一个字是“张”的人，并且年龄大于10岁的男人。SQL语句可能会这么写\n\n> where name like '张%' and age > 10 and male='男';\n\n对于这个语句，我们知道由于最左匹配原则，使用到了（name，age）这个索引。\n\n然后，接下来呢？\n\n在mysql 5.6之前，接下来就需要回表了。对扫描的每一条记录，都回表判断 age 和 male 是否满足；\n\n在mysql 5.6之后，由于有索引下推优化，对于age的判断可以直接完成，过滤掉age不符合的记录，然后对剩下的记录在此回表判断male是否满足。（减少了回表次数）\n\n\n\n### 索引潜水\n\n英文单词：index dive\n\n主要是用于 IN 查询语句的优化，\n\n跟这个词语相关的，还有一个配置参数 **eq_range_index_dive_limit**。\n\n**MySQL5.7.3**之前的版本，这个值默认是10，之后的版本，这个值默认是200。\n\n简单的理解就是：IN 语句的数量如果少于200个，就使用**索引潜水（Index dive）**预估扫描行数，很精确，所以选择索引的时候就不容易出错；\n\n如果 IN 语句的数量大于200个，就使用 **索引统计（Index statistics）**预估扫描行数，误差很大，所以会导致选择索引容易选错，导致性能下降\n\n\n\n具体见：Mysql性能调优从入门到入土\n\n\n\n### 普通索引默认联合主键索引\n\n二级索引会默认与主键索引做联合索引。\n\n为什么？\n\n因为二级索引的叶子节点存放的就是主键。\n\n\n\n\n\n## 索引比较\n\n\n\n### 普通索引和主键索引有什么区别？\n\n主键索引的查询，不需要回表了。\n\n普通索引的查询，如果要查询的字段没有被当前索引覆盖，是需要回表的。\n\n针对回表的优化：覆盖索引可以减少回表，索引下推可以减少回表\n\n\n\n\n\n### 普通索引和唯一索引有什么区别？\n\n#### 对查询的影响\n\n- 微乎其微。\n- 普通索引的查询：定位到当前记录之后，还要继续判断下一条记录是否满足条件。\n- 唯一索引的查询：定位到当前记录之后，由于是唯一的，不再需要判断下一条记录了。\n\n不过，后面看到加锁规则这部分的时候，有一个加锁规则是：唯一索引上的范围查询，会访问到不满足查询条件的第一个值为止。 \n\n也就是说：\n对于范围查询，普通索引和唯一索引是一样的，都需要访问下一条记录。\n对于等值查询，唯一索引比普通索引少了一次查找下一条记录的性能消耗。\n\n\n\n#### 对更新的影响\n\n- 参考：《mysql的日志从入门到入土》这篇文章中讲到了changebuffer：\n  - changebuffer（对更新的优化，对比redolog）\n  - changebuffer在 5.5 版本之前叫做：insert buffer，只支持插入操作；在5.5之后，支持了更新操作\n  - changebuffer的作用：\n  - 更新的时候不需要从内存中加载数据页，而是直接将更新后的记录写到changebuffer中（内存中）就可以返回了\n  - 那么changebuffer中的数据什么时候刷盘呢？\n  - 因为写入changebuffer了，一定会写入redolog，后台会定时将redolog中的数据应用到数据页中，同时也会将changebuffer的数据应用到数据页，这是两个后台线程，他俩没有任何关系。操作的都是内存中的数据页，然后刷脏页\n  - 具体的可以参考：《mysql的日志从入门到入土》\n\n- 为什么唯一索引的更新就不能使用 change buffer，只有普通索引可以使用。\n  - 因为唯一索引的更新，需要判断当前的这个更新会不会影响到唯一索引的唯一性。\n\n  - 所以必须判断要将数据页加载到内存中，进行判断才可以。\n\n- change buffer适应用：写多读少的场景和写多读多的场景；\n\n- change buffer 和 redo log的联系和区别。\n  - change buffer减少了磁盘的随机读（减少了加载数据页的操作）\n  - redo log减少的磁盘的随机写（WAL机制）\n\n\n\n### 前缀索引对覆盖索引的影响\n\n我们知道覆盖索引可以减少回表，提升查询性能；\n\n但是当我们的覆盖索引是前缀索引的时候，此时覆盖索引是不生效的。\n\n因为前缀索引是截取后的信息作为索引，要想获取完整的字段，就不得不回表在查询一次。\n\n\n\n\n\n\n## 选错索引（索引统计，强制索引）\n\n- 由于索引统计的更新机制，索引统计信息不准确导致的。\n\n- 解决：重新采集统计信息：analyze table\n\n- 解决：手动指定索引；force index\n\n\n\n### 如何进行索引统计\n\n\n\n\n\n## 索引排序（order by）\n\n\n\n在日常的开发过程中，经常会使用`order by`语句，那么排序的原理是什么呢？\n\n排序分为两种情况\n\n- 排序的字段有索引\n- 排序的字段没有索引\n\n\n\n### 有索引的排序\n\n我们知道索引是有序的，所以有索引的排序会直接使用索引；\n\n直接使用索引排序的话，`explain`结果中是不会有 Using filesort 的\n\n\n\n### 没有索引的排序\n\n没有索引的排序，相对来说复杂一下。\n\n一个语句是否使用了排序，可以通过：`explain`命令查看，结果中有 Using filesort ，表示的就是需要排序。\n\nMySQL 会给每个线程分配一块`内存`用于排序，称为` sort_buffer`。\n\n\n\n####  sort_buffer是什么\n\nMySQL 会给每个线程分配一块内存用于排序，称为 sort_buffer；\n\nsort_buffer既然是内存区域，就不可能无限的扩大，是可以通过参数`sort_buffer_size`控制的\n\n\n\n\n\n#### sort buffer有多大（内部排序和外部排序）\n\n通过参数`sort_buffer_size`控制的，默认是：262144（公司是：8388608）\n\nsort_buffer_size，就是 MySQL 为排序开辟的内存（sort_buffer）的大小。\n\n如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。 这叫做：`内部排序`\n\n但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。这叫做：`外部排序`\n\n\n\n内存放不下时，就需要使用外部排序，外部排序一般使用归并排序算法。\n\n可以这么简单理解，MySQL 将需要排序的数据分成 N 份，每一份单独排序后存在这些临时文件中。然后把这 N 个有序文件再合并成一个有序的大文件。\n\n\n\n内部排序：使用快速排序\n\n外部排序：使用归并排序\n\n\n\n#### 全字段排序\n\n\n\n示例数据\n\n```sql\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `city` varchar(16) NOT NULL,\n  `name` varchar(16) NOT NULL,\n  `age` int(11) NOT NULL,\n  `addr` varchar(128) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `city` (`city`)\n) ENGINE=InnoDB;\n\n-- 初始化10条数据\n```\n\n当我们执行下面的查询语句的时候\n\n```sql\nselect city,name,age from t where city='杭州' order by name limit 1000  ;\n```\n\n它的排序执行流程如下：\n\n- 初始化 sort_buffer，确定放入 name、city、age 这三个字段；\n- 从索引 city 找到第一个满足 city='杭州’条件的主键 id；\n- 到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中\n- 从索引 city 取下一个记录的主键 id；\n- 重复步骤 3、4 直到 city 的值不满足查询条件为止；\n- 对 sort_buffer 中的数据按照字段 name 做快速排序；\n- 按照排序结果取前 1000 行返回给客户端。\n\n\n\n\n\n在上面这个过程里面，只对原表的数据读了一遍，剩下的操作都是在 sort_buffer 和临时文件中执行的。\n\n但这个算法有一个问题，就是如果查询要返回的字段很多的话，那么 sort_buffer 里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。\n\n那么，如果 MySQL 认为排序的单行长度太大会怎么做呢？\n\n\n\n#### rowId排序\n\n如果 MySQL 认为排序的单行长度太大会怎么做呢？\n\n`max_length_for_sort_data`：是 MySQL 中专门控制用于排序的行数据的长度的一个参数。\n\n它的意思是，如果单行的长度超过这个值，MySQL 就认为单行太大，要换一个算法。\n\n\n\n示例数据\n\n```sql\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `city` varchar(16) NOT NULL,\n  `name` varchar(16) NOT NULL,\n  `age` int(11) NOT NULL,\n  `addr` varchar(128) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `city` (`city`)\n) ENGINE=InnoDB;\n\n-- 初始化10条数据\n```\n\n当我们执行下面的查询语句的时候\n\n```sql\nselect city,name,age from t where city='杭州' order by name limit 1000  ;\n```\n\ncity、name、age 这三个字段的定义总长度是 36，我把 max_length_for_sort_data 设置为 16，我们再来看看计算过程有什么改变。\n\n```sql\nSET max_length_for_sort_data = 16;\n```\n\n新的算法放入 sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id。\n\n但这时，排序的结果就因为少了 city 和 age 字段的值，不能直接返回了，整个执行流程就变成如下所示的样子：\n\n- 初始化 sort_buffer，确定放入两个字段，即 name 和 id；\n- 从索引 city 找到第一个满足 city='杭州’条件的主键 id；\n- 到主键 id 索引取出整行，取 name、id 这两个字段，存入 sort_buffer 中；\n- 从索引 city 取下一个记录的主键 id；\n- 重复步骤 3、4 直到不满足 city='杭州’条件为止；\n- 对 sort_buffer 中的数据按照字段 name 进行排序；\n- 遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回给客户端。\n\n\n\n#### 全字段排序 VS rowid 排序\n\n- MySQL 如果觉得排序内存太小，会影响排序效率，会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要回表\n- MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，可以减少回表\n\n\n\n## 索引的生效条件\n\n以下三种情况，都会影响索引的使用\n\n- 条件字段使用函数\n- 隐式类型转换\n- 隐式字符编码转换\n\n\n\n### 条件字段函数操作会影响索引\n\n> select * from t where month(created_time) = 7;  -- 查询创建时间是7月份的记录，created_time有索引\n\n由于加了 month() 函数操作，created_time索引树里面，并不认识7，所以MySQL 无法再使用索引快速定位功能，而**只能使用全索引扫描**。\n\n> select * from t where id + 1 = 5;\n\n这个加 1 操作并不会改变主键索引的有序性，但是 MySQL 优化器还是不能用 id 索引快速定位到 id=4 这一行。\n\n所以，需要你在写 SQL 语句的时候，手动改写成 where id = 5 -1 才可以。\n\n\n\n### 隐式类型转换\n\n这个坑，今天刚踩过，表里对于单号 order_no 有索引，order_no 字段是varchar类型，但是我的语句是这么写的\n\n> mysql> select * from t where order_no=110717;\n\nexplain 的结果显示：这条语句需要走全表扫描\n\n因为：order_no 字段是varchar类型，而我的查询条件里用到的是整型。所以mysql会默认用到类型转换，**导致索引失效**\n\n**字符串和数字做比较的话，是将字符串转换成数字**；所以，上面那个语句等同于\n\n> mysql> select * from t where CAST(order_no signed int)=110717;\n\n这也就明白了，为什么不会走索引了。\n\n然后考虑下面这个语句，会走主键索引吗\n\n> mysql> select * from t where id='11';\n\n答案是：会走索引，因为：字符串和数字做比较的话，是将字符串转换成数字，所以没有对条件字段使用函数。索引不会失效\n\n\n\n### 隐式字符编码转换\n\n如果关联的两张表，使用的字符集不同的话，比如一个使用utf8，一个使用utf8mb4； 所以做表连接查询的时候用不上关联字段的索引。\n\n为什么字符集不一样，不能用索引呢？\n\n因为字符集 utf8mb4 是 utf8 的超集，所以当这两个类型的字符串在做比较的时候，MySQL 内部的操作是，先把 utf8 字符串转成 utf8mb4 字符集，再做比较。\n\n（自动类型转换的时候，为了避免数据在转换过程中由于截断导致数据错误，也都是“**按数据长度增加的方向**”进行转换的。）\n\n\n\n\n\n\n\n\n\n\n\n## InnoB的索引模型（B+树）\n\n\n\n### 复习B+树\n\n每一个索引在 InnoDB 里面对应一棵 B+ 树。在复习一下B+树\n\n下面是一个3阶的B+树的示意图：\n\n- 节点之间含有重复元素\n- 叶子节点还用指针连在一起\n\n<img src=\"mysql的索引从入门到入土.assets/image-20221219215758443.png\" alt=\"image-20221219215758443\" style=\"zoom:50%;\" />\n\n接下来：\n\n- 我们考虑各个索引在InnoDB中具体的B+树结构是什么样子的\n- 这些索引是怎么维护的（插入数据，删除数据，更新数据对索引的影响，页分裂，页合并）\n- 这些索引是怎么使用的（等值查询，范围查询）\n\n\n\n### 案例数据\n\n准备一张表：\n\n```sql\nCREATE TABLE `user` (\n  `id` int(11) NOT NULL,\n  `name` varchar(16) NOT NULL,\n  `age` int(11) NOT NULL,\n  `city` varchar(16) NOT NULL,\n  `address` varchar(128) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `city` (`city`),\n  KEY `name_age`(`name`,`age`)\n) ENGINE=InnoDB;\n```\n\n- 表里有一个主键 id\n- 表里有一个普通索引 city\n- 表里有一个联合索引 name_age\n- 接下来，我们看看，不停的向表里写入数据，会发生什么\n\n\n\n**重要声明**：\n\n- 我们知道mysql的数据是按照 数据页 来进行存储的，对于索引来说，同样也是 数据页 存储的。\n- 一个数据页是 16KB，所以一个数据库能存放的元素是有限的。\n- 在测试的时候：我们假设\n  - `id`是int类型，占`4`个字节\n  - `name`是varchar类型，因为是不定长的，假设 name 不论写入什么，都占`8`个字节\n  - `age`是int类型，占`4`个字节\n  - `city`是varchar类型，因为是不定长的，假设 city 不论写入什么，都占`8`个字节\n  - `address`是varchar类型，因为是不定长的，假设 address 不论写入什么，都占`8`个字节\n- 正常情况下，一个数据页是16KB，我们这里假设，一个数据页只有：`36字节`\n  - 意味着：一个数据页最多能存9个id索引元素(4x9=36)；最多能存4个city索引元素(8x4=32)，最多能存3个 name_age 联合索引元素(12x3=36);\n\n\n\n### 主键索引\n\n\n\n我们上边介绍的`B+`树本身就是一个目录，或者说本身就是一个索引。它有两个特点：\n\n1. 使用记录主键值的大小进行记录和数据页的排序，这包括三个方面的含义：\n\n   - 数据页中的记录是按照主键的大小顺序排成一个单向链表。\n   - 各个 存放用户记录的数据页 也是 根据 数据页中用户记录的主键大小 顺序排成一个双向链表。\n   - 存放 目录项记录的数据页 分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个双向链表。\n\n2. `B+`树的叶子节点存储的是完整的用户记录。\n\n   所谓完整的用户记录，就是指这个记录中存储了所有列的值（包括隐藏列）。\n\n我们把具有这两种特性的`B+`树称为`聚簇索引`，所有完整的用户记录都存放在这个`聚簇索引`的叶子节点处。这种`聚簇索引`并不需要我们在`MySQL`语句中显式的使用`INDEX`语句去创建（后边会介绍索引相关的语句），`InnoDB`存储引擎会自动的为我们创建聚簇索引。另外有趣的一点是，在`InnoDB`存储引擎中，`聚簇索引`就是数据的存储方式（所有的用户记录都存储在了`叶子节点`），也就是所谓的索引即数据，数据即索引。\n\n\n\n\n\n\n\n\n\n### 普通索引\n\n\n\n\n\n\n\n### 联合索引\n\n`B+`树按照`name`和`age`列的大小进行排序，这个包含两层含义：\n\n- 先把各个记录和页按照`name`列进行排序。\n- 在记录的`name`列相同的情况下，采用`age`列进行排序\n\n\n\n\n\n\n\n### 索引维护\n\n\n\n索引维护（更新，页分裂，页合并）\n\n为什么我们一般在建表的时候都会创建一个自增主键，及时表中有业务唯一的id，也会创建一个自增主键？\n\n1、因为主键只会自增，在B+树中一直都是往后写的，不会触发页分裂；但是如果删除过多的话，会触发页合并；\n\n2、要考虑业务唯一的id的长度，比如身份证号，如果用身份证号作为主键，比直接使用整型自增的主键占用的字节数要多，这样每一个页存放的数据就会少，每个页存的数据少了，这样查询的时候，效率就会低。 同时主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。\n\n3、所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。\n\n\n\n\n\n索引的页分裂和页合并是怎么导致的，会有什么影响？具体的分裂和合并的过程是什么样子的？\n\n\n\n为什么mysql的b+树，在非叶子节点中，也保留了双向列表\n\nhttps://www.zhihu.com/question/478187330/answer/2050494617\n\n\n\n\n\n","tags":["索引","mysql"],"categories":["JAVA","数据库","MYSQL"]},{"title":"mysql的性能调优从入门到入土","url":"/note/JAVA/数据库/MYSQL/mysql的性能调优从入门到入土/","content":"\n---\n\n\n\n\n\n\n\n资料：打开\n\n\n\n\n\n事务中，行锁是在语句执行时才加上的，不是事务开始就加上，但释放是统一在事务结束时才释放。根据这个特性，对于高并发的行记录的操作语句就可以尽可能的安排到最后面，以减少锁等待的时间，提高并发性能\n\n\n\n减少大事务，可以减少主备延迟，可以降低锁的时间，减少死锁的产生，减少死锁检测的性能消耗。\n\n减少并发量（减少对于同一行更新的并发量），可以减少死锁的产生，可以减少死锁检测的性能消耗\n\n\n\n\n\n在删除数据的时候尽量加 limit。这样不仅可以控制删除数据的条数，让操作更安全，还可以减小加锁的范围。\n\n\n\n在出现IO瓶颈的时候，可以将sync_binglog设置为100-1000内的值，表示累计多少个事务之后才会刷盘，默认是1表示每一个事务都会刷盘写binlog；\n\n在出现IO瓶颈的时候，可以设置binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count，表示提升binlog的组提交的效果，但是会增加sql的响应时间\n\n写多读少的场景，由于 change buffer 的优化机制，因此如果业务可以接受，从性能角度出发我建议你优先考虑普通索引。\n\n\n\n\n\nmysql要调优，就要知道它为什么慢，哪里慢\n\n在低版本：show profiles\n\n在高版本：performance schema\n\n以上有什么用？\n\n- 比如在实际环境中，有一个sql，非常慢\n- sql本身没有问题，看执行计划也比较慢，这个时候就可以用到上面的\n- 可以更加详细的看到，这个sql到底是哪里慢\n\n\n\nprocess list 可以查看mysql的连接数量，顺丰云上的回话连接是不是？\n\n不过一般有druid，不用太关注这个。但是呢，druid是什么，要去看看github上面的官网了。了解一下它的优点\n\n\n\nMRR优化，将主键ID回表的随机读，改成主键ID的顺序读，但是默认是关闭的，优化器不倾向于使用MRR，不知道为啥；MRR 能够提升性能的核心在于，这条查询语句在索引 a 上做的是一个范围查询（也就是说，这是一个多值查询），可以得到足够多的主键 id。这样通过排序以后，再去主键索引查数据，才能体现出“顺序性”的优势。\n\n\n\n字符集和字符编码的区别\n\n\n\n大表拆分，将不常用的数据从表里拆出去，表小了之后，每个数据页存的数据就会多，查询的时候，就会减少磁盘IO\n\n\n\n优化：\n\n- 不同的数据选择对应的mysql自建数据类型，比如数字就用数字类型，不要用字符串，时间就用时间类型，不要用字符串；用错了数据类型，对业务可能没影响，但是对mysql来说，内部多了一层转换，sql执行会比较慢\n- 事件类型，建议用date，而不是timestamp，date类型\n\n\n\n大事务不仅会影响到主库，也是造成备库复制延迟的主要原因之一。因此，在平时的开发工作中，我建议你尽量减少大事务操作，把大事务拆成小事务。\n\n\n\nshow processlist\n\n- 看到的只是当前的并发连接数，如果你的系统动不动就成千上百个连接，其实都没关系，只是占用一些内存而已\n\n- ```sql\n  mysql> show processlist;\n  +----+-----------------+-----------------+------+---------+--------+------------------------+------------------+\n  | Id | User            | Host            | db   | Command | Time   | State                  | Info             |\n  +----+-----------------+-----------------+------+---------+--------+------------------------+------------------+\n  |  5 | event_scheduler | localhost       | NULL | Daemon  | 865562 | Waiting on empty queue | NULL             |\n  | 23 | root            | localhost:60179 | NULL | Query   |      0 | init                   | show processlist |\n  +----+-----------------+-----------------+------+---------+--------+------------------------+------------------+\n  2 rows in set (0.00 sec)\n  ```\n\n- 真正需要关注的是并发执行数，也就是并发执行的线程，因为连接存在，这个连接不一定在执行。\n\n- innodb的并发执行数，通过`innodb_thread_concurrency`这个参数来控制的，默认是0表示不限制。\n\n- `innodb_thread_concurrency`的计算规则是：正在执行的线程+1，如果一个线程在执行过程中，遇到了锁，需要等待，线程-1\n\n\n\n\n\n\n\n---\n\n\n\n# IN 查询优化\n\n\n\n这个名字还真不是我起的，今天要讲的知识点就叫**索引潜水（Index dive）**。\n\n先要从一件怪事说起：\n\n我先造点数据复现一下问题，创建一张用户表：\n\n```sql\nCREATE TABLE `user` (\n  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键ID',\n  `name` varchar(100) NOT NULL DEFAULT '' COMMENT '姓名',\n  `age` int(11) NOT NULL DEFAULT 0 COMMENT '年龄',\n  PRIMARY KEY (`id`),\n  KEY `idx_age` (`age`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n```\n\n通过一批用户年龄，查询该年龄的用户信息，并查看一下SQL执行计划：\n\n```sql\nexplain select * from user where age in (1,2,3,4,5,6,7,8,9);\n```\n\n![image-20230209164837331](mysql的性能调优从入门到入土.assets/image-20230209164837331.png)\n\nwhere条件中有9个参数，重点关注一下执行计划中的预估扫描行数为279行。\n\n到这里没什么问题，预估的非常准，实际就是279行。\n\n![image-20230209164853902](mysql的性能调优从入门到入土.assets/image-20230209164853902.png)\n\n但是，问题来了，当我们在where条件中，再加一个参数，变成了10个参数，预估扫描行数本应该增加，结果却大大减少了。\n\n```sql\nexplain select * from user \nwhere age in (1,2,3,4,5,6,7,8,9,10);\n```\n\n![image-20230209164903103](mysql的性能调优从入门到入土.assets/image-20230209164903103.png)\n\n一下子减少到了30行，可是实际行数是多少呢？\n\n![image-20230209164911034](mysql的性能调优从入门到入土.assets/image-20230209164911034.png)\n\n实际是310行，预估扫描行数是30行，真是错到姥姥家了。\n\n**MySQL咋回事啊，到底还能不能预估？**\n\n**不能预估的话，换其他人！**\n\n\n\n大家肯定也是满脸疑惑，直到我去官网上看到了一个词语，**索引潜水（Index dive）**。\n\n跟这个词语相关的，还有一个配置参数 **eq_range_index_dive_limit**。\n\n**MySQL5.7.3**之前的版本，这个值默认是10，之后的版本，这个值默认是200。\n\n可以使用命令查看一下这个值的大小：\n\n```sql\nshow variables like '%eq_range_index_dive_limit%';\n```\n\n![image-20230209164922082](mysql的性能调优从入门到入土.assets/image-20230209164922082.png)\n\n当然，我们也可以手动修改这个值的大小：\n\n```sql\nset eq_range_index_dive_limit=200;\n```\n\n这个 **eq_range_index_dive_limit** 配置的作用就是：\n\n当where语句in条件中参数个数小于这个值的时候，MySQL就采用**索引潜水（Index dive）**的方式预估扫描行数，非常准确。\n\n当where语句in条件中参数个数大于等于这个值的时候，MySQL就采用另一种方式**索引统计（Index statistics）**预估扫描行数，误差较大。\n\n**MySQL为什么要这么做呢？**\n\n都用**索引潜水（Index dive）**的方式预估扫描行数，不好吗？\n\n其实这是基于成本的考虑，**索引潜水**估算成本较高，适合小数据量。**索引统计**估算成本较低，适合大数据量。\n\n一般情况下，我们的where语句的in条件的参数不会太多，适合使用**索引潜水**预估扫描行数。\n\n建议还在使用**MySQL5.7.3**之前版本的同学们，手动修改一下**索引潜水**的配置参数，改成合适的数值。\n\n如果你们项目中in条件最多有500个参数，就把配置参数改成501。\n\n这样MySQL预估扫描行数更准确，可以选择更合适的索引。\n\n\n\n\n\n\n\n\n\n\n\n\n\n---\n\n\n\n## performance schema详解\n\n### performance_schema的介绍\n\nMySQL的performance schema 用于监控MySQL server在一个较低级别的运行过程中的资源消耗、资源等待等情况。\n\n特点如下：\n\n- 提供了一种在数据库运行时实时检查server的内部执行情况的方法\n\n  - performance_schema 数据库中的表使用PERFORMANCE_SCHEMA存储引擎。\n\n  - performance_schema 数据库主要关注数据库运行过程中的性能相关的数据\n\n  - information_schema 数据库主要关注server运行过程中的元数据信息\n\n- performance_schema 通过监视server的事件来实现监视server内部运行情况，\n\n  -  “事件”：就是server内部活动中所做的任何事情以及对应的时间消耗（比如函数调用、操作系统的等待、SQL语句执行的阶段）\n\n    - performance_schema中的事件与binlog中的记录的事件不同\n      - binlog中的事件是：描述数据修改的events\n      - performance_schema中的事件是：计划调度程序（这是一种存储程序）的事件\n\n\n    - performance_schema中的事件记录的是：server执行某些活动对某些资源的消耗、耗时、这些活动执行的次数等情况。\n    \n    - performance_schema中的事件只记录在本地server的performance_schema中，不会被写入binlog中，也不会被复制到其他server中。\n\n\n  - performance_schema的表中的数据不会持久化存储在磁盘中，而是保存在内存中，一旦服务器重启，这些数据会丢失\n\n### performance schema入门\n\n在mysql的5.7版本中，性能模式是默认开启的，如果想要显式的关闭的话需要修改配置文件，不能直接进行修改，会报错Variable 'performance_schema' is a read only variable。\n\n```sql\n-- 查看performance_schema的属性\nmysql> SHOW VARIABLES LIKE 'performance_schema';\n+--------------------+-------+\n| Variable_name      | Value |\n+--------------------+-------+\n| performance_schema | ON    |\n+--------------------+-------+\n1 row in set (0.01 sec)\n\n-- 在配置文件中修改performance_schema的属性值，on表示开启，off表示关闭\n[mysqld]\nperformance_schema=ON\n\n-- 切换数据库\nuse performance_schema;\n\n-- 查看当前数据库下的所有表,会看到有很多表存储着相关的信息\nshow tables;\n\n-- 可以通过show create table tablename来查看创建表的时候的表结构\nmysql> show create table setup_consumers;\n+-----------------+---------------------------------\n| Table           | Create Table                    \n+-----------------+---------------------------------\n| setup_consumers | CREATE TABLE `setup_consumers` (\n  `NAME` varchar(64) NOT NULL,                      \n  `ENABLED` enum('YES','NO') NOT NULL               \n) ENGINE=PERFORMANCE_SCHEMA DEFAULT CHARSET=utf8 |  \n+-----------------+---------------------------------\n1 row in set (0.00 sec)                             \n```\n\n​\t\n\n想要搞明白后续的内容，同学们需要理解两个基本概念：\n\n- instruments: 生产者，用于采集mysql中各种各样的操作产生的事件信息，对应配置表中的配置项我们可以称为监控采集配置项。\n- consumers: 消费者，对应的消费者表用于存储来自instruments采集的数据，对应配置表中的配置项我们可以称为消费存储配置项。\n\n### performance_schema表的分类\n\nperformance_schema库下的表可以按照监视纬度的不同进行分组。\n\n#### 语句事件记录表\n\n语句事件记录表，这些表记录了语句事件信息，当前语句事件表events_statements_current、历史语句事件表events_statements_history和长语句历史事件表events_statements_history_long、以及聚合后的摘要表summary，其中，summary表还可以根据帐号(account)，主机(host)，程序(program)，线程(thread)，用户(user)和全局(global)再进行细分)\n\n```sql\nshow tables like '%statement%';\n```\n\n#### 等待事件记录表\n\n等待事件记录表，与语句事件类型的相关记录表类似：\n\n```sql\nshow tables like '%wait%';\n```\n\n\n\n#### 阶段事件记录表\n\n阶段事件记录表，记录语句执行的阶段事件的表\n```sql\nshow tables like '%stage%';\n```\n\n\n\n#### 事务事件记录表\n\n事务事件记录表，记录事务相关的事件的表\n```sql\nshow tables like '%transaction%';\n```\n\n\n\n#### 监控文件系统层调用的表\n\n监控文件系统层调用的表\n```sql\nshow tables like '%file%';\n```\n\n\n\n#### 监视内存使用的表\n\n监视内存使用的表\n```sql\nshow tables like '%memory%';\n```\n\n\n\n#### 配置表\n\n动态对performance_schema进行配置的配置表\n```sql\nshow tables like '%setup%';\n```\n\n\n\n\n\n\n\n### performance_schema的简单配置与使用\n\n数据库刚刚初始化并启动时，并非所有instruments(事件采集项，在采集项的配置表中每一项都有一个开关字段，或为YES，或为NO)和consumers(与采集项类似，也有一个对应的事件类型保存表配置项，为YES就表示对应的表保存性能数据，为NO就表示对应的表不保存性能数据)都启用了，所以默认不会收集所有的事件，可能你需要检测的事件并没有打开，需要进行设置，可以使用如下两个语句打开对应的instruments和consumers（行计数可能会因MySQL版本而异)。\n\n#### 简单配置\n\n```sql\n-- 打开等待事件的采集器配置项开关，需要修改setup_instruments配置表中对应的采集器配置项\nUPDATE setup_instruments SET ENABLED = 'YES', TIMED = 'YES'where name like 'wait%';\n\n-- 打开等待事件的保存表配置开关，修改setup_consumers配置表中对应的配置项\nUPDATE setup_consumers SET ENABLED = 'YES'where name like '%wait%';\n\n```\n\n\n\n#### 查看当前server正在做什么\n\n当配置完成之后可以查看当前server正在做什么，可以通过查询events_waits_current表来得知，该表中每个线程只包含一行数据，用于显示每个线程的最新监视事件\n\n```sql\nselect * from events_waits_current\\G\n*************************** 1. row ***************************\n            THREAD_ID: 11\n             EVENT_ID: 570\n         END_EVENT_ID: 570\n           EVENT_NAME: wait/synch/mutex/innodb/buf_dblwr_mutex\n               SOURCE: \n          TIMER_START: 4508505105239280\n            TIMER_END: 4508505105270160\n           TIMER_WAIT: 30880\n                SPINS: NULL\n        OBJECT_SCHEMA: NULL\n          OBJECT_NAME: NULL\n           INDEX_NAME: NULL\n          OBJECT_TYPE: NULL\nOBJECT_INSTANCE_BEGIN: 67918392\n     NESTING_EVENT_ID: NULL\n   NESTING_EVENT_TYPE: NULL\n            OPERATION: lock\n      NUMBER_OF_BYTES: NULL\n                FLAGS: NULL\n```\n\n该信息表示线程id为11的线程正在等待buf_dblwr_mutex锁，等待事件为30880\n属性说明：\n\n- id:事件来自哪个线程，事件编号是多少\n- event_name:表示检测到的具体的内容\n- source:表示这个检测代码在哪个源文件中以及行号\n- timer_start:表示该事件的开始时间\n- timer_end:表示该事件的结束时间\n- timer_wait:表示该事件总的花费时间\n\n注意：_current表中每个线程只保留一条记录，一旦线程完成工作，该表中不会再记录该线程的事件信息\n\n\n\n\n\n#### 查看每个线程已经执行完成的事件信息\n\n_history表中记录每个线程应该执行完成的事件信息，但每个线程的事件信息只会记录10条，再多就会被覆盖，*_history_long表中记录所有线程的事件信息，但总记录数量是10000，超过就会被覆盖掉\n\n```sql\nselect thread_id,event_id,event_name,timer_wait from events_waits_history order by thread_id limit 21;\n```\n\n\n\n#### 查看事件的汇总信息\n\nsummary表提供所有事件的汇总信息，该组中的表以不同的方式汇总事件数据（如：按用户，按主机，按线程等等）。例如：要查看哪些instruments占用最多的时间，可以通过对events_waits_summary_global_by_event_name表的COUNT_STAR或SUM_TIMER_WAIT列进行查询（这两列是对事件的记录数执行COUNT（*）、事件记录的TIMER_WAIT列执行SUM（TIMER_WAIT）统计而来）\n\n```sql\nSELECT EVENT_NAME,COUNT_STAR FROM events_waits_summary_global_by_event_name  ORDER BY COUNT_STAR DESC LIMIT 10;\n```\n\n\n\n#### 查看被检测的对象\n\ninstance表记录了哪些类型的对象会被检测。这些对象在被server使用时，在该表中将会产生一条事件记录，例如，file_instances表列出了文件I/O操作及其关联文件名\n\n```sql\nselect * from file_instances limit 20; \n```\n\n\n\n### 常用配置项的参数说明\n\n#### 启动选项\n\n| 选项                                                         | 含义                                                         |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| performance_schema_consumer_events_statements_current=TRUE   | 是否在mysql server启动时就开启events_statements_current表的记录功能(该表记录当前的语句事件信息)，启动之后也可以在setup_consumers表中使用UPDATE语句进行动态更新setup_consumers配置表中的events_statements_current配置项，默认值为TRUE |\n| performance_schema_consumer_events_statements_history=TRUE   | 与performance_schema_consumer_events_statements_current选项类似，但该选项是用于配置是否记录语句事件短历史信息，默认为TRUE |\n| performance_schema_consumer_events_stages_history_long=FALSE | 与performance_schema_consumer_events_statements_current选项类似，但该选项是用于配置是否记录语句事件长历史信息，默认为FALSE |\n| *                                                            | 除了statement(语句)事件之外，还支持：wait(等待)事件、state(阶段)事件、transaction(事务)事件，他们与statement事件一样都有三个启动项分别进行配置，但这些等待事件默认未启用，如果需要在MySQL Server启动时一同启动，则通常需要写进my.cnf配置文件中 |\n| performance_schema_consumer_global_instrumentation=TRUE      | 是否在MySQL Server启动时就开启全局表（如：mutex_instances、rwlock_instances、cond_instances、file_instances、users、hostsaccounts、socket_summary_by_event_name、file_summary_by_instance等大部分的全局对象计数统计和事件汇总统计信息表 ）的记录功能，启动之后也可以在setup_consumers表中使用UPDATE语句进行动态更新全局配置项<br/>默认值为TRUE |\n| performance_schema_consumer_statements_digest=TRUE           | 是否在MySQL Server启动时就开启events_statements_summary_by_digest 表的记录功能，启动之后也可以在setup_consumers表中使用UPDATE语句进行动态更新digest配置项<br/>默认值为TRUE |\n| performance_schema_consumer_thread_instrumentation=TRUE      | 是否在MySQL Server启动时就开启events_xxx_summary_by_yyy_by_event_name表的记录功能，启动之后也可以在setup_consumers表中使用UPDATE语句进行动态更新线程配置项<br/>默认值为TRUE |\n| performance_schema_instrument[=name]                         | 是否在MySQL Server启动时就启用某些采集器，由于instruments配置项多达数千个，所以该配置项支持key-value模式，还支持%号进行通配等 |\n\n注意，这些启动选项要生效的前提是，需要设置performance_schema=ON。另外，这些启动选项虽然无法使用show variables语句查看，但我们可以通过setup_instruments和setup_consumers表查询这些选项指定的值。\n\n\n\n#### 系统变量\n\n```sql\nshow variables like '%performance_schema%';\n```\n\n重要的属性解释\n\n| 属性                                                         | 解释                                                         |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| performance_schema=ON                                        | 控制performance_schema功能的开关，要使用MySQL的performance_schema，需要在mysqld启动时启用，以启用事件收集功能<br/>该参数在5.7.x之前支持performance_schema的版本中默认关闭，5.7.x版本开始默认开启<br/>注意：如果mysqld在初始化performance_schema时发现无法分配任何相关的内部缓冲区，则performance_schema将自动禁用，并将performance_schema设置为OFF |\n| performance_schema_digests_size=10000                        | 控制events_statements_summary_by_digest表中的最大行数。如果产生的语句摘要信息超过此最大值，便无法继续存入该表，此时performance_schema会增加状态变量 |\n| performance_schema_events_statements_history_long_size=10000 | 控制events_statements_history_long表中的最大行数，该参数控制所有会话在events_statements_history_long表中能够存放的总事件记录数，超过这个限制之后，最早的记录将被覆盖 |\n| performance_schema_events_statements_history_size=10         | 控制events_statements_history表中单个线程（会话）的最大行数，该参数控制单个会话在events_statements_history表中能够存放的事件记录数，超过这个限制之后，单个会话最早的记录将被覆盖 |\n| performance_schema_max_digest_length=1024                    | 用于控制标准化形式的SQL语句文本在存入performance_schema时的限制长度，该变量与max_digest_length变量相关(max_digest_length变量含义请自行查阅相关资料)<br/>全局变量，只读变量，默认值1024字节，整型值，取值范围0~1048576 |\n| performance_schema_max_sql_text_length=1024                  | 控制存入events_statements_current，events_statements_history和events_statements_history_long语句事件表中的SQL_TEXT列的最大SQL长度字节数。 超出系统变量performance_schema_max_sql_text_length的部分将被丢弃，不会记录，一般情况下不需要调整该参数，除非被截断的部分与其他SQL比起来有很大差异<br/>全局变量，只读变量，整型值，默认值为1024字节，取值范围为0~1048576，5.7.6版本引入<br/>降低系统变量performance_schema_max_sql_text_length值可以减少内存使用，但如果汇总的SQL中，被截断部分有较大差异，会导致没有办法再对这些有较大差异的SQL进行区分。 增加该系统变量值会增加内存使用，但对于汇总SQL来讲可以更精准地区分不同的部分。 |\n\n\n\n### 重要配置表的相关说明\n\n配置表之间存在相互关联关系\n\n#### performance_timers表\n\nperformance_timers表中记录了server中有哪些可用的事件计时器\n\n```sql\nselect * from performance_timers;\n```\n\n| 字段             | 含义                                                         |\n| ---------------- | ------------------------------------------------------------ |\n| timer_name       | 表示可用计时器名称，CYCLE是基于CPU周期计数器的定时器         |\n| timer_frequency  | 表示每秒钟对应的计时器单位的数量,CYCLE计时器的换算值与CPU的频率相关 |\n| timer_resolution | 计时器精度值，表示在每个计时器被调用时额外增加的值           |\n| timer_overhead   | 表示在使用定时器获取事件时开销的最小周期值                   |\n\n\n\n#### setup_timers表\n\nsetup_timers表中记录当前使用的事件计时器信息\n\n```sql\nselect * from setup_timers;\n```\n\n| 字段       | 含义                         |\n| ---------- | ---------------------------- |\n| name       | 计时器类型，对应某个事件类别 |\n| timer_name | 计时器类型名称               |\n\n\n\n\n\n#### setup_consumers表\n\nsetup_consumers表中列出了consumers可配置列表项\n\n```sql\nselect * from setup_consumers;\n```\n\n| 字段    | 含义                                                         |\n| ------- | ------------------------------------------------------------ |\n| NAME    | consumers配置名称                                            |\n| ENABLED | consumers是否启用，有效值为YES或NO，此列可以使用UPDATE语句修改。 |\n\n\n\n#### setup_instruments 表\n\nsetup_instruments 表列出了instruments 列表配置项，即代表了哪些事件支持被收集：\n\n```sql\nSELECT * FROM setup_instruments;\n```\n\n| 字段    | 含义                                                         |\n| ------- | ------------------------------------------------------------ |\n| NAME    | instruments名称，instruments名称可能具有多个部分并形成层次结构 |\n| ENABLED | instrumetns是否启用，有效值为YES或NO，此列可以使用UPDATE语句修改。如果设置为NO，则这个instruments不会被执行，不会产生任何的事件信息 |\n| TIMED   | instruments是否收集时间信息，有效值为YES或NO，此列可以使用UPDATE语句修改，如果设置为NO，则这个instruments不会收集时间信息 |\n\n\n\n\n\n#### setup_actors表\n\nsetup_actors表的初始内容是匹配任何用户和主机，因此对于所有前台线程，默认情况下启用监视和历史事件收集功能\n\n```sql\nSELECT * FROM setup_actors;\n```\n\n| 字段    | 含义                                                         |\n| ------- | ------------------------------------------------------------ |\n| HOST    | 与grant语句类似的主机名，一个具体的字符串名字，或使用“％”表示“任何主机” |\n| USER    | 一个具体的字符串名称，或使用“％”表示“任何用户”               |\n| ROLE    | 当前未使用，MySQL 8.0中才启用角色功能                        |\n| ENABLED | 是否启用与HOST，USER，ROLE匹配的前台线程的监控功能，有效值为：YES或NO |\n| HISTORY | 是否启用与HOST， USER，ROLE匹配的前台线程的历史事件记录功能，有效值为：YES或NO |\n\n\n\n#### setup_objects表\n\nsetup_objects表控制performance_schema是否监视特定对象。默认情况下，此表的最大行数为100行。\n\n```sql\nSELECT * FROM setup_objects;\n```\n\n| 字段          | 含义                                                         |\n| ------------- | ------------------------------------------------------------ |\n| OBJECT_TYPE   | instruments类型，有效值为：“EVENT”（事件调度器事件）、“FUNCTION”（存储函数）、“PROCEDURE”（存储过程）、“TABLE”（基表）、“TRIGGER”（触发器），TABLE对象类型的配置会影响表I/O事件（wait/io/table/sql/handler instrument）和表锁事件（wait/lock/table/sql/handler instrument）的收集 |\n| OBJECT_SCHEMA | 某个监视类型对象涵盖的数据库名称，一个字符串名称，或“％”(表示“任何数据库”) |\n| OBJECT_NAME   | 某个监视类型对象涵盖的表名，一个字符串名称，或“％”(表示“任何数据库内的对象”) |\n| ENABLED       | 是否开启对某个类型对象的监视功能，有效值为：YES或NO。此列可以修改 |\n| TIMED         |                                                              |\n\n\n\n#### threads表\n\nthreads表对于每个server线程生成一行包含线程相关的信息，\n\n```sql\nselect * from threads\n```\n\n| 字段                | 含义                                                         |\n| ------------------- | ------------------------------------------------------------ |\n| THREAD_ID           | 线程的唯一标识符（ID）                                       |\n| NAME                | 与server中的线程检测代码相关联的名称(注意，这里不是instruments名称) |\n| TYPE                | 线程类型，有效值为：FOREGROUND、BACKGROUND。分别表示前台线程和后台线程 |\n| PROCESSLIST_ID      | 对应INFORMATION_SCHEMA.PROCESSLIST表中的ID列                 |\n| PROCESSLIST_USER    | 与前台线程相关联的用户名，对于后台线程为NULL                 |\n| PROCESSLIST_HOST    | 与前台线程关联的客户端的主机名，对于后台线程为NULL           |\n| PROCESSLIST_DB      | 线程的默认数据库，如果没有，则为NULL                         |\n| PROCESSLIST_COMMAND | 对于前台线程，该值代表着当前客户端正在执行的command类型，如果是sleep则表示当前会话处于空闲状态 |\n| PROCESSLIST_TIME    | 当前线程已处于当前线程状态的持续时间（秒）                   |\n| PROCESSLIST_STATE   | 表示线程正在做什么事情                                       |\n| PROCESSLIST_INFO    | 线程正在执行的语句，如果没有执行任何语句，则为NULL           |\n| PARENT_THREAD_ID    | 如果这个线程是一个子线程（由另一个线程生成），那么该字段显示其父线程ID |\n| ROLE                | 暂未使用                                                     |\n| INSTRUMENTED        | 线程执行的事件是否被检测。有效值：YES、NO                    |\n| HISTORY             | 是否记录线程的历史事件。有效值：YES、NO                      |\n| THREAD_OS_ID        | 由操作系统层定义的线程或任务标识符（ID）                     |\n\n\n\n注意：在performance_schema库中还包含了很多其他的库和表，能对数据库的性能做完整的监控，大家需要参考官网详细了解。\n\n### performance_schema实践操作\n\n基本了解了表的相关信息之后，可以通过这些表进行实际的查询操作来进行实际的分析。\n\n#### 哪类的SQL执行最多？\n\n```sql\nSELECT DIGEST_TEXT,COUNT_STAR,FIRST_SEEN,LAST_SEEN FROM events_statements_summary_by_digest ORDER BY COUNT_STAR DESC\n```\n\n#### 哪类SQL的平均响应时间最多？\n\n```sql\nSELECT DIGEST_TEXT,AVG_TIMER_WAIT FROM events_statements_summary_by_digest ORDER BY COUNT_STAR DESC\n```\n\n\n\n#### 哪类SQL排序记录数最多？\n\n```sql\nSELECT DIGEST_TEXT,SUM_SORT_ROWS FROM events_statements_summary_by_digest ORDER BY COUNT_STAR DESC\n```\n\n\n\n#### 哪类SQL扫描记录数最多？\n\n```sql\nSELECT DIGEST_TEXT,SUM_ROWS_EXAMINED FROM events_statements_summary_by_digest ORDER BY COUNT_STAR DESC\n```\n\n\n\n#### 哪类SQL使用临时表最多？\n\n```sql\nSELECT DIGEST_TEXT,SUM_CREATED_TMP_TABLES,SUM_CREATED_TMP_DISK_TABLES FROM events_statements_summary_by_digest ORDER BY COUNT_STAR DESC\n```\n\n\n\n#### 哪类SQL返回结果集最多？\n\n```sql\nSELECT DIGEST_TEXT,SUM_ROWS_SENT FROM events_statements_summary_by_digest ORDER BY COUNT_STAR DESC\n```\n\n\n\n#### 哪个表物理IO最多？\n\n```sql\nSELECT file_name,event_name,SUM_NUMBER_OF_BYTES_READ,SUM_NUMBER_OF_BYTES_WRITE FROM file_summary_by_instance ORDER BY SUM_NUMBER_OF_BYTES_READ + SUM_NUMBER_OF_BYTES_WRITE DESC\n```\n\n\n\n#### 哪个表逻辑IO最多？\n\n```sql\nSELECT object_name,COUNT_READ,COUNT_WRITE,COUNT_FETCH,SUM_TIMER_WAIT FROM table_io_waits_summary_by_table ORDER BY sum_timer_wait DESC\n```\n\n\n\n##### 哪个索引访问最多？\n\n```sql\nSELECT OBJECT_NAME,INDEX_NAME,COUNT_FETCH,COUNT_INSERT,COUNT_UPDATE,COUNT_DELETE FROM table_io_waits_summary_by_index_usage ORDER BY SUM_TIMER_WAIT DESC\n```\n\n\n\n##### 哪个索引从来没有用过？\n\n```sql\nSELECT OBJECT_SCHEMA,OBJECT_NAME,INDEX_NAME FROM table_io_waits_summary_by_index_usage WHERE INDEX_NAME IS NOT NULL AND COUNT_STAR = 0 AND OBJECT_SCHEMA <> 'mysql' ORDER BY OBJECT_SCHEMA,OBJECT_NAME;\n```\n\n\n\n##### 哪个等待事件消耗时间最多？\n\n```sql\nSELECT EVENT_NAME,COUNT_STAR,SUM_TIMER_WAIT,AVG_TIMER_WAIT FROM events_waits_summary_global_by_event_name WHERE event_name != 'idle' ORDER BY SUM_TIMER_WAIT DESC\n```\n\n\n\n#### 剖析某条SQL的执行情况，包括statement信息，stege信息，wait信息\n\n```sql\nSELECT EVENT_ID,sql_text FROM events_statements_history WHERE sql_text LIKE '%count(*)%';\n```\n\n\n\n#### 查看每个阶段的时间消耗\n\n```sql\nSELECT event_id,EVENT_NAME,SOURCE,TIMER_END - TIMER_START FROM events_stages_history_long WHERE NESTING_EVENT_ID = 1553;\n```\n\n\n\n#### 查看每个阶段的锁等待情况\n\n```sql\nSELECT event_id,event_name,source,timer_wait,object_name,index_name,operation,nesting_event_id FROM events_waits_history_long WHERE nesting_event_id = 1553;\n```\n\n\n\n\n\n## 看懂mysql执行计划\n\n​       在企业的应用场景中，为了知道优化SQL语句的执行，需要查看SQL语句的具体执行过程，以加快SQL语句的执行效率。\n\n​       可以使用explain+SQL语句来模拟优化器执行SQL查询语句，从而知道mysql是如何处理sql语句的。\n\n​\t   官网地址： https://dev.mysql.com/doc/refman/5.5/en/explain-output.html \n\n### 准备数据\n\n```sql\n/*\nNavicat MySQL Data Transfer\n\nSource Server         : mybatis\nSource Server Version : 50722\nSource Host           : localhost:3306\nSource Database       : demp\n\nTarget Server Type    : MYSQL\nTarget Server Version : 50722\nFile Encoding         : 65001\n\nDate: 2020-02-11 20:05:02\n*/\n\nSET FOREIGN_KEY_CHECKS=0;\n\n-- ----------------------------\n-- Table structure for dept\n-- ----------------------------\nDROP TABLE IF EXISTS `dept`;\nCREATE TABLE `dept` (\n  `DEPTNO` int(4) NOT NULL,\n  `DNAME` varchar(14) DEFAULT NULL,\n  `LOC` varchar(13) DEFAULT NULL,\n  PRIMARY KEY (`DEPTNO`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n-- ----------------------------\n-- Records of dept\n-- ----------------------------\nINSERT INTO `dept` VALUES ('10', 'ACCOUNTING', 'NEW YORK');\nINSERT INTO `dept` VALUES ('20', 'RESEARCH', 'DALLAS');\nINSERT INTO `dept` VALUES ('30', 'SALES', 'CHICAGO');\nINSERT INTO `dept` VALUES ('40', 'OPERATIONS', 'BOSTON');\n\n-- ----------------------------\n-- Table structure for emp\n-- ----------------------------\nDROP TABLE IF EXISTS `emp`;\nCREATE TABLE `emp` (\n  `EMPNO` int(4) NOT NULL,\n  `ENAME` varchar(10) DEFAULT NULL,\n  `JOB` varchar(9) DEFAULT NULL,\n  `MGR` int(4) DEFAULT NULL,\n  `HIREDATE` date DEFAULT NULL,\n  `SAL` double(7,2) DEFAULT NULL,\n  `COMM` double(7,2) DEFAULT NULL,\n  `DEPTNO` int(4) DEFAULT NULL,\n  PRIMARY KEY (`EMPNO`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n-- ----------------------------\n-- Records of emp\n-- ----------------------------\nINSERT INTO `emp` VALUES ('7369', 'SMITH', 'CLERK', '7902', '1980-12-17', '800.00', null, '20');\nINSERT INTO `emp` VALUES ('7499', 'ALLEN', 'SALESMAN', '7698', '1981-02-20', '1600.00', '300.00', '30');\nINSERT INTO `emp` VALUES ('7521', 'WARD', 'SALESMAN', '7698', '1981-02-22', '1250.00', '500.00', '30');\nINSERT INTO `emp` VALUES ('7566', 'JONES', 'MANAGER', '7839', '1981-02-02', '2975.00', null, '20');\nINSERT INTO `emp` VALUES ('7654', 'MARTIN', 'SALESMAN', '7698', '1981-09-28', '1250.00', '1400.00', '30');\nINSERT INTO `emp` VALUES ('7698', 'BLAKE', 'MANAGER', '7839', '1981-01-05', '2850.00', null, '30');\nINSERT INTO `emp` VALUES ('7782', 'CLARK', 'MANAGER', '7839', '1981-09-06', '2450.00', null, '10');\nINSERT INTO `emp` VALUES ('7839', 'KING', 'PRESIDENT', null, '1981-11-17', '5000.00', null, '10');\nINSERT INTO `emp` VALUES ('7844', 'TURNER', 'SALESMAN', '7698', '1981-09-08', '1500.00', '0.00', '30');\nINSERT INTO `emp` VALUES ('7900', 'JAMES', 'CLERK', '7698', '1981-12-03', '950.00', null, '30');\nINSERT INTO `emp` VALUES ('7902', 'FORD', 'ANALYST', '7566', '1981-12-03', '3000.00', null, '20');\nINSERT INTO `emp` VALUES ('7934', 'MILLER', 'CLERK', '7782', '1982-01-23', '1300.00', null, '10');\n\n-- ----------------------------\n-- Table structure for salgrade\n-- ----------------------------\nDROP TABLE IF EXISTS `salgrade`;\nCREATE TABLE `salgrade` (\n  `GRADE` int(11) NOT NULL,\n  `LOSAL` double DEFAULT NULL,\n  `HISAL` double DEFAULT NULL,\n  PRIMARY KEY (`GRADE`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n-- ----------------------------\n-- Records of salgrade\n-- ----------------------------\nINSERT INTO `salgrade` VALUES ('1', '700', '1200');\nINSERT INTO `salgrade` VALUES ('2', '1201', '1400');\nINSERT INTO `salgrade` VALUES ('3', '1401', '2000');\nINSERT INTO `salgrade` VALUES ('4', '2001', '3000');\nINSERT INTO `salgrade` VALUES ('5', '3001', '9999');\n```\n\n\n\n### 执行计划中包含的信息\n\n|      列       | 含义                                                         |\n| :-----------: | ------------------------------------------------------------ |\n|      id       | select查询的序列号                                           |\n|  select_type  | 查询的类型，是普通查询还是联合查询还是子查询                 |\n|     table     | 对应行正在访问哪一个表，表名或者别名，可能是临时表或者union合并结果集 |\n|  partitions   | 表示当前查询访问的分区，如果是NULL，表示当前不是分区表       |\n|     type      | type显示的是访问类型，访问类型表示我是以何种方式去访问我们的数据，最容易想的是全表扫描 |\n| possible_keys | 显示当前查询语句可能会被使用的索引，一个或多个，但不一定被查询实际使用 |\n|      key      | 实际使用的索引，如果为null，则没有使用索引                   |\n|    key_len    | 表示索引中使用的字节数，可以通过key_len计算查询中使用的索引长度，在不损失精度的情况下长度越短越好 |\n|      ref      | 显示索引的哪一列被使用了，如果可能的话，是一个常数           |\n|     rows      | 根据表的统计信息及索引使用情况，大致估算出找出所需记录需要读取的行数 |\n|   filtered    | 某个表经过搜索条件过滤后剩余记录条数的百分比，rows 乘以 filtered 的结果表示要与下表连接的行数 |\n|     extra     | 额外的信息                                                   |\n\n#### **id**\n\nselect查询的序列号，包含一组数字，表示查询中执行select子句或者操作表的顺序\n\nid号分为三种情况：\n\n- 如果id相同，那么执行顺序从上到下\n- 如果id不同，如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行\n- 同时存在id相同和不同的：相同的可以认为是一组，从上往下顺序执行，在所有组中，id值越大，优先级越高，越先执行\n\n#### **select_type**\n\n主要用来分辨查询的类型，是普通查询还是联合查询还是子查询\n\n- `sample`：简单的查询，不包含子查询和union\n- `primary`：查询中若包含任何复杂的子查询，最外层查询则被标记为Primary\n- `union`：若第二个select出现在union之后，则被标记为union\n- `dependent union` ： 跟union类似，此处的depentent表示union或union all联合而成的结果会受外部表影响\n- `union result`：从union表获取结果的select\n- `subquery`：在select或者where列表中包含子查询\n- `dependent subquery`：subquery的子查询要受到外部表查询的影响\n- `derived`：from子句中出现的子查询\n- `uncacheable subquery`：表示使用子查询的结果不能被缓存\n- `uncacheable union`：表示union的查询结果不能被缓存：sql语句未验证\n\n```sql\n-- sample:简单的查询，不包含子查询和union\nexplain select * from emp;\n\n-- primary:查询中若包含任何复杂的子查询，最外层查询则被标记为Primary\nexplain select staname,ename supname from (select ename staname,mgr from emp) t join emp on t.mgr=emp.empno ;\n\n-- union:若第二个select出现在union之后，则被标记为union\nexplain select * from emp where deptno = 10 union select * from emp where sal >2000;\n\n-- dependent union : 跟union类似，此处的depentent表示union或union all联合而成的结果会受外部表影响\nexplain select * from emp e where e.empno  in ( select empno from emp where deptno = 10 union select empno from emp where sal >2000)\n\n-- union result:从union表获取结果的select\nexplain select * from emp where deptno = 10 union select * from emp where sal >2000;\n\n-- subquery:在select或者where列表中包含子查询\nexplain select * from emp where sal > (select avg(sal) from emp) ;\n\n-- dependent subquery:subquery的子查询要受到外部表查询的影响\nexplain select * from emp e where e.deptno in (select distinct deptno from dept);\n\n-- DERIVED: from子句中出现的子查询，也叫做派生类，\nexplain select staname,ename supname from (select ename staname,mgr from emp) t join emp on t.mgr=emp.empno ;\n\n-- UNCACHEABLE SUBQUERY：表示使用子查询的结果不能被缓存\n explain select * from emp where empno = (select empno from emp where deptno=@@sort_buffer_size);\n \n-- uncacheable union:表示union的查询结果不能被缓存：sql语句未验证\n```\n\n#### **table**\n\n对应行正在访问哪一个表，表名或者别名，可能是临时表或者union合并结果集\n\n- 如果是具体的表名，则表明从实际的物理表中获取数据，当然也可以是表的别名\n- 表名是derivedN的形式，表示使用了id为N的查询产生的衍生表\n- 当有union result的时候，表名是union n1,n2等的形式，n1,n2表示参与union的id\n\n#### **partitions**\n\n表示当前查询访问的分区，如果是NULL，表示当前不是分区表\n\n#### **type**\n\ntype显示的是访问类型，访问类型表示我是以何种方式去访问我们的数据，最容易想的是全表扫描，直接暴力的遍历一张表去寻找需要的数据，效率非常低下，访问的类型有很多，效率从最好到最坏依次是：\n\n- `system`：表只有一行记录（等于系统表），这是const类型的特例，平时不会出现\n- `const`：这个表至多有一个匹配行，并不是说表里只有一条记录，一般使用主键id查询的时候是这种情况\n- `eq_ref`：使用唯一性索引进行数据查找\n- `ref`：使用了非唯一性索引进行数据的查找\n- `fulltext`\n- `ref_or_null`：对于某个字段即需要关联条件，也需要null值的情况下，查询优化器会选择这种访问方式\n- `index_merge`：在查询过程中需要多个索引组合使用\n- `unique_subquery`：该连接类型类似与index_subquery,使用的是唯一索引\n- `index_subquery`：利用索引来关联子查询，不再扫描全表\n- `range`：表示利用索引查询的时候限制了范围，在指定范围内进行查询，这样避免了index的全索引扫描\n- `index`：全索引扫描这个比all的效率要好，主要有两种情况，一种是当前的查询是覆盖索引，或者是使用了索引进行排序，这样就避免数据的重排序\n- `all`：全表扫描，一般情况下出现这样的sql语句而且数据量比较大的话那么就需要进行优化。\n\n一般情况下，得保证查询至少达到range级别，最好能达到ref\n\n```sql\n-- all:全表扫描，一般情况下出现这样的sql语句而且数据量比较大的话那么就需要进行优化。\nexplain select * from emp;\n\n-- index：全索引扫描这个比all的效率要好，主要有两种情况，一种是当前的查询是覆盖索引，即我们需要的数据在索引中就可以索取，或者是使用了索引进行排序，这样就避免数据的重排序\nexplain  select empno from emp;\n\n-- range：表示利用索引查询的时候限制了范围，在指定范围内进行查询，这样避免了index的全索引扫描，适用的操作符： =, <>, >, >=, <, <=, IS NULL, BETWEEN, LIKE, or IN() \nexplain select * from emp where empno between 7000 and 7500;\n\n-- index_subquery：利用索引来关联子查询，不再扫描全表\nexplain select * from emp where emp.job in (select job from t_job);\n\n-- unique_subquery:该连接类型类似与index_subquery,使用的是唯一索引\n explain select * from emp e where e.deptno in (select distinct deptno from dept);\n \n-- index_merge：在查询过程中需要多个索引组合使用，没有模拟出来\n\n-- ref_or_null：对于某个字段即需要关联条件，也需要null值的情况下，查询优化器会选择这种访问方式\nexplain select * from emp where mgr is null or mgr=7369;\n\n-- ref：使用了非唯一性索引进行数据的查找\n create index idx_3 on emp(deptno);\n explain select * from emp e,dept d where e.deptno =d.deptno;\n\n-- eq_ref ：使用唯一性索引进行数据查找\nexplain select * from emp,emp2 where emp.empno = emp2.empno;\n\n-- const：这个表至多有一个匹配行，并不是说表里只有一条记录，一般使用主键id查询的时候会有这个type\nexplain select * from emp where empno = 7369;\n \n-- system：表只有一行记录（等于系统表），这是const类型的特例，平时不会出现\n```\n\n####  **possible_keys** \n\n显示当前查询语句可能会被使用的索引，一个或多个，但不一定被查询实际使用\n\n```sql\nexplain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10;\n```\n\n#### **key**\n\n实际使用的索引，如果为null，则没有使用索引，查询中若使用了覆盖索引，则该索引和查询的select字段重叠。\n\n```sql\nexplain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10;\n```\n\n#### **key_len**\n\n表示索引中使用的字节数，可以通过key_len计算查询中使用的索引长度，在不损失精度的情况下长度越短越好。\n\n```sql\nexplain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10;\n```\n\n#### **ref**\n\n显示索引的哪一列被使用了，如果可能的话，是一个常数\n\n```sql\nexplain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10;\n```\n\n#### **rows**\n\n根据表的统计信息及索引使用情况，大致估算出找出所需记录需要读取的行数，此参数很重要，直接反应的sql找了多少数据，在完成目的的情况下越少越好\n\n```sql\nexplain select * from emp;\n```\n\n#### **filtered**\n\n某个表经过搜索条件`过滤后剩余记录条数`的百分比，这个`过滤后剩余记录条数`，是需要用来对下一张表进行连接的行数。什么意思呢？\n\n- 对于单表查询来说，这个filtered列的值没什么意义。\n- 对于多表联合查询，假设表1扫描的rows是10000行，filteres是20%，那么表示要与表2进行连接的行数是：10000 x 20% = 2000条记录。\n\n\n\n#### **extra**\n\n包含额外的信息。\n\n```sql\n-- using filesort :说明mysql无法利用索引进行排序，只能利用排序算法进行排序，会消耗额外的位置\nexplain select * from emp order by sal;\n\n-- using temporary:需要使用临时表来保存中间结果，查询完成之后把临时表删除\nexplain select ename,count(*) from emp where deptno = 10 group by ename;\n\n-- using index: 这个表示当前的查询时覆盖索引的，直接从索引中读取数据，而不用访问数据表。如果同时出现using where 表名索引被用来执行索引键值的查找，如果没有，表面索引被用来读取数据，而不是真的查找\nexplain select deptno,count(*) from emp group by deptno limit 10;\n\n-- using where:使用where进行条件过滤\nexplain select * from t_user where id = 1;\n\n-- using join buffer :使用连接缓存，情况没有模拟出来\n\n-- impossible where：where语句的结果总是false\nexplain select * from emp where empno = 7469;\n\n-- using MRR：使用了Multi-Range Read 优化 (MRR)\n\n```\n\n\n\n\n\n","tags":["mysql","性能调优","调优"],"categories":["JAVA","数据库","MYSQL"]},{"title":"通过docker安装mysql5.7.19并解决中文乱码问题的方法","url":"/note/JAVA/部署与容器/DOCKER/docker安装mysql5719并解决中文乱码/","content":"\n\n\n\n\n# 通过docker安装mysql5.7.19并解决中文乱码问题的方法\n\n\n\n## 安装Docker\n\n首先安装docker；\n\n## 安装mysql5.7.19\n\n从dokcer仓库下载5.7.19版本的数据，如果不指定版本的话，下载的就是最新的版本，为什么选择5.7.19，因为公司使用的数据库是这个版本，所以就选择这个了。\n\n```shell\ndocker pull mysql:5.7.19\n```\n\n下载好之后，如果有docker desktop的话，就可以看到了，没有desktop也可以使用命令查询\n\n```shell\nzhuansun@MacBook-Pro ~ % docker image list\nREPOSITORY  TAG    IMAGE ID    CREATED    SIZE\nmysql     5.7.19   3e3878acd190  3 years ago  412MB\n```\n\n或者docker desktop\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901193908153.png\" alt=\"image-20220901193908153\" style=\"zoom: 50%;\" />\n\n## 启动mysql5.7.19\n\n然后就是启动docker容器了：\n\n同样的，有两种方式：一种是通过图形化工具docker-desktop；一种是通过命令；\n\n\n\n### 命令行简单启动\n\n```shell\ndocker run -p 3306:3306 --name mysql5.7.19 \\\n-e MYSQL_ROOT_PASSWORD=123456 \\\n-d mysql:5.7.19\n```\n\n解释一下：\n\n| 命令                          | 说明                                                         |\n| ----------------------------- | ------------------------------------------------------------ |\n| docker run                    | 没什么说的                                                   |\n| -p 3306:3306                  | 指定mysql的端口，前面是本机端口，后面是容器端口；如果容器已经起了一个3306的mysql，你想在启动一个mysql，就得把后面的端口改了。 |\n| --name mysql5.7.19            | 容器启动后的名字，只是个名字                                 |\n| -e MYSQL_ROOT_PASSWORD=123456 | docker启动mysql必须要设置密码的，这里是直接设置了root密码，也可以通过其他命令指定新的用户名和密码。网上搜就有 |\n| -d mysql:5.7.19               | 使用mysql:5.7.19这个版本的镜像                               |\n\n\n\n\n\n### docker-desktop简单启动\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901195130908.png\" alt=\"image-20220901195130908\" style=\"zoom: 50%;\" />\n\n- 和命令行方式比较一下，就知道啥意思了。但是docker-desktop没找到哪里设置用户名和密码。所以我最终选择是使用命令行方式；\n\n- 但是我们注意到dokcer-desktop下面有一个Volume卷的设置，命令行没有，它是什么意思呢？\n\n- 它表示可以把容器中的数据和本地的数据进行互通；\n\n- 我们知道容器一旦被关闭，或者重启，所有的数据都会丢失；所以我们要把容器中的数据映射到本地磁盘，就是这个意思。\n\n- 不过，上面说了，我们使用命令行的方式启动mysql，那么命令行怎么映射呢？\n\n### 命令行完整启动\n\n```shell\ndocker run -p 3306:3306 --cpus 2 -m 4GB --name mysql5.7.19 \\\n-v /Users/zhuansun/workspace/docker/mysql5.7.19/conf:/etc/mysql \\\n-v /Users/zhuansun/workspace/docker/mysql5.7.19/logs:/var/log/mysql \\\n-v /Users/zhuansun/workspace/docker/mysql5.7.19/data:/var/lib/mysql \\\n-e MYSQL_ROOT_PASSWORD=123456 \\\n-d mysql:5.7.19\n```\n\n解释一下：\n\n| 命令   | 说明                                                         |\n| ------ | ------------------------------------------------------------ |\n| -v     | -v表示将本地的一个文件夹挂载到容器中，容器每次启动的时候，本地的文件都会覆盖到容器中；容器中产生的文件，也会写到本地中。这样保证了数据的持久化 |\n| --cpus | 配置cpu，有时候mysql跑的太快，cpu直接就满了，这里可以配置cpu，后面跟的数据，是cpu的核数 |\n| -m     | 配置内存，默认内存是2G，可以通过 docker stats 查看           |\n\n\n\n- 然后在docker-desktop中也可以看到启动后的app\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901195155480.png\" alt=\"image-20220901195155480\" style=\"zoom:50%;\" />\n\n\n\n\n\n## datagrip连接mysql\n\n```shell\nlocalhost\n3306\nroot\n123456\n```\n\n- datagrip也可以连接成功\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901195211507.png\" alt=\"image-20220901195211507\" style=\"zoom:50%;\" />\n\n\n\n\n\n## 中文乱码问题的发现\n\n- 我以为到这儿就结束了，其实不是的。\n\n- 因为我在mysql中创建了一个数据库，然后创建了一个表，通过代码插入一条记录之后，发现，妈的，乱码了。\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901195229189.png\" alt=\"image-20220901195229189\" style=\"zoom:50%;\" />\n\n- 然后排查为什么会乱码：\n\n- 这是因为我们的mysql通过docker启动之后，默认的编码其实并不是utf8，而是latin1；这就导致了中文乱码的原因。\n\n- 通过 \n\n```mysql\nshow variables like '%char%';\n```\n\n也可以验证这个问题\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901195253309.png\" alt=\"image-20220901195253309\" style=\"zoom:50%;\" />\n\n## 中文乱码问题的排查思路\n\n- 那么怎么办呢？就需要修改mysql的配置文件了。\n\n- 这里必须要吐槽一下百度，搜出来的东西，他娘的，都是狗屎；\n\n- 搜索关键字：mac mysql 5.7.19 中文乱码\n\n- 然后百度建议：修改etc下的my.cnf；他娘的，根本没有这个文件好不好。\n\n- 最后还是搜索了好久好久，一直折腾到大半夜，才找到原因\n\n- 在mysql5.7.18版本之前，在etc下会有一个my-default.cnf的文件（网上搜的，没有验证），然后我们安装好mysql之后，把这个文件改个名字就可以了。作为全局配置，但是后续版本，mysql把他删掉了。所以我使用的5.7.19是没有的。\n\n- 那么5.7.19的配置文件在哪里呢？\n\n- 应该是在etc/mysql/这个文件夹下面，但是我登录我的容器一看：这个文件夹下面是空的？卧槽？\n\n- 后续排查发现，因为我们使用了 \n\n```sh\n-v /Users/zhuansun/workspace/docker/mysql5.7.19/conf:/etc/mysql \\\n```\n\n- 所以我们本地的文件覆盖了容器中的内容；本地的文件夹是空的，所以容器里面技术空了。\n\n- 那么没办法了，我又重新启动了一个mysql-dokcer叫做 mysql5.7.19-1，然后没有指定-v,同时还得修改端口，要不然端口就冲突了；\n\n```sh\ndocker run -p 3307:3307 --name mysql5.7.19-1 \\\n-e MYSQL_ROOT_PASSWORD=123456 \\\n-d mysql:5.7.19\n```\n\n- 这样我们创建了一个新的mysql容器，登录进去看看：\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901195552187.png\" alt=\"image-20220901195552187\" style=\"zoom:50%;\" />\n\n- 通过docker-desktop登录进去看看：\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901195603660.png\" alt=\"image-20220901195603660\" style=\"zoom:50%;\" />\n\n- 进入到容器中，我们可以看到其实 etc/mysql下面是有配置文件的。\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901195615455.png\" alt=\"image-20220901195615455\" style=\"zoom:50%;\" />\n\n- 这样就验证了我们之前的猜测，就是本地的空文件夹把mysql里的配置文件给覆盖掉了。\n\n- 那么怎么办呢？ 我得改配置文件啊。得想办法把配置文件放在本地文件夹中，然后再次重启容器，配置文件就可以加载进去了。\n\n- 怎么把配置文件复制到本地呢？\n\n```sh\ndocker cp:mysql5.7.19-1:/etc/mysql /Users/zhuansun/workspace/docker/mysql5.7.19/conf\n```\n\n- 好了，用上面的命令就可以解决了，但是复制下来之后，发现，我草？这么多文件不知道改哪一个，而且怎么还有一个 连接 文件。\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901195651594.png\" alt=\"image-20220901195651594\" style=\"zoom:50%;\" />\n\n- 继续百度吧，又被恶心到了一圈。最终决定还是靠自己，但是还是搜到一些有用的东西的。关于这个链接文件是什么？\n- 其实并不是所有的配置文件都在etc下面的，mysql配置文件分为全局配置，和用户配置；是放在不同的地方的，但是**会通过一个链接文件，链接过来**；\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901195658976.png\" alt=\"image-20220901195658976\" style=\"zoom:50%;\" />\n\n- 所以按照这个思路，我们去 mysql5.7.19-1 中去看看，链接文件都连接到哪里啦。\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901195805841.png\" alt=\"image-20220901195805841\" style=\"zoom:50%;\" />\n\n- 打开这两个文件，看看\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901195823380.png\" alt=\"image-20220901195823380\" style=\"zoom:50%;\" />\n\n- 打开之后可以看到一个文件是用来配置 mysql 的一个文件是用来配置mysqld的，\n\n- 到这里就比较明确的，我们把设置编码的命令设置进来。像下面这个样子。注意要在本地的文件夹中修改哦：只需要修改mysqld.cnf就行了，因为它是全局的\n\n```sql\n# Copyright (c) 2014, 2016, Oracle and/or its affiliates. All rights reserved.\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; version 2 of the License.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA\n​\n#\n# The MySQL  Server configuration file.\n#\n# For explanations see\n# http://dev.mysql.com/doc/mysql/en/server-system-variables.html\n​\n[mysqld]\ncharacter-set-server=utf8\ncollation-server=utf8_general_ci\npid-file    = /var/run/mysqld/mysqld.pid\nsocket      = /var/run/mysqld/mysqld.sock\ndatadir     = /var/lib/mysql\n#log-error  = /var/log/mysql/error.log\n# By default we only accept connections from localhost\n#bind-address   = 127.0.0.1\n# Disabling symbolic-links is recommended to prevent assorted security risks\nsymbolic-links=0\n[client]\ndefault-character-set=utf8\n```\n\n\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901200508795.png\" alt=\"image-20220901200508795\" style=\"zoom:50%;\" />\n\n- 然后重启docker容器；欣喜若狂\n\n- 结果发现中文还是插入不进来。卧槽？而且查看编码，仍然是拉丁。奇怪了。\n\n- 最后仔细看了一下，发现是拉丁的参数是database级别的。\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901200533956.png\" alt=\"image-20220901200533956\" style=\"zoom:50%;\" />\n\n- 然后想到，创建数据库之后，数据库是有编码的，因为这个数据库是在修改编码之前创建的，所以他的编码是不会变的。\n\n- 重新创建一个数据库再次测试，发现新的数据库的编码是正确的。\n\n- 至此，中文乱码问题解决。\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901200554433.png\" alt=\"image-20220901200554433\" style=\"zoom:50%;\" />\n\n\n\n<img src=\"docker安装mysql5719并解决中文乱码.assets/image-20220901200614121.png\" alt=\"image-20220901200614121\" style=\"zoom:50%;\" />\n\n","tags":["docker","mysql","乱码"],"categories":["JAVA","部署与容器","DOCKER"]},{"title":"群辉记录","url":"/note/MYSELF/群辉记录/","content":"\n\n\n\n\n# 群辉的SSH\n\n\n\n---\n\n\n\n开启TTYD-WEB\n\n外网端口：7688，内网端口：7681\n\n用户名：admin\n\n密码：zspc-pass@2021mima\n\n进入之后获取root权限\n\n```shell\nadmin@DS918Plus:~$ sudo su\n\nPassword: \n\nash-4.3# //就获得了root权限\n```\n\n\n\n\n\n# 群辉账号记录\n\n---\n\n\n\n\n\n端口\n\n1335 : nas\n\n1995：为知笔记的（admin@win.cn 密码：zhuansun1995  管理员账号） 其他用户，直接注册就行；\n\n笔记的阿里云oss配置\n\n```json\n{\n \"bucket\": \"win-data-nas\",\n \"region\": \"oss-cn-hangzhou\",\n \"accessKeyId\": \"LTAIBy8ViubtsSc7\",\n \"accessKeySecret\": \"ywuXX8QDv4HeswsRhtwuYGF7N1ml9o\",\n \"internal\": false\n}\n```\n\n\n\n同步到网盘的密码都是：zhuansun1995\n\n\\- 群辉文件是加密的\n\n\\- docker文件不是加密的，方便docker迁移\n\n<img src=\"群辉记录.assets/image-20220822121931965.png\" alt=\"image-20220822121931965\" style=\"zoom:50%;\" />\n\n\n\n\n\n常用密码：\n\nzhuansun1995\n\nzhuansun1996\n\nZHUANsun1995\n\nzhuansun666\n\nZHUANsun1996\n\nZHUANsun@1995\n\nPENGcheng@1995\n\nzspc521\n\n### gpg秘钥\n\n- name:zhuansun\n- email:[zhuansunpengcheng@qq.com](mailto:zhuansunpengcheng@qq.com)\n- Passphrase:ZHUANsun@1995\n\n### snapType\n\n- zhuansun\n- PENGcheng@1995","tags":["个人账号"],"categories":["NAS"]},{"title":"mermaid","url":"/note/TOOLS/markdown/mermaid/","content":"\n\n\n# mermaid\n\n\n\n## 关于 Mermaid\n\n[https://github.com/mermaid-js/mermaid/blob/develop/README.zh-CN.md](https://github.com/mermaid-js/mermaid/blob/develop/README.zh-CN.md)\n\n\n\n## 示例\n\n**下面是一些可以使用 Mermaid 创建的图表示例。点击 [语法](https://mermaid-js.github.io/mermaid/#/n00b-syntaxReference) 查看详情。**\n\n<table>\n<!-- <Flowchart> -->\n\n### 流程图 [<a href=\"https://mermaid-js.github.io/mermaid/#/flowchart\">文档</a> ]\n\n```\nflowchart LR\nA[Hard] -->|Text| B(Round)\nB --> C{Decision}\nC -->|One| D[Result 1]\nC -->|Two| E[Result 2]\n```\n\n```mermaid\nflowchart LR\nA[Hard] -->|Text| B(Round)\nB --> C{Decision}\nC -->|One| D[Result 1]\nC -->|Two| E[Result 2]\n```\n\n### 时序图 [<a href=\"https://mermaid-js.github.io/mermaid/#/sequenceDiagram\">文档</a> ]\n\n```\nsequenceDiagram\nAlice->>John: Hello John, how are you?\nloop Healthcheck\n    John->>John: Fight against hypochondria\nend\nNote right of John: Rational thoughts!\nJohn-->>Alice: Great!\nJohn->>Bob: How about you?\nBob-->>John: Jolly good!\n```\n\n```mermaid\nsequenceDiagram\nAlice->>John: Hello John, how are you?\nloop Healthcheck\n    John->>John: Fight against hypochondria\nend\nNote right of John: Rational thoughts!\nJohn-->>Alice: Great!\nJohn->>Bob: How about you?\nBob-->>John: Jolly good!\n```\n\n### 甘特图 [<a href=\"https://mermaid-js.github.io/mermaid/#/gantt\">文档</a> ]\n\n```\ngantt\n    section Section\n    Completed :done,    des1, 2014-01-06,2014-01-08\n    Active        :active,  des2, 2014-01-07, 3d\n    Parallel 1   :         des3, after des1, 1d\n    Parallel 2   :         des4, after des1, 1d\n    Parallel 3   :         des5, after des3, 1d\n    Parallel 4   :         des6, after des4, 1d\n```\n\n```mermaid\ngantt\n    section Section\n    Completed :done,    des1, 2014-01-06,2014-01-08\n    Active        :active,  des2, 2014-01-07, 3d\n    Parallel 1   :         des3, after des1, 1d\n    Parallel 2   :         des4, after des1, 1d\n    Parallel 3   :         des5, after des3, 1d\n    Parallel 4   :         des6, after des4, 1d\n```\n\n### 类图 [<a href=\"https://mermaid-js.github.io/mermaid/#/classDiagram\">文档</a> ]\n\n```\nclassDiagram\nClass01 <|-- AveryLongClass : Cool\n<<Interface>> Class01\nClass09 --> C2 : Where am I?\nClass09 --* C3\nClass09 --|> Class07\nClass07 : equals()\nClass07 : Object[] elementData\nClass01 : size()\nClass01 : int chimp\nClass01 : int gorilla\nclass Class10 {\n  <<service>>\n  int id\n  size()\n}\n```\n\n```mermaid\nclassDiagram\nClass01 <|-- AveryLongClass : Cool\n<<Interface>> Class01\nClass09 --> C2 : Where am I?\nClass09 --* C3\nClass09 --|> Class07\nClass07 : equals()\nClass07 : Object[] elementData\nClass01 : size()\nClass01 : int chimp\nClass01 : int gorilla\nclass Class10 {\n  <<service>>\n  int id\n  size()\n}\n```\n\n### 状态图 [<a href=\"https://mermaid-js.github.io/mermaid/#/stateDiagram\">文档</a>]\n\n```\nstateDiagram-v2\n[*] --> Still\nStill --> [*]\nStill --> Moving\nMoving --> Still\nMoving --> Crash\nCrash --> [*]\n```\n\n```mermaid\nstateDiagram-v2\n[*] --> Still\nStill --> [*]\nStill --> Moving\nMoving --> Still\nMoving --> Crash\nCrash --> [*]\n```\n\n### 饼图 [<a href=\"https://mermaid-js.github.io/mermaid/#/pie\">文档</a> ]\n\n```\npie\n\"Dogs\" : 386\n\"Cats\" : 85\n\"Rats\" : 15\n```\n\n```mermaid\npie\n\"Dogs\" : 386\n\"Cats\" : 85\n\"Rats\" : 15\n```\n\n### Git 图 [实验特性 ]\n\n### 用户体验旅程图 [<a href=\"https://mermaid-js.github.io/mermaid/#/user-journey\">文档</a> ]\n\n```\n  journey\n    title My working day\n    section Go to work\n      Make tea: 5: Me\n      Go upstairs: 3: Me\n      Do work: 1: Me, Cat\n    section Go home\n      Go downstairs: 5: Me\n      Sit down: 3: Me\n```\n\n```mermaid\n  journey\n    title My working day\n    section Go to work\n      Make tea: 5: Me\n      Go upstairs: 3: Me\n      Do work: 1: Me, Cat\n    section Go home\n      Go downstairs: 5: Me\n      Sit down: 3: Me\n```\n\n### C4 图 [<a href=\"https://mermaid-js.github.io/mermaid/#/c4c\">文档</a>]\n\n```\nC4Context\ntitle System Context diagram for Internet Banking System\nPerson(customerA, \"Banking Customer A\", \"A customer of the bank, with personal bank accounts.\")\nPerson(customerB, \"Banking Customer B\")\nPerson_Ext(customerC, \"Banking Customer C\")\nSystem(SystemAA, \"Internet Banking System\", \"Allows customers to view information about their bank accounts, and make payments.\")\nPerson(customerD, \"Banking Customer D\", \"A customer of the bank, <br/> with personal bank accounts.\")\nEnterprise_Boundary(b1, \"BankBoundary\") {\n  SystemDb_Ext(SystemE, \"Mainframe Banking System\", \"Stores all of the core banking information about customers, accounts, transactions, etc.\")\n  System_Boundary(b2, \"BankBoundary2\") {\n    System(SystemA, \"Banking System A\")\n    System(SystemB, \"Banking System B\", \"A system of the bank, with personal bank accounts.\")\n  }\n  System_Ext(SystemC, \"E-mail system\", \"The internal Microsoft Exchange e-mail system.\")\n  SystemDb(SystemD, \"Banking System D Database\", \"A system of the bank, with personal bank accounts.\")\n  Boundary(b3, \"BankBoundary3\", \"boundary\") {\n    SystemQueue(SystemF, \"Banking System F Queue\", \"A system of the bank, with personal bank accounts.\")\n    SystemQueue_Ext(SystemG, \"Banking System G Queue\", \"A system of the bank, with personal bank accounts.\")\n  }\n}\nBiRel(customerA, SystemAA, \"Uses\")\nBiRel(SystemAA, SystemE, \"Uses\")\nRel(SystemAA, SystemC, \"Sends e-mails\", \"SMTP\")\nRel(SystemC, customerA, \"Sends e-mails to\")\n```\n\n```mermaid\nC4Context\ntitle System Context diagram for Internet Banking System\nPerson(customerA, \"Banking Customer A\", \"A customer of the bank, with personal bank accounts.\")\nPerson(customerB, \"Banking Customer B\")\nPerson_Ext(customerC, \"Banking Customer C\")\nSystem(SystemAA, \"Internet Banking System\", \"Allows customers to view information about their bank accounts, and make payments.\")\nPerson(customerD, \"Banking Customer D\", \"A customer of the bank, <br/> with personal bank accounts.\")\nEnterprise_Boundary(b1, \"BankBoundary\") {\n  SystemDb_Ext(SystemE, \"Mainframe Banking System\", \"Stores all of the core banking information about customers, accounts, transactions, etc.\")\n  System_Boundary(b2, \"BankBoundary2\") {\n    System(SystemA, \"Banking System A\")\n    System(SystemB, \"Banking System B\", \"A system of the bank, with personal bank accounts.\")\n  }\n  System_Ext(SystemC, \"E-mail system\", \"The internal Microsoft Exchange e-mail system.\")\n  SystemDb(SystemD, \"Banking System D Database\", \"A system of the bank, with personal bank accounts.\")\n  Boundary(b3, \"BankBoundary3\", \"boundary\") {\n    SystemQueue(SystemF, \"Banking System F Queue\", \"A system of the bank, with personal bank accounts.\")\n    SystemQueue_Ext(SystemG, \"Banking System G Queue\", \"A system of the bank, with personal bank accounts.\")\n  }\n}\nBiRel(customerA, SystemAA, \"Uses\")\nBiRel(SystemAA, SystemE, \"Uses\")\nRel(SystemAA, SystemC, \"Sends e-mails\", \"SMTP\")\nRel(SystemC, customerA, \"Sends e-mails to\")\n```\n\n","tags":["mermaid","markdown"],"categories":["TOOLS","MARKDOWN"]},{"title":"mysql的问题汇总","url":"/note/JAVA/数据库/MYSQL/mysql的问题汇总/","content":"\n\n\n\n\n\n\n准备从45将的评论区找问题，汇总一下\n\n\n\n","tags":["mysql"],"categories":["JAVA","数据库","MYSQL"]},{"title":"mysql的日志从入门到入土","url":"/note/JAVA/数据库/MYSQL/mysql的日志从入门到入土/","content":"\n\n\n# mysql的日志从入门到入土\n\n---\n\n本文有xmind，配合xmind查看更加友好哦\n\n> 点击下载：[mysql日志从入门到入土.xmind](mysql的日志从入门到入土.assets/mysql日志从入门到入土.xmind)\n\n\n\n## 一条更新语句的执行流程是什么（引入redolog和binlog）\n\n> update T set c=c+1 where ID=2;\n\nmysql的更新流程和SQL语句的基本执行链路是一样的：连接器->分析器->优化器->执行器->存储引擎\n\n<img src=\"mysql的日志从入门到入土.assets/image-20221012203451881.png\" alt=\"image-20221012203451881\" style=\"zoom: 80%;\" />\n\n\n\n通过连接器，先连接数据库。\n\n清空查询缓存：在一个表上有更新的时候，跟这个表有关的查询缓存会失效。这也就是我们一般不建议使用查询缓存的原因（在mysql8.0中，已经把查询缓存整个模块都删掉了）。\n\n分析器会通过词法和语法解析知道这是一条更新语句。\n\n优化器决定要使用哪个索引。\n\n执行器负责具体执行，找到这一行，然后更新。\n\n\n\n## 重要的日志模块（redolog和binlog）\n\n与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和 binlog（归档日志）\n\n\n\n### 什么是redolog\n\n作者举了《孔乙己》中一个赊账的例子，孔乙己来到酒店喝酒，老板会先把孔乙己的酒钱记录在一个粉板上，然后等不忙的时候，在累加到账本上。\n\n这里的粉板就是redolog，账本就是磁盘；\n\n\n\n考虑这个场景，当很多很多的人来喝酒的时候，老板一般有两种方式记账（当数据库更新一条记录的时候，一般有两种方式）：\n\n- 直接掏出账本，在账本上加加减减。（直接操作磁盘的数据，进行更新）\n- 另一种做法是先在粉板上记下这次的账，等打烊以后再把账本翻出来核算（先记录到redolog，等mysql空闲的时候，刷到磁盘）。\n\n\n\n在酒店生意红火的时候，老板一定选择后者，因为前者操作实在是太麻烦了。\n\n- 首先，你得找到这个人的赊账总额那条记录。你想想，密密麻麻几十页，掌柜要找到那个名字，可能还得带上老花镜慢慢找。（磁盘的随机IO读）\n- 找到之后再拿出算盘计算，最后再将结果写回到账本上（磁盘的随机写）\n\n\n\n粉板（redolog）就完美的解决了这两个问题：\n\n- 首先说找记录：mysql的所有记录都是从数据页中查的，如果要更新的数据所在的数据页在内存中，可以直接找到，如果不在内存中，会先从磁盘把这个数据库加载到内存中。（这个步骤没法省，redolog优化的地方并不在于这里，这里会有**changebuffer**优化（后面说））\n- 找到记录之后，然后说更新记录：更新的结果是写到redolog中，而不是写到磁盘中，就避免了磁盘的随机IO，虽然redolog也是写到磁盘中的，但是由于组提交的存在，一次磁盘的写入是大量的顺序IO；（redolog是顺序写，并且可以组提交，还有别的一些优化，收益最大是是这两个因素；）\n\n\n\n### redo log有什么用\n\n有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 **crash-safe**。\n\n\n\n### 为什么要引入redolog\n\n因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，**binlog** 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。\n\nmysql要保证数据的持久性，保证持久性就需要将数据写到磁盘，但是写到磁盘的话，涉及到\n\n- 刷页，因为mysql所有的操作是针对数据页操作的，而一个简单的更新，可能就要刷整整一个数据页\n- 随机IO，一个事务所更新的涉及到的数据页可能不止一个，而且数据页可能不相连，就涉及到随机IO\n\n这俩问题，咋办呢？就引入了redolog，一个更新不刷页（先记录到redolog日志中），这样就避免了上面两个问题\n\n- redolog将随机IO改成了顺序IO，而且避免了每次更新都刷盘（刷盘也是要刷的，但是是组提交）\n\n\n\n### redolog的结构\n\nredolog是循环写的文件，InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。\n\n<img src=\"mysql的日志从入门到入土.assets/image-20221027161923607.png\" alt=\"image-20221027161923607\" style=\"zoom: 50%;\" />\n\nwrite pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。\n\nwrite pos 和 checkpoint 之间的是还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。\n\n\n\n### redolog一般设置多大\n\nredo log 太小的话，会导致很快就被写满，然后不得不强行刷 redo log，这样 WAL 机制的能力就发挥不出来了。所以，如果是现在常见的几个 TB 的磁盘的话，就不要太小气了，直接将 redo log 设置为 4 个文件、每个文件 1GB 吧。\n\n- innodb_log_file_size：该参数指定了每个redo日志文件的大小，在MySQL 5.7.21这个版本中的默认值为48MB，\n- innodb_log_files_in_group：该参数指定redo日志文件的个数，默认值为2，最大值为100。\n\n\n\n\n\n### binlog是什么\n\n我们知道mysql是由两部分组成，server层和引擎层，上面介绍的redolog就是innodb引擎独有的日志，而MySQL的server层也有自己的日志，叫做binlog；\n\n为什么要有两个日志呢\n\n因为mysql在5.5版本之前，默认的存储引擎是MyISAM，但是MyISAM并没有**Crash-Safe**的能力，而server层自带的binlog又只有归档的能力，也不具备**Crash-Safe**的能力，所以才会有后来的innodb以插件的形式引入mysql中，作为mysql的引擎，并使用了redo log，实现了Crash-Safe的能力\n\n除了以上的原因，binlog和redolog还有其他的区别，解释了为什么存在两个日志\n\n- 这两个日志的使用方不一样：redolog是innodb引擎所特有的，只有innodb才能用；而binlog是mysql的server层有的，所有的引擎都可以使用；\n- 这两个日志记录的内容不一样：redolog是物理日志，记录的是在某个数据页上做了什么修改；而binlog是逻辑日志，简单的说就是sql语句。\n- 这两个日志的记录方式不一样：redolog是循环写，redolog文件写满了，会从头重新写；binlog是追加写；binlog文件写满了，会切割，在新文件中继续写\n\n\n\n### binlog有什么用\n\n主要是归档（归档之后可以用于数据恢复）和主从同步\n\n\n\n### binlog的结构\n\n#### binlog的存储目录\n\n在磁盘的上的结构，binlog默认是存放在**MySQL服务器的数据目录**下，（可以修改binlog的存放路径和binlog的文件名），如果你不知道数据目录是哪个，可以通过这个命令查看\n\n```sql\nmysql> show variables like '%datadir%';\n+---------------+---------------------------------------------+\n| Variable_name | Value                                       |\n+---------------+---------------------------------------------+\n| datadir       | C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Data\\ |\n+---------------+---------------------------------------------+\n1 row in set, 1 warning (0.00 sec)\n```\n\n在数据目录下，你就可以看到binlog的文件，就像是这样，binlog是二进制文件，就像它的全名一样：binary log，所以是不能直接打开的：\n\n```sql\nxxx-bin.000001\nxxx-bin.000002\nxxx-bin.000003\nxxx-bin.000004\n...\n```\n\n除了真正存储binlog日志的文件外，MySQL服务器还会在相同的路径下生成一个关于binlog的索引文件，它的名称就是：\n\n```sql\nxxx-bin.index\n```\n\n这个索引文件是一个文本文件，我们可以直接打开：\n\n```mysql\nshell> cat xxx-bin.index\n./xxx-bin.000001\n./xxx-bin.000001\n./xxx-bin.000001\n./xxx-bin.000001\n```\n\n可以看到，这个索引文件只是简单的将各个binlog文件的路径存储了起来而已。\n\n\n\n#### 怎么查看binlog的格式\n\n下面的三种查看方式，前两个是一样的，都表示查看当前session的binlog格式；最后一个表示查看全局的binlog格式\n\n```sql\nmysql> show variables like '%binlog_format%';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| binlog_format | ROW   |\n+---------------+-------+\n1 row in set, 1 warning (0.02 sec)\n\nmysql> show session variables like '%binlog_format%';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| binlog_format | ROW   |\n+---------------+-------+\n1 row in set, 1 warning (0.02 sec)\n\nmysql> show global variables like '%binlog_format%';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| binlog_format | ROW   |\n+---------------+-------+\n1 row in set, 1 warning (0.02 sec)\n```\n\n\n\n#### 怎么设置binlog的格式\n\n下面展示三种设置binlog的方式，前两个是一样的，设置当前session的binlog格式，重启后就失效了。最后一个表示设置全局的binlog格式，需要重启后才生效。\n\n```sql\nmysql> SET binlog_format = 'statement';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> SET session binlog_format = 'statement';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> SET global binlog_format = 'statement';\nQuery OK, 0 rows affected (0.00 sec)\n\n```\n\n\n\n\n\n#### binlog的三种格式\n\nbinlog 有两种格式，一种是 statement，一种是 row。可能你在其他资料上还会看到有第三种格式，叫作 mixed，其实它就是前两种格式的混合。\n\n下面看一下三种格式分别记录了什么？\n\n准备以下数据：\n\n```sql\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `a` int(11) DEFAULT NULL,\n  `t_modified` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n  PRIMARY KEY (`id`),\n  KEY `a` (`a`),\n  KEY `t_modified`(`t_modified`)\n) ENGINE=InnoDB;\n\ninsert into t values(1,1,'2018-11-13');\ninsert into t values(2,2,'2018-11-12');\ninsert into t values(3,3,'2018-11-11');\ninsert into t values(4,4,'2018-11-10');\ninsert into t values(5,5,'2018-11-09');\n```\n\n\n\n###### **statement**\n\n按照上面的方式，查看当前binlog的格式，并将当前会话的binlog的格式设置为：statement\n\n```sql\nmysql> show variables like '%binlog_format%';\n+---------------+-----------+\n| Variable_name | Value     |\n+---------------+-----------+\n| binlog_format | STATEMENT |\n+---------------+-----------+\n1 row in set, 1 warning (0.00 sec)\n```\n\n\n\n执行以下语句\n\n```sql\nmysql> delete from t where a>=4 and t_modified<='2018-11-10' limit 1;\nQuery OK, 1 row affected (0.01 sec)\n```\n\n\n\n在查看binlog的内容之前，首先查看当前binlog写在了哪个文件上，因为binlog有很多个\n\n```sql\nmysql> show master status;\n+----------------+----------+--------------+------------------+-------------------+\n| File           | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |\n+----------------+----------+--------------+------------------+-------------------+\n| xxx-bin.000002 |     7829 |              |                  |                   |\n+----------------+----------+--------------+------------------+-------------------+\n1 row in set (0.00 sec)\n```\n\n然后查看binlog的内容（binlog很大，这里只截取了一部分）\n\n```sql\nmysql> show binlog events in 'xxx-bin.000002';\n+----------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------+\n| Log_name       | Pos  | Event_type     | Server_id | End_log_pos | Info                                                                      |\n+----------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------+\n| xxx-bin.000002 | 7489 | Anonymous_Gtid |         1 |        7568 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                      |\n| xxx-bin.000002 | 7568 | Query          |         1 |        7654 | BEGIN                                                                     |\n| xxx-bin.000002 | 7654 | Query          |         1 |        7798 | use `zs`; delete from t   where a>=4 and t_modified<='2018-11-10' limit 1 |\n| xxx-bin.000002 | 7798 | Xid            |         1 |        7829 | COMMIT /* xid=1840 */                                                     |\n+----------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------+\n56 rows in set (0.00 sec)\n```\n\n内容解释\n\n| 行                                           | 含义                                                         |\n| -------------------------------------------- | ------------------------------------------------------------ |\n| 第一行：SET @@SESSION.GTID_NEXT= 'ANONYMOUS' | 主备切换用的，见《mysql主备从入门到入土》                    |\n| 第二行：BEGIN                                | 跟第四行的 commit 对应，表示中间是一个事务                   |\n| 第三行                                       | 是真实执行的语句了。可以看到，在真实执行的 delete 命令之前，还有一个“use ‘zs’”命令。这条命令是 MySQL 根据当前要操作的表所在的数据库，自行添加的。这样做可以保证日志传到备库去执行的时候，不论当前的工作线程在哪个库里，都能够正确地更新到 test 库的表 t。use 'zs’命令之后的 delete 语句，就是我们输入的 SQL 原文了。 |\n| 第四行：COMMIT /* xid=1840 */                | 你可以看到里面写着 xid=1840,xid是崩溃恢复的时候，和redolog关联，用来校验binlog完整性的 |\n\n- 使用mysqlbinlog工具，可以查看到更加具体的内容\n\n```mysql\nC:\\Program Files\\MySQL\\MySQL Server 8.0\\bin>mysqlbinlog.exe  -vv \"C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Data\\SF0001408876LA-bin.000002\" --start-position=7829 --stop-position=8126\n# The proper term is pseudo_replica_mode, but we use this compatibility alias\n# to make the statement usable on server versions 8.0.24 and older.\n/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;\n/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;\nDELIMITER /*!*/;\n# at 156\n#221101 14:12:06 server id 1  end_log_pos 125 CRC32 0xe443eab6  Start: binlog v 4, server v 8.0.26 created 221101 14:12:06 at startup\n# Warning: this binlog is either in use or was not closed properly.\nROLLBACK/*!*/;\nBINLOG '\ntrhgYw8BAAAAeQAAAH0AAAABAAQAOC4wLjI2AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAC2uGBjEwANAAgAAAAABAAEAAAAYQAEGggAAAAICAgCAAAACgoKKioAEjQA\nCigBtupD5A==\n'/*!*/;\n# at 7829\n#221108 17:42:32 server id 1  end_log_pos 7908 CRC32 0x816102e0         Anonymous_GTID  last_committed=26       sequence_number=27      rbr_only=no     original_committed_timestamp=1667900553012960   immediate_commit_timestamp=1667900553012960  transaction_length=327\n# original_commit_timestamp=1667900553012960 (2022-11-08 17:42:33.012960 中国标准时间)\n# immediate_commit_timestamp=1667900553012960 (2022-11-08 17:42:33.012960 中国标准时间)\n/*!80001 SET @@session.original_commit_timestamp=1667900553012960*//*!*/;\n/*!80014 SET @@session.original_server_version=80026*//*!*/;\n/*!80014 SET @@session.immediate_server_version=80026*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 7908\n#221108 17:42:32 server id 1  end_log_pos 7986 CRC32 0x08dd3003         Query   thread_id=18    exec_time=1     error_code=0\nSET TIMESTAMP=1667900552/*!*/;\nSET @@session.pseudo_thread_id=18/*!*/;\nSET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/;\nSET @@session.sql_mode=1075838976/*!*/;\nSET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/;\n/*!\\C utf8mb4 *//*!*/;\nSET @@session.character_set_client=255,@@session.collation_connection=255,@@session.collation_server=255/*!*/;\nSET @@session.lc_time_names=0/*!*/;\nSET @@session.collation_database=DEFAULT/*!*/;\n/*!80011 SET @@session.default_collation_for_utf8mb4=255*//*!*/;\nBEGIN\n/*!*/;\n# at 7986\n#221108 17:42:32 server id 1  end_log_pos 8125 CRC32 0x8d364776         Query   thread_id=18    exec_time=1     error_code=0\nuse `zs`/*!*/;\nSET TIMESTAMP=1667900552/*!*/;\n/* ApplicationName=DataGrip 2021.1.2 */ delete from t where id = 3\n/*!*/;\n# at 8125\n#221108 17:42:32 server id 1  end_log_pos 8156 CRC32 0x9959b790         Xid = 2011\nCOMMIT/*!*/;\nSET @@SESSION.GTID_NEXT= 'AUTOMATIC' /* added by mysqlbinlog */ /*!*/;\nDELIMITER ;\n# End of log file\n/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;\n/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/;\n```\n\n\n\n###### **row**\n\n先将测试数据复原，重新导入\n\n按照上面的方式，查看当前binlog的格式，并将当前会话的binlog的格式设置为：row\n\n```sql\nmysql> show variables like '%binlog_format%';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| binlog_format | ROW   |\n+---------------+-------+\n1 row in set, 1 warning (0.00 sec)\n```\n\n执行以下语句\n\n```sql\nmysql> delete from t where a>=4 and t_modified<='2018-11-10' limit 1;\nQuery OK, 1 row affected (0.01 sec)\n```\n\n在查看binlog的内容之前，首先查看当前binlog写在了哪个文件上，因为binlog有很多个\n\n```sql\nmysql> show master status;\n+----------------+----------+--------------+------------------+-------------------+\n| File           | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |\n+----------------+----------+--------------+------------------+-------------------+\n| xxx-bin.000002 |     7829 |              |                  |                   |\n+----------------+----------+--------------+------------------+-------------------+\n1 row in set (0.00 sec)\n```\n\n然后查看binlog的内容（binlog很大，这里只截取了一部分）\n\n```sql\nmysql> show binlog events in 'xxx-bin.000002';\n+----------------+------+----------------+-----------+-------------+--------------------------------------+\n| Log_name       | Pos  | Event_type     | Server_id | End_log_pos | Info                                 |\n+----------------+------+----------------+-----------+-------------+--------------------------------------+\n| xxx-bin.000002 | 5043 | Anonymous_Gtid |         1 |        5122 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS' |\n| xxx-bin.000002 | 5122 | Query          |         1 |        5203 | BEGIN                                |\n| xxx-bin.000002 | 5203 | Table_map      |         1 |        5251 | table_id: 169 (zs.t)                 |\n| xxx-bin.000002 | 5251 | Delete_rows    |         1 |        5299 | table_id: 169 flags: STMT_END_F      |\n| xxx-bin.000002 | 5299 | Xid            |         1 |        5330 | COMMIT /* xid=1924 */                |\n+----------------+------+----------------+-----------+-------------+--------------------------------------+\n74 rows in set (0.00 sec)\n```\n\n内容解释\n\n| 行                                           | 含义                                                         |\n| -------------------------------------------- | ------------------------------------------------------------ |\n| 第一行：SET @@SESSION.GTID_NEXT= 'ANONYMOUS' | 主备切换用的，见《mysql主备从入门到入土》                    |\n| 第二行：BEGIN                                | 跟第五行的 commit 对应，表示中间是一个事务                   |\n| 第三行：                                     | 在statement格式中，记录的是sql原文，在row格式下，记录的是两个event：Table_map和Delete_rows这两个动作 |\n| 第四行：                                     | Table_map表示要操作哪个数据库的那张表； Delete_rows表示删除一行，具体的内容，这里看不到，需要借助mysqlbinlog工具来看 |\n| 第五行：COMMIT /* xid=1924 */                | 你可以看到里面写着 xid=1924 ,xid是崩溃恢复的时候，和redolog关联，用来校验binlog完整性的 |\n\n在第四行中，我们看不到具体的内容，所以需要通过mysqlbinlog工具来看\n\nmysqlbinlog是啥，就是一个可以执行的工具，在windows系统下，这个工具在mysql的安装目录下，叫：mysqlbinlog.exe\n\n因为我现在用的windows，所以就用windows来展示了\n\n```sh\nC:\\Users>cd C:\\Program Files\\MySQL\\MySQL Server 8.0\\bin\n\nC:\\Program Files\\MySQL\\MySQL Server 8.0\\bin>dir\n 驱动器 C 中的卷是 系统\n 卷的序列号是 0003-57E7\n\n C:\\Program Files\\MySQL\\MySQL Server 8.0\\bin 的目录\n\n2022/11/01  14:10    <DIR>          .\n2022/11/01  14:10    <DIR>          ..\n2021/07/01  02:12         xxxxxxxxx xxxxxxxx（因为文件太多了，所以我这里就是省略了）\n2021/07/01  02:12         6,960,408 mysql.exe\n2021/07/01  02:12         6,854,952 mysqladmin.exe\n2021/07/01  02:12         7,168,808 mysqlbinlog.exe\n              48 个文件    270,259,595 字节\n               2 个目录 24,862,003,200 可用字节\n```\n\n可以看到在bin目录下，有一个工具叫做：mysqlbinlog.exe\n\n然后我们在`C:\\Program Files\\MySQL\\MySQL Server 8.0\\bin`这个目录下运行下面的命令\n\n```sh\nmysqlbinlog.exe  -vv \"C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Data\\SF0001408876LA-bin.000002\" --start-position=5043 --stop-position=5300\n```\n\n- `-vv`表示verbose，啰嗦模式，为了把内容都解析出来，所以从结果里面可以看到各个字段的值（比如，@1=4、 @2=4 这些值）。\n- `--start-position`表示binlog的开始位置，值哪里来的，来自于`show binlog events in 'xxx-bin.000002'`结果的pos字段\n- `--stop-position`表示binlog的结束位置，值哪里来的，来自于`show binlog events in 'xxx-bin.000002'`结果的pos字段，写大一点，要不然不包括进来（比如我的pos=5299，但是我这里写的是5300）\n\n运行结果如下（结果还挺长的，删掉了一些，只列出比较重要的几个内容）\n\n```sh\nC:\\Program Files\\MySQL\\MySQL Server 8.0\\bin>mysqlbinlog.exe  -vv \"C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Data\\SF0001408876LA-bin.000002\" --start-position=5043 --stop-position=5300\n# at 5043\n#221101 15:21:11 server id 1  end_log_pos 5122 CRC32 0x72f668e7         Anonymous_GTID  last_committed=17       sequence_number=18      rbr_only=yes    original_committed_timestamp=1667287271257812   immediate_commit_timestamp=1667287271257812  transaction_length=287\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 5122\n#221101 15:21:11 server id 1  end_log_pos 5203 CRC32 0xe3288066         Query   thread_id=11    exec_time=0     error_code=0\nSET TIMESTAMP=1667287271/*!*/;\nSET @@session.pseudo_thread_id=11/*!*/;\nSET @@session.sql_mode=1075838976/*!*/;\nBEGIN\n/*!*/;\n# at 5203\n#221101 15:21:11 server id 1  end_log_pos 5251 CRC32 0x613f3131         Table_map: `zs`.`t` mapped to number 169\n# at 5251\n#221101 15:21:11 server id 1  end_log_pos 5299 CRC32 0x75141201         Delete_rows: table id 169 flags: STMT_END_F\n\nBINLOG '\n58hgYxMBAAAAMAAAAIMUAAAAAKkAAAAAAAEAAnpzAAF0AAMDAxEBAAIBAQAxMT9h\n58hgYyABAAAAMAAAALMUAAAAAKkAAAAAAAEAAgAD/wAEAAAABAAAAFvlrwABEhR1\n'/*!*/;\n### DELETE FROM `zs`.`t`\n### WHERE\n###   @1=4 /* INT meta=0 nullable=0 is_null=0 */\n###   @2=4 /* INT meta=0 nullable=1 is_null=0 */\n###   @3=1541779200 /* TIMESTAMP(0) meta=0 nullable=0 is_null=0 */\n# at 5299\n#221101 15:21:11 server id 1  end_log_pos 5330 CRC32 0xc30d2901         Xid = 1924\nCOMMIT/*!*/;\nSET @@SESSION.GTID_NEXT= 'AUTOMATIC' /* added by mysqlbinlog */ /*!*/;\nDELIMITER ;\n# End of log file\n/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;\n/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/;\n```\n\n- `server id 1`表示这个事务是在 server_id=1 的这个库上执行的。\n- `CRC32 0xc30d2901`每个 event 都有 CRC32 的值，这是因为我把参数 binlog_checksum 设置成了 CRC32。\n\n  - 可以通过`show variables like '%binlog_checksum%';` 查看binlog_checksum 的值\n- `SET TIMESTAMP=1667287271/*!*/;`当前sql执行的时间戳，在主备同步的时候，如果有延迟，而sql中又使用了日期函数的话，容易导致主备不一致，所以mysql在binlog中，保存了每个sql执行的时间，这样主备同步的时候，日期函数就不会出问题了。\n- `@1=4 /* INT meta=0 nullable=0 is_null=0 */`\n- `@2=4 /* INT meta=0 nullable=1 is_null=0 */`\n- `@3=1541779200 /* TIMESTAMP(0) meta=0 nullable=0 is_null=0 */`\n- 上面这三行，表示被删掉的这条记录的原始的值。为什么会记录的这么详细？因为我们开启了记录全部信息\n\n```sql\nmysql> show variables like '%binlog_row_image%';\n+------------------+-------+\n| Variable_name    | Value |\n+------------------+-------+\n| binlog_row_image | FULL  |\n+------------------+-------+\n1 row in set, 1 warning (0.02 sec)\n```\n\n- 如果把`binlog_row_image`设置为：MINIMAL，就不会记录的这么详细的，只会记录一个id而已。\n- 最后的 Xid event，用于表示事务被正确地提交了。\n\n\n\n###### **mix**\n\nmix格式的binlog其实就是statement和row格式的结合。\n\n对于statement来说，容易导致主备同步不一致的问题，比如主备上索引选择不一致的话，就会导致主备不一致；\n\n对于row来说，如果删除的数据很多，row会把删掉的每一条记录都记下来，占用磁盘IO，浪费空间。\n\n所以就诞生了mix格式\n\n在binlog_format=mix格式下，mysql会自己判断，如果当前语句存在数据不一致的风险，就会采用row格式，否则采用statement格式；\n\n但是mix格式下，因为是mysql自己判断的，这些判断逻辑在实际环境中，可能会出现不可预知的问题。\n\n所以：一般都是直接使用 ROW 格式\n\n\n\n在公司的生产环境，使用的binlog格式是：ROW\n\n```sql\n-- 查看binlog的格式\nshow variables like 'binlog_format';\n```\n\n\n\n### undo log是什么\n\nundo log 是 MVCC实现的基础，关于undo log的详细内容，请参考：[mysql的事务从入门到入土](mysql的事务从入门到入土)\n\n\n\n\n\n## mysql的WAL机制\n\nWAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。\n\nWAL 机制是减少磁盘写，可是每次提交事务都要写 redo log 和 binlog，这磁盘读写次数也没变少呀？\n\n现在你就能理解了，WAL 机制主要得益于两个方面：\n\n- redo log 和 binlog 都是顺序写，磁盘的顺序写比随机写速度要快；\n- 组提交机制，可以大幅度降低磁盘的 IOPS 消耗。怎么降低的呢？[点我去看：组提交（双1配置，LSN）](#组提交（双1配置，LSN）)\n\n## 日志的写入流程（二阶段提交）\n\n\n\n### redolog和binlog的整体写入流程（二阶段提交）\n\n```mermaid\nsequenceDiagram\nclient->>server:更新ID=2这一行\nserver->>innodb:获取ID=2这一行\ninnodb->>innodb:ID=2这一行是否在内存页中\ninnodb->>server:在内存中，直接返回ID=2的行数据\ninnodb->>disk:不在内存中<br/>从磁盘中加载ID=2这一行的数据页\ndisk->>innodb:返回ID=2所在的数据页\ninnodb->>server:返回ID=2的行数据\nserver->>server:对ID=2的数据，进行更新操作\nserver->>innodb:写入更新后的数据\ninnodb->>innodb:更新内存\ninnodb->>innodb:记录redolog，处于prepare状态\ninnodb->>server:更新成功\nserver->>server:记录binlog\nserver->>innodb:提交事务\ninnodb->>innodb:redolog提交，处于commit状态\ninnodb->>server:更新完成\nserver->>client:更新完成\n\n\n```\n\n#### 为什么要有两阶段提交（反证法）\n\n为什么必须要有两阶段提交呢？我们知道两阶段提交是为了保证分布式事务的数据一致性的， 那么mysql是要保证什么数据的一致性。\n\n很显然：是为了保证redolog和binlog的数据一致性\n\n那么为什么要保证redolog和binlog的数据一致性呢？这就涉及到redolog和binlog的作用是什么呢？对！是为了崩溃恢复。\n\n那么我们看看没有两阶段提交会怎么样？\n\n1、**先写redolog后写binlog**。假设redolog写完了，binlog没写完，mysql崩了。重启之后，因为redolog完整，数据恢复；但是binlog不完整， binlog 里面就没有记录这个语句。然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库恢复出来数据与原库的值不同。 另外主备一致也是通过binlog同步的，binlog不完整，备库的数据就不对了。\n\n2、**先写binlog后写redolog**。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以数据不变。但是 binlog 里面已经记录了数据变更的日志。所以，在之后用 binlog 来恢复的时候数据与原库的值不同。\n\n\n\n### redolog的写入流程\n\n```mermaid\nsequenceDiagram\ninnodb->>innodb:dml成功\\n更新内存\\n开始记录redolog\ninnodb->>redologbuffer:redolog记录到内存中\nredologbuffer->>pagecache:写入到文件系统的pagecache中(write)\npagecache->>disk:持久化到磁盘文件(fsync)\n```\n\n我们看到redolog写盘是有三步的，先写redologbuffer，在write到pagecache中，在fsync到disk中；\n\n那么问题来了，这三步，对于mysql来说，究竟哪一步才算做redolog写入成功了呢？\n\n是写到redologbuffer就行了，还是必须要fsync之后才行呢？\n\n其实这是通过一个参数配置的：innodb_flush_log_at_trx_commit\n\n- 当innodb_flush_log_at_trx_commit=0的时候，表示每次事务提交都只留在redologbuffer中；\n- 当innodb_flush_log_at_trx_commit=1的时候，表示每次事务提交都会fsync持久化到磁盘中；\n- 当innodb_flush_log_at_trx_commit=2的时候，表示每次事务提交都只是write到文件系统的pagecache中；\n\n公司的生产环境，配置的是：innodb_flush_log_at_trx_commit = 1\n\n```sql\n-- redolog的刷盘配置\nshow variables like 'innodb_flush_log_at_trx_commit';\n```\n\n\n\nredologbuffer有多大？超过了怎么办呢？\n\n- redologbuffer有多大是通过innodb_log_buffer_size来控制的。show viriables like 'innodb_log_buffer_size'\n- 超过了怎么办？不会超过的，为什么呢？\n  - 当redologbuffer中存的内容超过innodb_log_buffer_size一半的时候，就会触发write到文件系统的pagecache中\n  - 所以不会超过的。\n\n\n\n### binlog的写入流程\n\n```mermaid\nsequenceDiagram\ninnodb->>server:当dml成功\\n此时redolog处于prepare状态\nserver->>server:开始记录binlog\nserver->>binlogcache:将binlog记录到binlogcache中\nbinlogcache->>binlogcache:binlog cache\\n是在内存中的\\n每个线程私有的\nbinlogcache->>pagecache:将binlog写入到操作系统的pagecache中（write）\npagecache->>disk:数据持久化到磁盘（fsync）\\n这一步占用IOPS\n```\n\n我们看到binlog写盘是有三步的，先写binlogcache，在write到pagecache中，在fsync到disk中；\n\n那么问题来了，这三步，对于mysql来说，究竟哪一步才算做binlog写入成功了呢？\n\n是写到binlogcache就行了，还是必须要fsync之后才行呢？\n\n其实这是通过一个参数配置的：sync_binlog\n\n- sync_binlog=0的时候，表示每次事务提交都只write，不fsync\n- sync_binlog=1的时候，表示每次事务提交都会执行fsync\n- sync_binlog=N（N>1）的时候，表示累计到N个事务之后，才fsync\n\n公司的生产环境，配置的是：sync_binlog = 1\n\n```sql\n-- binlog的刷盘配置\nshow variables like 'sync_binlog';\n```\n\n\n\nbinlogcache有多大呢？超过了怎么办呢？\n\n- show viriables like 'binlog_cache_size'；表示单个线程内 binlog cache 所占内存的大小\n- 超过了会刷盘，但是并不是写到binlog文件中，而是暂存到磁盘中，写在临时文件中；\n\n为什么binlogcache是线程私有的？\n\n- 因为一个线程是一个事务，我们要保证一个事务的binlog是完整的，中间不能插入其他的binlog，所以binlogcache是线程私有的\n\n\n\n### redolog和binlog的刷盘时机\n\n**redolog的刷盘时机**\n\n考虑一个极端的情况，当我们把innodb_flash_log_at_trx_commit设置为0的时候，此时redolog只会写到redologbuffer（redologbuffer是在mysql的内存中的），那么什么时候刷盘呢？\n\n- 刷盘时机1：InnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。\n- 刷盘时机2：当redologbuffer中的内容占用超过redolog_buffer_size大小的一半时候，后台线程会主动写盘\n- 刷盘时机3：当并行的其他事务提交的时候，会将redologbuffer中的所有内容都刷盘；\n  - 对于时机3：不能将innodb_flash_log_at_trx_commit配置设置为0，因为这个时候，事务提交的时候不会刷盘\n  - 对于时机3：可能会把进行汇总的事务的redolog进行刷盘，会有问题吗？\n    - 不会，这个和崩溃恢复的流程有关。此时redolog是prepare阶段的，要想恢复的话，还得去找binlog呢。\n\n**binlog的刷盘时机**\n\nbinlog会有刷盘时机吗？sync_binlog不管设置成多少，都至少保证了binlog会写到文件系统的pagecache中，接下来就是操作系统的范畴了。\n\n详细的说，binlog的刷盘是在 “二阶段三步骤” 的第二步骤 sync state 中\n\n\n\n### 在两阶段提交的不同时刻，MySQL 异常重启会出现什么现象。\n\n在讨论这个问题的时候，简化一下二阶段提交，只看最基本的情况。\n\n```mermaid\ngraph TD;\nid1([写入binlog处于prepare阶段])--时刻A-->id2([写binlog]);\nid2--时刻B-->id3([提交事务处于comit阶段])\n\n```\n\n\n\n**时刻A**\n\n就是写入 redo log 处于 prepare 阶段之后、写 binlog 之前，发生了崩溃（crash）由于此时 binlog 还没写，redo log 也还没提交，所以崩溃恢复的时候，这个事务会回滚。这时候，binlog 还没写，所以也不会传到备库。到这里，大家都可以理解。\n\n**时刻B**\n\n就是 binlog 写完，redo log 还没 commit 前发生 crash，那崩溃恢复的时候 MySQL 会怎么处理呢？\n\n- 如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交；\n- 如果 redo log 里面的事务只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整：\n  - a. 如果是，则提交事务；\n  - b. 否则，回滚事务。\n\n\n\n## 组提交（双1配置，LSN）\n\n\n\nmysql5.7关于组提交的源码：[点我跳转到github](https://github.com/mysql/mysql-server/blob/5.7/sql/binlog.cc)\n\nmysql8.0关于组提交的源码：[点我跳转到github](https://github.com/mysql/mysql-server/blob/8.0/sql/binlog.cc)\n\n关于组提交，这篇文章是我见过讲的最好的一篇：https://zhuanlan.zhihu.com/p/567154450\n\n这篇文章里面有一些图，可以帮助更好的理解：https://blog.51cto.com/u_15080021/2642167\n\n还有这一篇，也挺不错的：[mysql - MySQL 2PC & Group Commit](https://segmentfault.com/a/1190000014810628)\n\n### 双1配置\n\n我们看到mysql默认会把innodb_flash_log_at_trx_commit设置为1，sync_binlog设置为1，这就是通常我们所说的 MySQL 的**“ 双 1 ”配置**。\n\n也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog。\n\n那么问题来了，如果mysql的TPS是2万，也就意味着每秒要写4万次磁盘，但是单独测试磁盘的IOPS，也就在2万左右，怎么能承受住mysql的2万的TPS呢？\n\n这个时候就用到了组提交（group commit）\n\n### 日志逻辑序列号（LSN）\n\n在介绍组提交之前，需要先了解日志逻辑序列号（log sequence number）LSN\n\n#### 什么是LSN\n\n- 每一个redolog的生成，都伴随着LSN的生成\n- LSN是单调递增的\n- LSN是用来对应redolog的一个一个的写入点（write pos）\n- 每当写入长度为length的redolog，LSN的值就会加上length\n\n#### flushed_to_disk_lsn\n\n已经刷到磁盘的LSN\n\n系统第一次启动时，该变量的值和初始的`lsn`值是相同的。随着系统的运行，`redo`日志被不断写入`log buffer`，但是并不会立即刷新到磁盘，`lsn`的值就和`flushed_to_disk_lsn`的值拉开了差距。\n\n如果两者的值相同时，说明log buffer中的所有redo日志都已经刷新到磁盘中了。\n\n#### current_flush_lsn\n\n当前正在刷的LSN\n\n#### LSN有什么用\n\n- 减少redolog组提交时候的刷盘次数，减少磁盘IO（看后面的组提交有详细的说明）\n\n### 组提交\n\n简单的记忆就是：两个阶段，三个步骤\n\n经过 5.6/5.7/8.0 的逐步优化，两阶段提交的逻辑优化为：\n\n- Prepare 阶段：只是将`redolog`写入`pagecache`（并不刷盘）。\n- Commit 阶段按步骤做流水线批处理，拆为三个步骤：\n  - flush stage：`redo log` 刷盘（多个事务 `redo log`合并刷盘），按事务进入的顺序将 `binlog`  写入`pagecache`（并不刷盘）。\n  - sync stage：对 binlog 刷盘（多个事务的 binlog 合并刷盘）。\n  - commit stage：各个线程按顺序做 InnoDB commit 操作。\n\n三个步骤（stage）中，每个 stage 一个队列，第一个进入该队列的线程成为 leader，后续进入的线程会作为follower，并且一直阻塞直至leader完成提交（sql语句会阻塞）。leader 线程会领导队列中的所有线程执行该 stage 的任务，并带领所有 follower 进入到下一个 stage 去执行，当遇到下一个 stage 队列不为空的时候，leader 会变成 follower 注册到此队列中。\n\n\n\n```mermaid\nsequenceDiagram\nserver->>innodb:更新数据\nnote left of innodb:第一阶段\ninnodb->>innodb:记录redolog\ninnodb->>pagecache:redolog write 事务处于prepare\npagecache->>innodb:write 成功\ninnodb->>server:redolog prepare 成功\nnote left of innodb:第二阶段第一步骤\npagecache->>disk:redolog刷盘(【组提交】)\nserver->>server:记录binlog\nserver->>pagecache:binlog写到文件系统的cache\npagecache->>server:binlog 成功\nnote left of disk:第二阶段第二步骤\npagecache->>disk:binlog刷盘（【组提交】）\nnote left of innodb:第二阶段第三步骤\nserver->>innodb:提交事务\ninnodb->>innodb:事务处于commit\ninnodb->>server:更新完成\n```\n\n#### 第二阶段的第一步骤中，是先写binlog还是先刷redolog？\n\n我看了mysql5.7的源码，其实是先刷的redolog，但是mysql45讲说的是先写binlog，在刷redolog。（老师讲错了）\n\nmysql5.7关于组提交的源码：[点我跳转到github](https://github.com/mysql/mysql-server/blob/5.7/sql/binlog.cc)\n\nmysql8.0关于组提交的源码：[点我跳转到github](https://github.com/mysql/mysql-server/blob/8.0/sql/binlog.cc)\n\n第二步骤的第一阶段（就是FLASH_STAGE）的代码如下（mysql5.7）\n\n```c\n\n/**\n  Execute the flush stage.\n  @param total_bytes_var Pointer to variable that will be set to total\n  number of bytes flushed, or NULL.\n  @param rotate_var Pointer to variable that will be set to true if\n  binlog rotation should be performed after releasing locks. If rotate\n  is not necessary, the variable will not be touched.\n  @return Error code on error, zero on success\n */\n\nint\nMYSQL_BIN_LOG::process_flush_stage_queue(my_off_t *total_bytes_var,\n                                         bool *rotate_var,\n                                         THD **out_queue_var)\n{\n  DBUG_ENTER(\"MYSQL_BIN_LOG::process_flush_stage_queue\");\n  #ifndef NDEBUG\n  // number of flushes per group.\n  int no_flushes= 0;\n  #endif\n  assert(total_bytes_var && rotate_var && out_queue_var);\n  my_off_t total_bytes= 0;\n  int flush_error= 1;\n  mysql_mutex_assert_owner(&LOCK_log);\n\n  /*\n    Fetch the entire flush queue and empty it, so that the next batch\n    has a leader. We must do this before invoking ha_flush_logs(...)\n    for guaranteeing to flush prepared records of transactions before\n    flushing them to binary log, which is required by crash recovery.\n  */\n  THD *first_seen= stage_manager.fetch_queue_for(Stage_manager::FLUSH_STAGE);\n  assert(first_seen != NULL);\n  /*\n    We flush prepared records of transactions to the log of storage\n    engine (for example, InnoDB redo log) in a group right before\n    flushing them to binary log. \n    这段注释表示，在写binlog之前需要先把redolog刷盘\n    ha_flush_logs(NULL, true);这个方法就是对redolog刷盘\n  */\n  ha_flush_logs(NULL, true);\n  DBUG_EXECUTE_IF(\"crash_after_flush_engine_log\", DBUG_SUICIDE(););\n  assign_automatic_gtids_to_flush_group(first_seen);\n  /* \n  \tFlush thread caches to binary log. \n  \t这段注释表示，开始 write binlog【这里不明白的是，为什么也叫flush？】\n  */\n  for (THD *head= first_seen ; head ; head = head->next_to_commit)\n  {\n    std::pair<int,my_off_t> result= flush_thread_caches(head);\n    total_bytes+= result.second;\n    if (flush_error == 1)\n      flush_error= result.first;\n#ifndef NDEBUG\n    no_flushes++;\n#endif\n  }\n\n  *out_queue_var= first_seen;\n  *total_bytes_var= total_bytes;\n  if (total_bytes > 0 && my_b_tell(&log_file) >= (my_off_t) max_size)\n    *rotate_var= true;\n#ifndef NDEBUG\n  DBUG_PRINT(\"info\",(\"no_flushes:= %d\", no_flushes));\n  no_flushes= 0;\n#endif\n  DBUG_RETURN(flush_error);\n}\n\n```\n\n\n\n#### 组提交“组”在了哪里\n\n在第二阶段的第一步骤中，redolog进行了组提交刷盘\n\n在第二阶段的第二步骤中，binlog进行组提交刷盘\n\n那么具体是怎么“`组`”提交的呢？\n\n1、在第一阶段，事务线程不停地，刷刷刷的进来，写redolog，此时只写到pagecache中；\n\n2、在第二阶段的第一步骤中，有一个队列，假设叫【队列-1】，\n\n- 当前线程会先把【队列-1】清空，以便下一批有一个leader\n- 当前线程中直接对redolog进行刷盘。ha_flush_logs(NULL, true);\n- 第一个进来的线程作为leader，后续的线程作为follower，进入【队列-1】（也就是说队列-1和刷redolog并没有啥关系）\n\n3、redolog刷盘完成之后，leader对【队列-1】中的事务线程，进行循环，write binlog\n\n4、write bin完成之后，【队列-1】的leader进入第二阶段的第二步骤，会进入到【队列-2】\n\n5、在第二阶段的第二步骤中，【队列-2】的leader会等待（受**binlog_group_commit_sync_delay** 和 **binlog_group_commit_sync_no_delay_count**控制）\n\n6、等待之后，【队列-2】的leader开始对binlog进行刷盘（因为等待了一段时间，所以binlog这里也是组提交）\n\n7、fsync binlog之后，【队列-2】的leader进入第二阶段的第三步骤，会进入到【队列-3】\n\n8、【队列-3】的leader会按照串行化的方式，循环，一个一个的对事务线程进行COMMIT\n\n\n\n#### 组提交和LSN有啥关系\n\n首先明确：\n\n- 在innodb中，每条redolog都有自己的LSN，这是一个单调递增的值。\n- 每个事务的更新操作都会包含一条或者**多条**redo log\n- 各个事务在将redo log写入 redo log buffer (通过log_mutex保护)时，都会获取**当前事务**最大的LSN。\n\n在组提交的第二阶段的第一步骤中，redolog会被组提交刷盘，组提交刷盘的时候，会有下面的流程\n\n那么假设三个事务 tx1, tx2, tx3的最大LSN分别为 100 , 200 , 300  时，他们同时进行提交，如果tx3获取到了 log_mutex 互斥锁, 那么他会将小于 300 之前的redo log一起落盘，同时记录  **flushed_to_disk_lsn**=300， 这样 tx1, tx2不用再次请求磁盘io。\n\n同时，如果存在 tx0 的 LSN0 < 300，LSN0 也会落盘，即使tx0还没有提交。然后当tx0的事务开始提交的时候，发现redolog已经刷盘了（ flushed_to_disk_lsn >= lsn），就直接返回了，节省了时间。\n\n1. 获取 log mutex互斥锁\n2. 如果 flushed_to_disk_lsn >= lsn, 表示日志已经被刷盘，跳转 5 后进入等待状态\n3. 如果 current_flush_lsn >= lsn, 表示日志正在刷盘中，跳转 5 后进入等待状态\n4. 将小于 lsn 的日志刷盘 (sync)\n5. 释放 log_mutex互斥锁\n\n\n\n\n\n## mysql的崩溃恢复Crash-Safe能力（重要作用）\n\n\n\n### 崩溃恢复的具体步骤\n\n1、mysql崩溃重启后，进行恢复\n\n2、判断redolog的状态，如果redolog=commit，直接提交事务\n\n3、如果redolog=prepare，则通过xid去找binlog\n\n4、binlog存在，并且binlog是完整的，提交事务\n\n5、binlog不存在，或者binlog存在，但是不完整，回滚事务\n\n\n\n### MySQL 怎么知道 binlog 是完整的\n\n回答：一个事务的 binlog 是有完整格式的：\n\n- statement 格式的 binlog，最后会有 COMMIT；\n- row 格式的 binlog，最后会有一个 XID event。\n\n另外，在 MySQL 5.6.2 版本以后，还引入了 binlog-checksum 参数，用来验证 binlog 内容的正确性。对于 binlog 日志由于磁盘原因，可能会在日志中间出错的情况，MySQL 可以通过校验 checksum 的结果来发现。所以，MySQL 还是有办法验证事务 binlog 的完整性的。\n\n可以通过下面的命令查看`binlog-checksum`的值\n\n```sql\nmysql> show variables like '%binlog_checksum%';\n+-----------------+-------+\n| Variable_name   | Value |\n+-----------------+-------+\n| binlog_checksum | CRC32 |\n+-----------------+-------+\n1 row in set, 1 warning (0.00 sec)\n```\n\n这是`mysql 8.0`版本的默认值，就是CRC32，它有什么用呢，在ROW格式下的binlog，通过`mysqlbinlog`工具可以看到具体的内容\n\n```sh\n............省略\n# at 5299\n#221101 15:21:11 server id 1  end_log_pos 5330 CRC32 0xc30d2901         Xid = 1824\n............省略\n```\n\n这个`CRC32 0xc30d2901`内容就是CRC32的值，用来校验binlog的完整性。\n\n\n\n### redo log 和 binlog 是怎么关联起来的\n\n它们有一个共同的数据字段，叫 XID。崩溃恢复的时候，会按顺序扫描 redo log：\n\n- 如果碰到既有 prepare、又有 commit 的 redo log，就直接提交；\n- 如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。\n\n\n\n### 如果只有binlog可以Crash-Safe吗？\n\n不行，因为binlog是逻辑日志，binlog写完了，并不保证数据一定在磁盘中。\n\n在崩溃恢复的时候，顺序扫描binlog，发现日志是完整的，那么此时要执行这个binlog吗？\n\n- 执行：如果数据已经写到了磁盘，又执行一遍，那数据就不对了呀。\n- 不执行：如果数据没写到磁盘，不执行，数据就少了呀。\n\n综上，只用binlog是不可以的。\n\n\n\n\n\n### 如果只有redolog可以Crash-Safe吗？\n\n如果只从崩溃恢复的角度来讲是可以的。你可以把 binlog 关掉，这样就没有两阶段提交了，但系统依然是 crash-safe 的。\n\n但是呢，redolog是循环写的，所以只用redolog，mysql虽然可以crash-safe，但是不具备归档的能力了。\n\n\n\n## 刷脏页\n\n### 正常运行中的实例，数据写入后的最终落盘，是从 redo log 更新过来的还是从 buffer pool 更新过来的呢？\n\n这里涉及到了，“redo log 里面到底是什么”的问题。\n\n实际上，redo log 并没有记录数据页的完整数据，所以它并没有能力自己去更新磁盘数据页，也就不存在“数据最终落盘，是由 redo log 更新过去”的情况。\n\n- 如果是正常运行的实例的话，数据页被修改以后，跟磁盘的数据页不一致，称为脏页。最终数据落盘，就是把**内存中的数据页**写盘。这个过程，甚至与 redo log 毫无关系。\n- 在崩溃恢复场景中，InnoDB 如果判断到一个数据页可能在崩溃恢复的时候丢失了更新，就会将这个数据页加载到内存中，然后让 redo log 更新内存中的数据页内容。更新完成后，内存页变成脏页，就回到了第一种情况的状态。\n- 在正常运行的实例中，change buffer中的数据也会定期merge到**内存中的数据页**中，然后刷盘\n- 猜测\n  - 正常运行的实例：是 change buffer --> 内存中的数据页 --> 刷盘；\n  - 崩溃恢复的场景：此时change buffer都没了。 所以是：redolog --> 内存中的数据页 --> 刷盘\n\n\n\n\n## changebuffer（对更新的优化，对比redolog）\n\n### changebuffer的更新流程\n\n每一次更新都必须从内存中（不在内存中，就要从磁盘中load）获取到要更新的这一行吗？\n\n- 不是的；\n- 在mysql5.5之前，changebuffer叫做insert buffer，仅支持插入，在5.5之后，叫change buffer，支持了更新和删除；\n- changebuffer只有普通索引才能用到，因为唯一索引要判定记录是否存在，所以查询一定要的\n\n```mermaid\nsequenceDiagram\nclient->>server:执行dml语句\nserver->>innodb:要执行dml语句\ninnodb->>innodb:判断要执行的语句\\n是否在内存中\ninnodb->>cache:在内存中，直接操作内存\ncache->>innodb:操作完成，返回\ninnodb->>server:操作完成，返回\nserver->>client:操作完成\ninnodb->>changebuffer:不在内存中，记录到change buffer\nchangebuffer->>innodb:操作完成，返回\ninnodb->>server:操作完成，返回\nserver->>client:操作完成\nnote over client,disk:记录到缓存中，什么时候刷到磁盘呢？\nchangebuffer->>disk:后台线程定时刷\nchangebuffer->>disk:changebuffer不足时\nchangebuffer->>disk:数据库正常关闭时\nchangebuffer->>disk:redolog写满时\\n（此时数据库不可用）\nnote over client,disk:记录在缓存中，查询的时候怎么办\nclient->>server:请求查询某一条记录\nserver->>innodb:查询某一条记录\ninnodb->>innodb:判断要查询的语句\\n是否在内存中\ncache->>innodb:在内存中，直接返回\ninnodb->>server:操作完成，返回\nserver->>client:操作完成\ndisk->>cache:不在内存中，从disk加载到内存中\ncache->>innodb:获取内存中的这一条记录\ninnodb->>innodb:判断这一条记录是否有更新\ninnodb->>server:没有更新\\n操作完成，返回\nchangebuffer->>innodb:有更新，应用changebuffer的更新\\n这一步叫merge\ninnodb->>cache:将更新后的记录先记录到内存页中\ninnodb->>server:操作完成，返回\nserver->>client:操作完成\n\n```\n\n### 普通索引和唯一索引的更新流程\n\n在普通索引的更新流程（可以用changebuffer）\n\n```mermaid\nsequenceDiagram\nclient->server:执行dml语句\nserver->>innodb:要执行dml语句\ninnodb->>innodb:判断要执行的语句\\n是否在内存中\ninnodb->>cache:在内存中，直接操作内存\ncache->>innodb:操作完成，返回\ninnodb->>server:操作完成，返回\nserver->>client:操作完成\ninnodb->>changebuffer:不在内存中，记录到change buffer\nchangebuffer->>innodb:操作完成，返回\ninnodb->>server:操作完成，返回\nserver->>client:操作完成\n\n```\n\n在唯一索引的更新流程（不可以用changebuffer）\n\n```mermaid\nsequenceDiagram\nclient->server:执行dml语句\nserver->>innodb:要执行dml语句\ninnodb->>innodb:判断要执行的语句\\n是否在内存中\ninnodb->>cache:在内存中，直接操作内存\ncache->>innodb:操作完成，返回\ninnodb->>server:操作完成，返回\nserver->>client:操作完成\ninnodb->>disk:不在内存中，从磁盘中加载到内存\ndisk->>cache:从磁盘中加载到内存\ncache->>innodb:操作完成，返回\ninnodb->>server:操作完成，返回\nserver->>client:操作完成\n```\n\n\n\n### change buffer 和 redo log\n\nredo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。\n\n\n\n\n\n## 一些简单的问题\n\n执行一个 update 语句以后，我再去执行 hexdump 命令直接查看 ibd 文件内容，为什么没有看到数据有改变呢？\n\n\n\n为什么 binlog cache 是线程私有的，而 redo log buffer 是全局共用的？\n\n\n\n事务执行期间，还没到提交阶段，如果发生 crash 的话，redo log 肯定丢了，这会不会导致主备不一致呢？\n\n\n\n如果 binlog 写完盘以后发生 crash，这时候还没给客户端答复就重启了。等客户端再重连进来，发现事务已经提交成功了，这是不是 bug？\n\n","tags":["mysql","binlog","redolog"],"categories":["JAVA","数据库","MYSQL"]},{"title":"mysql的事务从入门到入土","url":"/note/JAVA/数据库/MYSQL/mysql的事务从入门到入土/","content":"\n\n\n\n\n\\#### 事务相关 * \n\n03 | 事务隔离：为什么你改了我还看不见？ * \n\n08 | 事务到底是隔离的还是不隔离的？ * \n\n20 | 幻读是什么，幻读有什么问题？\n\n\n\n\n\nundo log\n\nundolog只会记录 insert delete update 的语句，DDL不会记录undo log；这个和undo log 的作用有关\n\n以下摘录自官网：[14.6.7 Undo Logs](https://dev.mysql.com/doc/refman/5.7/en/innodb-undo-logs.html)\n\n```\nA transaction is assigned up to four undo logs, one for each of the following operation types:\n\n\tINSERT operations on user-defined tables\n\n\tUPDATE and DELETE operations on user-defined tables\n\n\tINSERT operations on user-defined temporary tables\n\n\tUPDATE and DELETE operations on user-defined temporary tables\n```\n\n\n\n","tags":["事务","mysql","MVCC","幻读","隔离级别"],"categories":["JAVA","数据库","MYSQL"]},{"title":"mysql的临时表从入门到入土","url":"/note/JAVA/数据库/MYSQL/mysql的临时表从入门到入土/","content":"\n\n\n\n\nmysql临时表的xmind，[点击下载]()\n\n\n\n\\#### 临时表相关 \n\n* 17 | 如何正确地显示随机消息？ \n* 34 | 到底可不可以使用join？ \n* 35 | join语句怎么优化？ \n* 36 | 为什么临时表可以重名？\n* 37 | 什么时候会使用内部临时表？\n* 43 | 要不要使用分区表？\n\n\n\n\n\n\n\n\n\n\n\n\n\nmysql\n\n\n\n\n\n临时文件和临时表\n\n\n\n临时表：为什么这里的临时表使用的引擎是memory？ A：tmp_table_size 这个配置限制了内存临时表的大小，默认值是 16M。如果临时表大小超过了 tmp_table_size，那么内存临时表就会转成磁盘临时表；磁盘临时表使用的引擎默认是 InnoDB，是由参数 internal_tmp_disk_storage_engine 控制的。而内存临时表使用的是memory引擎\n\n\n\n\n\n\n\n```sql\n-- 创建表t2，主键索引和字段a的普通索引\nCREATE TABLE `t2` (\n  `id` int(11) NOT NULL,\n  `a` int(11) DEFAULT NULL,\n  `b` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `a` (`a`)\n) ENGINE=InnoDB;\n\n-- 通过存储过程，初始化1000条记录\ndrop procedure idata;\ndelimiter ;;\ncreate procedure idata()\nbegin\n  declare i int;\n  set i=1;\n  while(i<=1000)do\n    insert into t2 values(i, i, i);\n    set i=i+1;\n  end while;\nend;;\ndelimiter ;\ncall idata();\n\n-- 创建表t1，主键索引和字段a的普通索引\n-- 初始化100条记录\ncreate table t1 like t2;\ninsert into t1 (select * from t2 where id<=100)\n```\n\n\n\n#### 驱动表使用索引字段关联；被驱动表使用索引关联；查所有字段\n\n```sql\nmysql> explain select * from t1 straight_join t2 on t1.a = t2.a;\n+----+-------------+-------+------------+------+---------------+------+---------+---------+------+----------+-------------+\n| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref     | rows | filtered | Extra       |\n+----+-------------+-------+------------+------+---------------+------+---------+---------+------+----------+-------------+\n|  1 | SIMPLE      | t1    | NULL       | ALL  | a             | NULL | NULL    | NULL    |  100 |   100.00 | Using where |\n|  1 | SIMPLE      | t2    | NULL       | ref  | a             | a    | 5       | zs.t1.a |    1 |   100.00 | NULL        |\n+----+-------------+-------+------------+------+---------------+------+---------+---------+------+----------+-------------+\n2 rows in set, 1 warning (0.00 sec)\n```\n\n为什么驱动表的字段a有索引，为啥不走索引呢？\n\n- 原因可能是因为没有过滤条件；\n\n#### 驱动表使用索引字段关联；被驱动表使用索引关联；查所有字段；驱动表添加过滤条件\n\n```sql\nmysql> explain select * from t1 straight_join t2 on t1.a = t2.a where t1.a > 50;\n+----+-------------+-------+------------+------+---------------+------+---------+---------+------+----------+-------------+\n| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref     | rows | filtered | Extra       |\n+----+-------------+-------+------------+------+---------------+------+---------+---------+------+----------+-------------+\n|  1 | SIMPLE      | t1    | NULL       | ALL  | a             | NULL | NULL    | NULL    |  100 |    50.00 | Using where |\n|  1 | SIMPLE      | t2    | NULL       | ref  | a             | a    | 5       | zs.t1.a |    1 |   100.00 | NULL        |\n+----+-------------+-------+------------+------+---------------+------+---------+---------+------+----------+-------------+\n2 rows in set, 1 warning (0.00 sec)\n```\n\n为什么驱动表的字段a有索引，而且添加了过滤条件，还是不走索引呢？\n\n- 因为是查询的 * ，即使走了索引 a，还是需要回表，优化器认为还不如直接走全表呢\n- 因为过滤条件过滤的数据比较少，只过滤了50%，优化器认为全表扫描比走索引更快\n\n\n\n#### 驱动表使用索引字段关联；被驱动表使用索引关联；查驱动表的主键id；驱动表添加过滤条件\n\n```sql\nmysql> explain select t1.id from t1 straight_join t2 on t1.a = t2.a where t1.a > 50;\n+----+-------------+-------+------------+-------+---------------+------+---------+---------+------+----------+--------------------------+\n| id | select_type | table | partitions | type  | possible_keys | key  | key_len | ref     | rows | filtered | Extra                    |\n+----+-------------+-------+------------+-------+---------------+------+---------+---------+------+----------+--------------------------+\n|  1 | SIMPLE      | t1    | NULL       | range | a             | a    | 5       | NULL    |   50 |   100.00 | Using where; Using index |\n|  1 | SIMPLE      | t2    | NULL       | ref   | a             | a    | 5       | zs.t1.a |    1 |   100.00 | Using index              |\n+----+-------------+-------+------------+-------+---------------+------+---------+---------+------+----------+--------------------------+\n2 rows in set, 1 warning (0.00 sec)\n```\n\n驱动表的字段a有索引，而且添加了过滤条件，查询驱动表的主键id，会用到覆盖索引，不回表了，所以走了索引；\n\n\n\n#### 驱动表使用索引字段关联；被驱动表使用索引关联；查所有字段；驱动表添加过滤条件，大范围过滤\n\n```sql\nmysql> explain select * from t1 straight_join t2 on t1.a = t2.a where t1.a > 90;\n+----+-------------+-------+------------+-------+---------------+------+---------+---------+------+----------+-----------------------+\n| id | select_type | table | partitions | type  | possible_keys | key  | key_len | ref     | rows | filtered | Extra                 |\n+----+-------------+-------+------------+-------+---------------+------+---------+---------+------+----------+-----------------------+\n|  1 | SIMPLE      | t1    | NULL       | range | a             | a    | 5       | NULL    |   10 |   100.00 | Using index condition |\n|  1 | SIMPLE      | t2    | NULL       | ref   | a             | a    | 5       | zs.t1.a |    1 |   100.00 | NULL                  |\n+----+-------------+-------+------------+-------+---------------+------+---------+---------+------+----------+-----------------------+\n2 rows in set, 1 warning (0.00 sec)\n```\n\n驱动表的字段a有索引，而且添加了过滤条件，虽然查询的是所有字段，需要回表，但是因为过滤条件过滤90%的数据，优化器因为回表比全表扫描代价低\n\n\n\n以上的情况适用于 t1 的单表查询也是一样的，所以总结下来 join查询其实就是多个单表查询，然后汇总在一起；\n\n```sql\n-- ALL\nexplain select * from t1;\n-- ALL\nexplain select * from t1 where t1.a > 50;\n-- range\nexplain select * from t1 where t1.a > 90;\n-- range\nexplain select id from t1 where t1.a > 50;\n```\n\n\n\n\n\nmysql5\n\n- Using where; Using join buffer (Block Nested Loop)\n\n```sql\nmysql> explain select * from t1 straight_join t2 on t1.a = t2.b;\n+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+\n| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra                                              |\n+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+\n|  1 | SIMPLE      | t1    | NULL       | ALL  | a             | NULL | NULL    | NULL |  100 |   100.00 | NULL                                               |\n|  1 | SIMPLE      | t2    | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 1000 |    10.00 | Using where; Using join buffer (Block Nested Loop) |\n+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+\n2 rows in set, 1 warning (0.06 sec)\n```\n\n\n\nmysql8\n\n- Using where; Using join buffer (hash join)\n\n```sql\nmysql> explain select * from t1 straight_join t2 on t1.a = t2.b;\n+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------------------+\n| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra                                      |\n+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------------------+\n|  1 | SIMPLE      | t1    | NULL       | ALL  | a             | NULL | NULL    | NULL |  100 |   100.00 | NULL                                       |\n|  1 | SIMPLE      | t2    | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 1000 |    10.00 | Using where; Using join buffer (hash join) |\n+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------------------+\n2 rows in set, 1 warning (0.00 sec)\n```\n\n\n\n\n\n使用order by的话，就不会使用MRR了\n\n```sql\nmysql> set optimizer_switch=\"mrr_cost_based=off\";\nQuery OK, 0 rows affected (0.01 sec)\nmysql> explain select * from t2 where a >= 100 and a<=200;\n+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+----------------------------------+\n| id | select_type | table | partitions | type  | possible_keys | key  | key_len | ref  | rows | filtered | Extra                            |\n+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+----------------------------------+\n|  1 | SIMPLE      | t2    | NULL       | range | a             | a    | 5       | NULL |  101 |   100.00 | Using index condition; Using MRR |\n+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+----------------------------------+\n1 row in set, 1 warning (0.00 sec)\n\nmysql> explain select * from t2 where a >= 100 and a<=200 order by a asc;\n+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-----------------------+\n| id | select_type | table | partitions | type  | possible_keys | key  | key_len | ref  | rows | filtered | Extra                 |\n+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-----------------------+\n|  1 | SIMPLE      | t2    | NULL       | range | a             | a    | 5       | NULL |  101 |   100.00 | Using index condition |\n+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-----------------------+\n1 row in set, 1 warning (0.01 sec)\n```\n\n\n\n\n\n\n\n\n\nmysql的临时表存放位置\n\n在 5.6 以及之前的版本里，MySQL 会在临时文件目录下创建一个相同前缀、以.ibd 为后缀的文件，用来存放数据文件；\n\n```sql\nmysql> show variables like 'tmpdir';\n+---------------+-------------------------------------------------+\n| Variable_name | Value                                           |\n+---------------+-------------------------------------------------+\n| tmpdir        | C:\\Windows\\SERVIC~1\\NETWOR~1\\AppData\\Local\\Temp |\n+---------------+-------------------------------------------------+\n1 row in set, 1 warning (0.00 sec)\n```\n\n\n\n- 其中 C:\\Windows\\SERVIC~1\\NETWOR~1\\AppData\\Local\\Temp  就是临时文件目录，其中的 SERVIC~1\\NETWOR~1 是啥意思，我不知道\n- 我是win电脑，可以创建一个临时表，然后用everything搜一下，就能找到这个目录\n\n而从 5.7 版本开始，MySQL 引入了一个临时文件表空间，专门用来存放临时文件的数据。因此，我们就不需要再创建 ibd 文件了。\n\n```sql\nmysql> show variables like 'innodb_temp_data_file_path';\n+----------------------------+-----------------------+\n| Variable_name              | Value                 |\n+----------------------------+-----------------------+\n| innodb_temp_data_file_path | ibtmp1:12M:autoextend |\n+----------------------------+-----------------------+\n1 row in set, 1 warning (0.00 sec)\n```\n\n- innodb_temp_data_file_path：定义临时表空间的路径、文件名、初始化大小和最大上限。\n- 其中`ibtmp1`就是临时文件表空间，它在windows下是一个文件，不是一个目录。\n\n![image-20230105211938528](mysql的临时表从入门到入土.assets/image-20230105211938528.png)\n\n\n\n\n\n\n\n\n\n# 分区表的底层原理\n\n​\t\t分区表由多个相关的底层表实现，这个底层表也是由句柄对象标识，我们可以直接访问各个分区。存储引擎管理分区的各个底层表和管理普通表一样（所有的底层表都必须使用相同的存储引擎），分区表的索引知识在各个底层表上各自加上一个完全相同的索引。从存储引擎的角度来看，底层表和普通表没有任何不同，存储引擎也无须知道这是一个普通表还是一个分区表的一部分。\n\n​\t\t分区表的操作按照以下的操作逻辑进行：\n\n​\t\t**select查询**\n\n​\t\t当查询一个分区表的时候，分区层先打开并锁住所有的底层表，优化器先判断是否可以过滤部分分区，然后再调用对应的存储引擎接口访问各个分区的数据\n\n​\t\t**insert操作**\n\n​\t\t当写入一条记录的时候，分区层先打开并锁住所有的底层表，然后确定哪个分区接受这条记录，再将记录写入对应底层表\n\n​\t\t**delete操作**\n\n​\t\t当删除一条记录时，分区层先打开并锁住所有的底层表，然后确定数据对应的分区，最后对相应底层表进行删除操作\n\n​\t\t**update操作**\n\n​\t\t当更新一条记录时，分区层先打开并锁住所有的底层表，mysql先确定需要更新的记录再哪个分区，然后取出数据并更新，再判断更新后的数据应该再哪个分区，最后对底层表进行写入操作，并对源数据所在的底层表进行删除操作\n\n​\t\t有些操作时支持过滤的，例如，当删除一条记录时，MySQL需要先找到这条记录，如果where条件恰好和分区表达式匹配，就可以将所有不包含这条记录的分区都过滤掉，这对update同样有效。如果是insert操作，则本身就是只命中一个分区，其他分区都会被过滤掉。mysql先确定这条记录属于哪个分区，再将记录写入对应得曾分区表，无须对任何其他分区进行操作\n\n​\t\t虽然每个操作都会“先打开并锁住所有的底层表”，但这并不是说分区表在处理过程中是锁住全表的，如果存储引擎能够自己实现行级锁，例如innodb，则会在分区层释放对应表锁。\n\n\n\n# 范围分区\n\n​\t\t范围分区表的分区方式是：每个分区都包含行数据且分区的表达式在给定的范围内，分区的范围应该是连续的且不能重叠，可以使用values less than运算符来定义。\n\n​\t\t1、创建普通的表\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE NOT NULL DEFAULT '9999-12-31',\n    job_code INT NOT NULL,\n    store_id INT NOT NULL\n);\n```\n\n​\t\t2、创建带分区的表，下面建表的语句是按照store_id来进行分区的，指定了4个分区\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE NOT NULL DEFAULT '9999-12-31',\n    job_code INT NOT NULL,\n    store_id INT NOT NULL\n)\nPARTITION BY RANGE (store_id) (\n    PARTITION p0 VALUES LESS THAN (6),\n    PARTITION p1 VALUES LESS THAN (11),\n    PARTITION p2 VALUES LESS THAN (16),\n    PARTITION p3 VALUES LESS THAN (21)\n);\n--在当前的建表语句中可以看到，store_id的值在1-5的在p0分区，6-10的在p1分区，11-15的在p3分区，16-20的在p4分区，但是如果插入超过20的值就会报错，因为mysql不知道将数据放在哪个分区\n```\n\n​\t\t3、可以使用less than maxvalue来避免此种情况\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE NOT NULL DEFAULT '9999-12-31',\n    job_code INT NOT NULL,\n    store_id INT NOT NULL\n)\nPARTITION BY RANGE (store_id) (\n    PARTITION p0 VALUES LESS THAN (6),\n    PARTITION p1 VALUES LESS THAN (11),\n    PARTITION p2 VALUES LESS THAN (16),\n    PARTITION p3 VALUES LESS THAN MAXVALUE\n);\n--maxvalue表示始终大于等于最大可能整数值的整数值\n```\n\n​\t\t4、可以使用相同的方式根据员工的职务代码对表进行分区\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE NOT NULL DEFAULT '9999-12-31',\n    job_code INT NOT NULL,\n    store_id INT NOT NULL\n)\nPARTITION BY RANGE (job_code) (\n    PARTITION p0 VALUES LESS THAN (100),\n    PARTITION p1 VALUES LESS THAN (1000),\n    PARTITION p2 VALUES LESS THAN (10000)\n);\n```\n\n​\t\t5、可以使用date类型进行分区：如虚妄根据每个员工离开公司的年份进行划分，如year(separated)\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE NOT NULL DEFAULT '9999-12-31',\n    job_code INT,\n    store_id INT\n)\nPARTITION BY RANGE ( YEAR(separated) ) (\n    PARTITION p0 VALUES LESS THAN (1991),\n    PARTITION p1 VALUES LESS THAN (1996),\n    PARTITION p2 VALUES LESS THAN (2001),\n    PARTITION p3 VALUES LESS THAN MAXVALUE\n);\n```\n\n​\t\t6、可以使用函数根据range的值来对表进行分区，如timestampunix_timestamp()\n\n```sql\nCREATE TABLE quarterly_report_status (\n    report_id INT NOT NULL,\n    report_status VARCHAR(20) NOT NULL,\n    report_updated TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n)\nPARTITION BY RANGE ( UNIX_TIMESTAMP(report_updated) ) (\n    PARTITION p0 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-01-01 00:00:00') ),\n    PARTITION p1 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-04-01 00:00:00') ),\n    PARTITION p2 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-07-01 00:00:00') ),\n    PARTITION p3 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-10-01 00:00:00') ),\n    PARTITION p4 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-01-01 00:00:00') ),\n    PARTITION p5 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-04-01 00:00:00') ),\n    PARTITION p6 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-07-01 00:00:00') ),\n    PARTITION p7 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-10-01 00:00:00') ),\n    PARTITION p8 VALUES LESS THAN ( UNIX_TIMESTAMP('2010-01-01 00:00:00') ),\n    PARTITION p9 VALUES LESS THAN (MAXVALUE)\n);\n--timestamp不允许使用任何其他涉及值的表达式\n```\n\n基于时间间隔的分区方案，在mysql5.7中，可以基于范围或事件间隔实现分区方案，有两种选择\n\n1、基于范围的分区，对于分区表达式，可以使用操作函数基于date、time、或者datatime列来返回一个整数值\n\n```sql\nCREATE TABLE members (\n    firstname VARCHAR(25) NOT NULL,\n    lastname VARCHAR(25) NOT NULL,\n    username VARCHAR(16) NOT NULL,\n    email VARCHAR(35),\n    joined DATE NOT NULL\n)\nPARTITION BY RANGE( YEAR(joined) ) (\n    PARTITION p0 VALUES LESS THAN (1960),\n    PARTITION p1 VALUES LESS THAN (1970),\n    PARTITION p2 VALUES LESS THAN (1980),\n    PARTITION p3 VALUES LESS THAN (1990),\n    PARTITION p4 VALUES LESS THAN MAXVALUE\n);\n\nCREATE TABLE quarterly_report_status (\n    report_id INT NOT NULL,\n    report_status VARCHAR(20) NOT NULL,\n    report_updated TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n)\nPARTITION BY RANGE ( UNIX_TIMESTAMP(report_updated) ) (\n    PARTITION p0 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-01-01 00:00:00') ),\n    PARTITION p1 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-04-01 00:00:00') ),\n    PARTITION p2 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-07-01 00:00:00') ),\n    PARTITION p3 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-10-01 00:00:00') ),\n    PARTITION p4 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-01-01 00:00:00') ),\n    PARTITION p5 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-04-01 00:00:00') ),\n    PARTITION p6 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-07-01 00:00:00') ),\n    PARTITION p7 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-10-01 00:00:00') ),\n    PARTITION p8 VALUES LESS THAN ( UNIX_TIMESTAMP('2010-01-01 00:00:00') ),\n    PARTITION p9 VALUES LESS THAN (MAXVALUE)\n);\n```\n\n2、基于范围列的分区，使用date或者datatime列作为分区列\n\n```sql\nCREATE TABLE members (\n    firstname VARCHAR(25) NOT NULL,\n    lastname VARCHAR(25) NOT NULL,\n    username VARCHAR(16) NOT NULL,\n    email VARCHAR(35),\n    joined DATE NOT NULL\n)\nPARTITION BY RANGE COLUMNS(joined) (\n    PARTITION p0 VALUES LESS THAN ('1960-01-01'),\n    PARTITION p1 VALUES LESS THAN ('1970-01-01'),\n    PARTITION p2 VALUES LESS THAN ('1980-01-01'),\n    PARTITION p3 VALUES LESS THAN ('1990-01-01'),\n    PARTITION p4 VALUES LESS THAN MAXVALUE\n);\n```\n\n### \t\t真实案例：\n\n```sql\n#不分区的表\nCREATE TABLE no_part_tab\n(id INT DEFAULT NULL,\nremark VARCHAR(50) DEFAULT NULL,\nd_date DATE DEFAULT NULL\n)ENGINE=MYISAM;\n#分区的表\nCREATE TABLE part_tab\n(id INT DEFAULT NULL,\nremark VARCHAR(50) DEFAULT NULL,\nd_date DATE DEFAULT NULL\n)ENGINE=MYISAM\nPARTITION BY RANGE(YEAR(d_date))(\nPARTITION p0 VALUES LESS THAN(1995),\nPARTITION p1 VALUES LESS THAN(1996),\nPARTITION p2 VALUES LESS THAN(1997),\nPARTITION p3 VALUES LESS THAN(1998),\nPARTITION p4 VALUES LESS THAN(1999),\nPARTITION p5 VALUES LESS THAN(2000),\nPARTITION p6 VALUES LESS THAN(2001),\nPARTITION p7 VALUES LESS THAN(2002),\nPARTITION p8 VALUES LESS THAN(2003),\nPARTITION p9 VALUES LESS THAN(2004),\nPARTITION p10 VALUES LESS THAN maxvalue);\n#插入未分区表记录\nDROP PROCEDURE IF EXISTS no_load_part;\n \n\nDELIMITER//\nCREATE PROCEDURE no_load_part()\nBEGIN\n    DECLARE i INT;\n    SET i =1;\n    WHILE i<80001\n    DO\n    INSERT INTO no_part_tab VALUES(i,'no',ADDDATE('1995-01-01',(RAND(i)*36520) MOD 3652));\n    SET i=i+1;\n    END WHILE;\nEND//\nDELIMITER ;\n \nCALL no_load_part;\n#插入分区表记录\nDROP PROCEDURE IF EXISTS load_part;\n \nDELIMITER&& \nCREATE PROCEDURE load_part()\nBEGIN\n    DECLARE i INT;\n    SET i=1;\n    WHILE i<80001\n    DO\n    INSERT INTO part_tab VALUES(i,'partition',ADDDATE('1995-01-01',(RAND(i)*36520) MOD 3652));\n    SET i=i+1;\n    END WHILE;\nEND&&\nDELIMITER ;\n \nCALL load_part;\n```\n\n","tags":["mysql","join","临时表","order by","分区表"],"categories":["JAVA","数据库","MYSQL"]},{"title":"mysql的连接查询","url":"/note/JAVA/数据库/MYSQL/MySql的连接查询/","content":"\n\n\n\n\n## mysql的连接查询\n\n### 笛卡尔集\n\n对于两张表的连接，会造成笛卡尔乘积现象：\n\n假设表a有m行，表b有n行：则结果是m*n行。\n\n<img src=\"mysql的连接查询.assets/image-20220831204520815-16671893143504.png\" alt=\"image-20220831204520815\" style=\"zoom:80%;\" />\n\n对于上面的两张表，如果直接查询：\n\n```mysql\nselect name,boyName from beauty,boys; \n```\n\n则结果是：12 * 4 = 48行\n\n很显然不是我们想要的结果！\n\n\n\n那么怎么解决笛卡尔集呢\n\n\n\n通过添加连接条件，可以过滤掉笛卡尔集\n\n```mysql\nselect name,boyName from beauty,boys where beauty.boyfriend_id = boys.id;\n```\n\n\n\n### 内连接-等值连接\n\n```mysql\n-- 语法\nSELECT 字段1, 字段2... FROM 表1,表2... WHERE 表1.xxx = 表2.xxx\n```\n\n原理：\n\n- 先从表1中拿出第一行的数据,然后匹配表2中的每一行记录，进行WHERE后面的判断\n- 满足判断：找到结果\n- 不满足判断：过滤掉，继续下一个\n\n特点\n\n- 等值连接，可以进行分组\n\n```sql\nselect COUNT(*) ,city from departments d, location l WHERE d.location_id = l.id GROUP BY city;\n```\n\n\n\n### 内连接-非等值连接\n\n与等值连接的不同之处在于\n\n- 等值连接的WEHER中是使用 = 进行判断\n\n- 非等值连接，就是不使用 = 进行判断，比如 xxx > xxx，或者 xxx BETWEEN xxx and xxx\n\n```mysql\n-- 语法\nSELECT 字段1, 字段2 ... FROM 表1,表2... WHERE 判断条件\n-- 例子\nselect 员工表.员工工资, 工资等级表.工资等级 from 员工表, 工资等级表 WHERE 员工表.员工工资 BETWEEN 工资等级表.最低工资 AND 工资等级表.最高工资;\n```\n\n原理：\n\n- 就是拿到员工表的中的每一个员工的工资，去工资登记表中进行判断，是否满足WHERE条件后的判断：\n- 满足判断：找到结果\n- 不满足判断：过滤掉，继续下一个\n\n\n\n### 自连接\n\n自连接，简单地说，就是一张表，自己与自己进行连接.\n\n```mysql\nSELECT a.name AS '部门名', b.name AS '上级部门' FROM sys_department a ,sys_department b WHERE a.parent_id = b.id;\n```\n\n\n\n\n\n### 外连接-左外连接(LEFT  JOIN)\n\n- LEFT JOIN\n- LEFT OUTER JOIN\n- 只有形式不一样。left join 是 left outer join 的简写，两者含义一样的。\n- 左外连接 = 左表全部记录 + 右表相关联记录\n\n\n\n### 外连接-右外连接(RIGHT JOIN)\n\n- RIGHT JOIN\n- RIGHT OUTER JOIN\n- 有形式不一样。left join 是 left outer join 的简写，两者含义一样的。\n- 右外连接 = 右表全部记录 + 左表相关联记录\n\n\n\n### 外连接-全外连接(不支持)\n\n- mysql不支持全外连接。\n- 全外连接 = 【左有右没有的记录】 + 【右有左没有的记录】 + 【两张表的交集记录】\n\n\n\n### 内连接(JOIN / INNER JOIN)\n\n- JOIN\n- INNER JOIN\n- join 是 inner join 的缩写\n- 内连接 = 两张表的交集记录\n\n\n\n### 交叉连接(CROSS JOIN)\n\n- CROSS JOIN\n- 交叉连接 = 笛卡尔积\n\n\n\n### 总结\n\n<img src=\"mysql的连接查询.assets/image-20220831210715528-16671893143505.png\" alt=\"image-20220831210715528\" style=\"zoom:50%;\" />\n\n\n\n\n\n\n\n\n\n","tags":["mysql","join"],"categories":["JAVA","数据库","MYSQL"]},{"title":"mysql的SQL优化实战","url":"/note/JAVA/数据库/MYSQL/mysql的SQL优化实战/","content":"\n\n\n# mysql的SQL优化实战\n\n\n\n以下是公司实际业务上的SQL调优实战\n\n\n\n## 金铎项目\n\n金铎项目是一个异常汇总平台，属于半路接手的项目，其中有很多sql都是比较复杂的，可读性差，维护性差，正好拿来练练手\n\n### 表结构\n\n一下是涉及的相关表结构\n\n#### amp_event_log\n\n```sql\n-- auto-generated definition\ncreate table amp_event_log\n(\n    id                         bigint auto_increment comment 'id'\n        primary key,\n    event_id                   bigint                             not null comment '事件ID',\n    event_code                 varchar(250)                       null comment '事件代码',\n    event_name                 varchar(250)                       null comment '事件名称',\n    exception_subject          varchar(512)                       null comment '异常主体',\n    monitor_object_name        varchar(250)                       null comment '监控对象名称',\n    process_status             varchar(250)                       null comment '处理状态',\n    happen_time                datetime                           null comment '发生时间',\n    exception_level            varchar(250)                       null comment '异常等级',\n    event_detail               varchar(3000)                      null comment '事件详情',\n    last_event_id              bigint                             null comment '上一级事件id',\n    timeout                    datetime                           null comment '超时时间',\n    version                    bigint                             null comment '版本',\n    push_user                  text                               null comment '推送对象',\n    is_push                    varchar(250)                       null comment '是否推送',\n    process_user               text                               null comment '处理对象',\n    actual_process_user        varchar(250)                       null comment '实际处理人',\n    feedback_duty_organization varchar(250)                       null comment '反馈责任组织',\n    feedback_process_results   varchar(512)                       null comment '反馈处理结果',\n    feedback_note              varchar(3000)                      null comment '具体原因',\n    start_process_time         datetime                           null comment '开始处理时间',\n    end_process_time           datetime                           null comment '结束处理时间',\n    reason                     varchar(250)                       null comment '原因',\n    create_time                datetime                           null comment '创建时间',\n    detail_url                 varchar(255) charset utf8mb4       null comment '明细路径',\n    is_transfer                bigint   default 0                 null comment '是否转办 0：否 ，1：是',\n    transfer_user              varchar(255)                       null comment '转办人',\n    is_approve                 bigint                             null comment '是否审批 0:否 1:是',\n    process_opinion            varchar(255)                       null comment '处理意见',\n    evaluate                   varchar(3000)                      null comment '评价',\n    approve_status             varchar(255)                       null comment '审批人状态',\n    predict_improve_time       datetime(3)                        null comment '预计改善时间',\n    approve_user               varchar(255)                       null comment '审批人',\n    latest_improve_time        datetime                           null comment '最晚改善时间',\n    update_time                datetime default CURRENT_TIMESTAMP not null on update CURRENT_TIMESTAMP comment '更新时间',\n    actual_approve_user        varchar(255)                       null comment '实际审批人',\n    person_liable              varchar(256)                       null comment '责任人',\n    improve_plan               varchar(1500)                      null comment '改善方案',\n    predict_improve_target     varchar(50)                        null comment '预计改善目标',\n    prov                       varchar(20)                        null comment '省',\n    city                       varchar(20)                        null comment '城市',\n    transfer_time              datetime                           null comment '转办时间',\n    feedback_process_images    varchar(250)                       null comment '现场处理图片'\n)\n    comment '异常事件记录表' collate = utf8mb4_bin;\n\ncreate index idx_actualuser_status\n    on amp_event_log (actual_process_user, process_status);\n\ncreate index idx_approve_status_actual_approve_user\n    on amp_event_log (approve_status, actual_approve_user);\n\ncreate index idx_city_happen_time\n    on amp_event_log (city, happen_time);\n\ncreate index idx_event_code\n    on amp_event_log (event_code);\n\ncreate index idx_event_id\n    on amp_event_log (event_id);\n\ncreate index idx_exception_subject\n    on amp_event_log (exception_subject);\n\ncreate index idx_happendtime\n    on amp_event_log (happen_time);\n\n\n```\n\n\n\n#### amp_site\n\n```sql\n-- auto-generated definition\ncreate table amp_site\n(\n    id            bigint       not null comment '站点id'\n        primary key,\n    name          varchar(255) null comment '站点名字',\n    area          varchar(255) null,\n    center_area   varchar(255) null,\n    province      varchar(255) null comment '站点所属省',\n    business_area varchar(255) null comment '站点所属的大区（浙北区）',\n    city          varchar(255) null comment '站点所属城市',\n    county        varchar(255) null comment '站点所属区',\n    sfcode        varchar(255) null comment '大网code',\n    sf_site       int(2)       null comment '1是0否'\n);\n\ncreate index amp_site_name_index\n    on amp_site (name);\n\n\n```\n\n#### amp_event\n\n```sql\n-- auto-generated definition\ncreate table amp_event\n(\n    id                   bigint auto_increment comment 'id'\n        primary key,\n    updator              varchar(250)                 null comment '修改人',\n    update_time          datetime                     null comment '修改时间',\n    create_time          datetime                     null comment '创建时间',\n    event_code           varchar(250)                 null comment '异常代码',\n    event_name           varchar(250)                 null comment '名字',\n    monitor_id           bigint                       null comment '监控对象id',\n    is_external          bigint                       null comment '是否外部渠道接入',\n    rule                 text                         null comment '触发规则',\n    message              text                         null comment '消息详情',\n    upgrade_rule         varchar(250)                 null comment '升级规则',\n    exception_level      varchar(250)                 null comment '异常等级',\n    reason               varchar(250)                 null comment '原因',\n    process_time         varchar(250)                 null comment '处理时效',\n    process_cycle        varchar(250)                 null comment '处理周期',\n    duty_organization_id bigint                       null comment '责任组织id',\n    deal_organization_id bigint                       null comment '处理组织id',\n    valid_time           datetime                     null comment '有效时间',\n    version              bigint default 1001          null comment '版本号',\n    is_history           bigint default 0             null comment '是否历史版本:0否,1是',\n    detail_url           varchar(255) charset utf8mb4 null comment '明细路径',\n    push_frequency       varchar(255)                 null comment '推送频率 天 ,周，双周，月',\n    push_day             bigint                       null comment '推送日期',\n    latest_improve_day   bigint                       null comment '最晚改善时间(天)',\n    is_approve           bigint                       null comment '是否审批 0:否 1:是',\n    approve_user         varchar(255)                 null comment '审批人',\n    rule_explain         varchar(3072)                null comment '规则说明',\n    push_job_id          varchar(250)                 null comment '推送岗位ID',\n    process_job_id       varchar(250)                 null comment '处理岗位id',\n    franchise_fee        decimal(10, 2)               null comment '加盟商收入'\n)\n    comment '异常事件表' collate = utf8mb4_bin;\n\ncreate index idx_dealorganizationid\n    on amp_event (deal_organization_id);\n\ncreate index idx_dutyorganizationid\n    on amp_event (duty_organization_id);\n\ncreate index idx_event_code_version\n    on amp_event (event_code, version);\n\ncreate index idx_monitorid\n    on amp_event (monitor_id);\n\n\n```\n\n\n\n\n\n\n\n\n\n### 优化SQL一\n\n#### SQL\n\n```sql\nexplain SELECT t1.process_status AS STATUS,\n       count(t1.id)         cnt\nFROM amp_event_log t1\n         JOIN\n     (select *\n      from amp_site\n      where sf_site = 0\n     ) t3\n     ON t3.NAME = t1.exception_subject\n         JOIN (\n    SELECT event_code,\n           deal_organization_id\n    FROM amp_event\n    WHERE is_history = 0\n      AND deal_organization_id = 1024\n) t2 ON t1.event_code = t2.event_code\nwhere 1 = 1\n  and t1.happen_time >= '2022-1-6 11:34:42'\n  and t1.happen_time <= '2023-1-6 11:34:42'\n  and t1.city in ('宁波市',\n                  '广州市',\n                  '杭州市',\n                  '深圳市',\n                  '温州市',\n                  '湖州市',\n                  '绍兴市',\n                  '重庆市',\n                  '金华市')\nGROUP BY process_status;\n```\n\n#### 初始执行计划\n\n```sql\n+----+-------------+-----------+------------+------+-------------------------------------------------------------------------------------------------+-----------------------+---------+------+-------+----------+----------------------------------------------------+\n| id | select_type | table     | partitions | type | possible_keys                                                                                   | key                   | key_len | ref  | rows  | filtered | Extra                                              |\n+----+-------------+-----------+------------+------+-------------------------------------------------------------------------------------------------+-----------------------+---------+------+-------+----------+----------------------------------------------------+\n|  1 | SIMPLE      | amp_event | NULL       | ALL  | idx_dealorganizationid,idx_event_code_version                                                   | NULL                  | NULL    | NULL |     8 |    12.50 | Using where; Using temporary; Using filesort       |\n|  1 | SIMPLE      | amp_site  | NULL       | ALL  | amp_site_name_index                                                                             | NULL                  | NULL    | NULL | 56586 |    10.00 | Using where; Using join buffer (Block Nested Loop) |\n|  1 | SIMPLE      | t1        | NULL       | ref  | idx_happendtime,idx_actualuser_status,idx_event_code,idx_city_happen_time,idx_exception_subject | idx_exception_subject | 2051    | func |     1 |    25.00 | Using index condition; Using where                 |\n+----+-------------+-----------+------------+------+-------------------------------------------------------------------------------------------------+-----------------------+---------+------+-------+----------+----------------------------------------------------+\n3 rows in set, 2 warnings (0.04 sec)\n```\n\n\n\n#### 分析语句\n\nUsing where; Using temporary; Using filesort\n\nUsing where; Using join buffer (Block Nested Loop)\n\nUsing index condition; Using where\n\n\n\n- Using temporary\n\n  - `group by` 语句一般会使用内部临时表，因为需要使用内部临时表存储数量；\n  - 一般情况下`group by`都是会使用临时表的，但是再`有些业务场景`下，也是可以优化成不用临时表的，所以效率会更高\n  - 【优化点一】：是否可以将这个 group by 语句取消使用内部临时表\n- Using filesort\n\n  - group by 分组后默认自然顺序排序，如果不需要排序，可以改成：order by null；（8.0版本group by已经不会排序了）\n  - 所以可以根据具体的业务场景看，是否需要排序\n  - 【优化点二】：group by 语句是否可以不需要排序\n- Using join buffer (Block Nested Loop)\n  - 使用了join语句，而且是BNL算法，效率很低，首先看这个语句是否可以不用join\n\n  - 【优化点三】：是否可以避免join？从业务代码的角度进行优化？\n\n  - 如果join优化不了，是否可以将BNL算法优化成NLJ（BKA算法）\n\n  - 【优化点四】：是否可以将BNL算法优化成NLJ（BKA算法）\n\n  - 但是呢，即使使用了NLJ（BKA算法），效率还是不如hash-join（mysql8.0支持hash-join，效率高）\n\n  - 【优化点五】：是否可以通过业务代码实现hash-join\n\n\n\n\n初次分析结论如上，先进行优化一波\n\n\n\n\n\n\n\n\n\n","tags":["mysql","SQL优化","性能调优","调优"],"categories":["JAVA","数据库","MYSQL"]},{"title":"快速排序算法","url":"/note/ALGORITHM/SORT/快速排序算法/","content":"\n# 快速排序算法\n\n","tags":["算法","排序算法","快速排序"],"categories":["ALGORITHM","SORT"]},{"title":"spring的事务生效条件","url":"/note/JAVA/SSM三大框架/【spring】spring的事务生效条件/","content":"\n\n\n\n\n参考文章：https://blog.csdn.net/XiaoWenJava123/article/details/102776574\n\n同一个类中，没有事务的方法调用有事务的方法，事务会生效吗？\n\n```java\n@Service\npublic class UserService {\n    \n    public void testA(){\n        //方法A没有事务，调用有事务的方法B\n        testB();\n    }\n    \n    @Transactional\n    public void testB(){\n        \n    }\n   \n}\n```\n\n答：不会生效。\n\n原因：\n\n- spring 在扫描bean的时候会扫描方法上是否包含@Transactional注解。\n- 如果包含，spring会为这个bean动态地生成一个子类（即代理类，proxy），代理类是继承原来那个bean的。\n- 此时，当这个有注解的方法被调用的时候，实际上是由代理类来调用的，代理类在调用之前就会启动事务。\n- 然而，如果这个有注解的方法是被同一个类中的其他方法调用的，那么该方法的调用并没有通过代理类，所以就不会启动事务。\n\n\n\n解决的方法就简单了：\n\n1、将被调用的本类方法重新从Spring中获取\n\n2、在Application上加注解：@EnableAspectJAutoProxy(proxyTargetClass = true, exposeProxy = true)解决同类方法调用时异步和事务不生效：\n\n3、还有一种开启类的事物（不认为是个合理的解决方案）\n\n\n\n\n\n\n\n\n\n方法不是public修饰，因为Spring代理是Cglib,是生成一个子类去调用的，子类没有父类的非public方法，自然不会有事务产生\n\n\n\n","tags":["spring","事务"],"categories":["JAVA","SSM三大框架"]},{"title":"优先队列排序算法","url":"/note/ALGORITHM/SORT/优先队列排序算法/","content":"\n# 优先队列排序算法\n\n","tags":["算法","排序算法","优先队列排序"],"categories":["ALGORITHM","SORT"]},{"title":"归并排序算法","url":"/note/ALGORITHM/SORT/归并排序算法/","content":"\n# 归并排序算法","tags":["算法","排序算法","归并排序"],"categories":["ALGORITHM","SORT"]},{"title":"idea单测的覆盖率","url":"/note/TOOLS/idea/idea单测的覆盖率/","content":"\n\n\n\n\n---\n\n\n\n最近公司让写单测，写好了一个类的单测，运行之后，发现：怎么没有覆盖率呀？\n\n<img src=\"idea单测的覆盖率.assets/image-20221122213020377.png\" alt=\"image-20221122213020377\"  />\n\n原因\n由于test类文件的包名`com.sf.fw.aging.test.core.rule.RuleExecuteTest`和被测试的类的包名`com.sf.fw.aging.eagle.core.RuleExecutor`不同，引起的。\n\n\n\n解决办法\n\n将测试类和被测试类，放在同一个包名下。简单的操作就是：\n\n在被测试类上，alt + enter 生成测试类，就可以了。\n\n<img src=\"idea单测的覆盖率.assets/image-20221122213324957.png\" alt=\"image-20221122213324957\" style=\"zoom:80%;\" />\n\n\n\n正常\n\n<img src=\"idea单测的覆盖率.assets/image-20221122213708490.png\" alt=\"image-20221122213708490\" style=\"zoom:80%;\" />\n\n\n\n可以设置单测覆盖率的统计路径\n\n<img src=\"idea单测的覆盖率.assets/image-20221125161420585.png\" alt=\"image-20221125161420585\"  />","tags":["idea","单测"],"categories":["TOOLS","idea"]},{"title":"mysql的主备从入门到入土","url":"/note/JAVA/数据库/MYSQL/mysql的主备从入门到入土/","content":"\n# mysql的主备从入门到入土\n\n---\n\n本文有xmind，配合观看，效果更佳：[Mysql主备从入门到入土.xmind](mysql的主备从入门到入土.assets/Mysql主备从入门到入土.xmind)\n\nbinlog 可以用来归档，也可以用来做主备同步，那么具体是怎么做主备同步的呢？为什么备库执行了 binlog 就可以跟主库保持一致了呢？\n\n\n\n## 什么是主备\n\n传统的单机架构在目前的应用场景中不足以应对，后来才衍生出高可用，解决单点问题。mysql也不例外\n\n\n\n## 主备同步（主备一致）的流程\n\n\n\n```mermaid\nflowchart LR\n开始((开始))-->undologmem[undolog<br/>mem]\nsubgraph masterA\nundologmem-->datamem[data<br/>mem]\ndatamem-->redologprepare[redolog<br/>prepare]\nredologprepare-->binlog[binlog]\nbinlog-->redologcommit[redolog<br/>commit]\nbinlog-->dump_thread[<font color='red'>dump_thread</font>]\nbg_thread[bg_thread]-->undologdisk[undolog<br/>disk]\nundologdisk-->datadisk[data<br/>disk]\nend\nredologcommit-->ack((ack))\nsubgraph masterB\ndump_thread-->io_thread[<font color='red'>io_thread</font>]\nio_thread-->relaylog[relaylog]\nrelaylog-->sql_thread[<font color='red'>sql_thread</font>]\nsql_thread-->data[data]\nend\n```\n\n上图是主备切换的大致流程图，包括undolog，redolog，和binlog的大致流程（复习一下redolog和binlog的二阶段提交）。\n\n从左到右的流程是：\n\n- masterA主库接收到Client过来的更新请求，先记录undolog，然后写data缓存，记录redolog和binlog。\n- 当binlog记录成功之后，在masterA主库中有一个线程dump_thread，这个dump_thread线程是用来维护和masterB备库的长链接。\n- 在masterB备库中，也有一个io_thread线程，负责与主库建立长链接\n- masterB备库会向masterA主库请求同步binlog，备库会告诉主库需要同步的位置\n- masterA主库会按照masterB备库传过来的位置，读取binlog，发送给masterB备库\n- masterB备库，接收到masterA主库传过来的binlog，先暂存到本地文件，称为中转日志 relaylog\n- 在备库B中有一个线程sql_thread，会负责解析中转日志relaylog，并执行\n\n\n\n### 两种主备架构\n\n主备架构有两种，并且两种不同的架构，主备同步的流程有一些区别\n\n### M-S结构的主备同步流程\n\n\n\n```mermaid\nflowchart TD\nclient-->mysqlA\nmysqlA-->mysqlB\\nreadonly\n\n```\n\nM-S结构如图所示，mysqlB是只读的，作为A的备库。\n\n在主备切换的时候，会把client的请求转发到B\n\n同时将mysqlA设置为readonly，设置为mysqlB的备库（修改主备关系）\n\n当然在主备切换的时候，会有问题，比如把client的请求转发到B，但是此时B是readonly的，写不进去数据怎么办？\n\n- 其实这是主备切换必须要面对的问题，后面会介绍到：可用性优先切换，和可靠性优先切换 两个方案。\n- 点此查看：[可靠性优先切换策略](#可靠性优先策略)\n- 点此查看：[可用性优先切换策略](#可用性优先策略)\n\n\n\n\n\n### 双M结构的主备同步流程\n\n```mermaid\nflowchart TD\nclient-->mysqlA\nmysqlA-->mysqlB\\nreadonly\nmysqlB\\nreadonly-->mysqlA\n```\n\n\n\n双 M 结构和 M-S 结构，其实区别只是多了一条线，即：mysql A 和 mysql B 之间总是互为主备关系。这样在切换的时候就不用再修改主备关系。\n\n#### 循环复制问题\n\n但是双M结构会有一个问题，就是主备同步的时候：\n\n- mysqlA把binlog发给mysqlB，mysqlB执行完binlog之后，又会生成binlog（通过参数log_slave_updates控制执行relaylog执行后是否产生binlog）\n- mysqlB再把binlog发给mysqlA，mysqlA又会执行...这就是双M结构的循环复制问题\n\n解决这个问题，也很简单，思考一下：\n\n- 方案一：在mysqlB执行完之后，不生成binlog不就完事了。\n- 方案二：在mysqlB执行完之后，生成了binlog，但是不发给mysqlA不就完事了。\n- 方案三：在mysqlB执行完之后，生成了binlog，发给mysqlA，但是mysqlA不执行不就完事了。\n\n在上面三个方案，其实都可以解决循环复制的问题，\n\n- 对于方案一，可以通过`log_slave_updates`这个参数控制relaylog执行后是否产生binlog，但是呢，我们建议打开备库的binlog。\n- 对于方案二，mysql没采用这个方法，不知道为什么。\n- 对于方案三，mysql采用的这个方式，mysql具体是怎么实现的呢？\n\n方案三的实现：\n\n在mysql的binlog中，记录了一个叫做server id 的东西，如下是在ROW格式下的binlog（截取了部分）：\n\n```sh\nC:\\Program Files\\MySQL\\MySQL Server 8.0\\bin>mysqlbinlog.exe  -vv \"C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Data\\xxxxx-bin.000002\" --start-position=5043 --stop-position=5300\n# at 5043\n#221101 15:21:11 server id 1  end_log_pos 5122 CRC32 0x72f668e7         Anonymous_GTID  last_committed=17       sequence_number=18      rbr_only=yes    original_committed_timestamp=1667287271257812   immediate_commit_timestamp=1667287271257812  transaction_length=287\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n\n```\n\n可以看到，是有 `server id 1 `这个内容的，就表示当前这个binlog是在server=1这个库上产生的。\n\n`server id`在mysql主备中，不同的实例必须是唯一的，如果它们相同，那么它们不能构成主备关系。\n\nROW格式的是有serverid的，那么statement格式的有吗？当然也有。也得用mysqlbinlog工具查看才行，都可以看到`server id`这个字段\n\n```mysql\nmysqlbinlog.exe  -vv \"C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Data\\xxxxx-bin.000002\" --start-position=7829 --stop-position=8126\n```\n\n具体的binlog的格式可以参考：`mysql的日志从入门到入库->重要的日志模块(redolog和binlog)->binlog的结构` 这一篇文章\n\n- 规定两个库的 server id 必须不同，如果相同，则它们之间不能设定为主备关系；\n- 一个备库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog；\n- 每个库在收到从自己的主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志。\n\n\n\n### 公司生产使用的是M-S结构\n\n\n\n<img src=\"mysql的主备从入门到入土.assets/image-20221108181935875.png\" alt=\"image-20221108181935875\" style=\"zoom:80%;\" />\n\n\n\n上面两个是生产环境（prod）的主节点和从节点；下面两个是容灾环境（dr）的主节点和从节点。\n\n\n\n## 主备延迟和产生和避免\n\n从主备同步可以看出来，备库从主库同步binlog，会有网络的问题，主库和备库机器性能问题等，都会导致备库执行binlog的速度比主库生成的速度慢，这就会导致主备延迟。\n\n但是呢，只要主库生成的binlog都被备库正常接收了，并且正确执行了，那么主库和备库之间，数据就会保持：最终一致性。\n\n但是仅仅有最终一致性，是不满足高可用的，因为还有之前说的主备延迟的问题。考虑这么一个场景：主库突然出问题了，要进行主备切换，然后现在备库还没有执行完主库的binlog，强行切换到备库，就会导致数据不一致的问题。\n\n\n\n### 什么是主备延迟\n\n备库从主库同步binlog，会有网络的问题，主库和备库机器性能问题等，都会导致备库执行binlog的速度比主库生成的速度慢，这就会导致主备延迟。\n\n```mermaid\nflowchart LR\n主库A的binlog生成完成-T1-->备库B接收到binlog-T2-->备库B执行完成binlog-T3\n```\n\n上面三个时间点，分别记为：T1，T2，T3，那么T3-T1就是主备延迟的时间。\n\n在mysql中，可以通过在备库上执行`show slave status`命令，在执行结果中有一个`seconds_behind_master`字段，表示备库落后于主库多少秒。\n\n\n\n### 为什么会产生主备延迟\n\n- 备库的机器性能比主库差\n\n- 备库压力大\n  - 我们一般把备库设置为readonly，并提供一些只读的查询，这些只读的查询，一般会比较复杂，会对数据库造成压力\n  - 解决：可以采用一主多从的架构，分担读压力；还可以将数据分散到外部系统，比如大数据。\n- 大事务\n  - 因为主库上必须等事务执行完成才会写入 binlog，再传给备库。所以，如果一个主库上的语句执行 10 分钟，那这个事务很可能就会导致从库延迟 10 分钟。\n  - 常见的大事务有哪些？\n    - 大量DML：一次性地用 delete 语句删除太多数据。解决办法就是，少量多次删除。\n    - 大表DDL：使用gh-ost方案。\n- 主库的事务是并发的，而备库是sql_thread单线程同步的\n  - 备库的并行复制能力\n\n\n### 备库的并行复制能力\n\n通过以上产生主备延迟的原因，基本都有对应的解决办法，都是mysql也为了减少主备延迟的时间，做了很多努力。\n\n为什么要有多线程复制呢？这是因为单线程复制的能力全面低于多线程复制，对于更新压力较大的主库，备库是可能一直追不上主库的。从现象上看就是，备库上 seconds_behind_master 的值越来越大。\n\n都做了什么努力呢？我们再来回顾主备同步的步骤。\n\n```mermaid\nflowchart LR\n开始((开始))-->undologmem[undolog<br/>mem]\nsubgraph masterA\nundologmem-->datamem[data<br/>mem]\ndatamem-->redologprepare[redolog<br/>prepare]\nredologprepare-->binlog[binlog]\nbinlog-->redologcommit[redolog<br/>commit]\nbinlog-->dump_thread[dump_thread]\nbg_thread[bg_thread]-->undologdisk[undolog<br/>disk]\nundologdisk-->datadisk[data<br/>disk]\nend\nredologcommit-->ack((ack))\nsubgraph masterB\ndump_thread-->io_thread[io_thread]\nio_thread-->relaylog[relaylog]\nrelaylog-->sql_thread[sql_thread]\nsql_thread==>data[data]\nend\n```\n\n这个图，可文章开始的图有一点点细微的差别，这个细微的差别，在于最后一步：sql_thread->data 这一步的线，比之前的图，粗了一点点。为什么呢？\n\n因为mysql对这一步骤做了优化，在通过sql_thread执行relaylog的时候，是通过多线程执行的。\n\n使用了多线程，就可以提高备库的能力。减少主备延迟的时间。\n\n\n\n#### 并行复制的基本原理\n\n从单线程复制到最新版本的多线程复制，中间的演化经历了好几个版本。接下来，我就跟你说说 MySQL 多线程复制的演进过程。\n\n其实说到底，所有的多线程复制机制，都是要把上图中只有一个线程的 sql_thread，拆成多个线程：\n\n```mermaid\nflowchart LR\nrelaylog-->coordinator\nsubgraph sql_thread\ncoordinator-->worker-1\ncoordinator-->worker-2\ncoordinator-->worker-3\ncoordinator-->点点点[..........]\ncoordinator-->worker-n\nend\nworker-1-->data\nworker-2-->data\nworker-3-->data\n点点点-->data\nworker-n-->data\n\n```\n\n其中的`coordinator`和`worker`就是原来的`sql_thread`;\n\n- `coordinator`复制读取和分发`relaylog`\n- `worker`线程是真正来执行`relaylog`的\n\nworker线程有多少个？\n\n- 是通过`slave_parallel_workers`参数控制的，一般32C的机器，建议设置为8-16之间，要留一些用于读查询\n- 对于单机的mysql，默认是0\n- 公司的生产环境配置的是 16\n\n\n\n#### 并行复制面临的问题\n\n了解了并行复制的基本原理，接下来思考几个问题？\n\n**`coordinator`是怎么分发`relaylog`给worker线程的？能不能按照轮训的方式，也就是事务1的日志分为worker1，事务2的日志分给worker2？**\n\n- 不能\n- 因为不同的事务可能会操作同一行，由于CPU的调度问题，很有可能导致事务2的日志先执行，事务1的日志后执行，导致数据不一致的问题。\n\n**`coordinator`能不能把一个事务的不同的更新语句的binlog分给多个worker执行？**\n\n- 不能\n- 一个事务的binlog是完整的，分开执行，很有可能也会出现数据不一致的情况。\n\n所以，对`coordinator`的分发，就提出了两个最基本的要求\n\n- 要求更新同一行的不同事务的binlog，必须被分发到同一个 worker 中。\n- 要求同一个事务的binlog不能被分开，必须被分发到同一个 worker 中。\n\n\n\n#### MySQL 5.5 版本的并行复制策略（不支持）\n\n官方 MySQL 5.5 版本是不支持并行复制的。\n\n但是，在 2012 年的时候，我自己服务的业务出现了严重的主备延迟，原因就是备库只有单线程复制。\n\n然后，我就先后写了两个版本的并行策略。即按表分发策略和按行分发策略，以帮助你理解 MySQL 官方版本并行复制策略的迭代。\n\n\n\n##### 按表分发策略\n\n**基本原理**：如果两个事务操作的是不同的表，那么这两个事务就可以并行处理；如果一个事务中操作了多张表，那么把这些表当做一张表。\n\n**实现细节**：\n\n- 每一个worker线程，维护一个hashmap，key是【库名+表名】，value是待执行的事务日志的个数\n  - 当`coordinator`把一个事务分给worker执行的时候，value+1\n  - 当worker把某一个事务执行完成的时候，value-1\n- 当coordinator读取下一个事务的日志之后，先获取到这个事务所操作的【库名+表名】（可能是一张表，也可能是多张表）\n- 如果是一张表\n  - 遍历所有的worker\n  - 判断当前worker中的hashmap的key，是不是包含这张表\n  - 如果包含：则把这个事务，交给这个worker执行\n  - 如果不包含，继续遍历下一个worker\n  - 如果所有的worker都不包含，则把这个事务交给负载最低的worker执行，并将当前的【库名+表名】添加到这个worker的hashmap中\n- 如果是多张表\n  - 遍历所有的worker\n  - 如果这多张表，是由一个worker执行的，则把这个事务，交给这个worker执行\n  - 如果这多张表，目前没有worker执行，则把这个事务，交给负载最低的worker执行\n  - 如果这多张表，是由多个woker执行的，则`coordinator`进入等待，并且不停的进行遍历worker进行判断\n\n**缺点**：这个按表分发的方案，在多个表负载均匀的场景里应用效果很好。但是，如果碰到热点表，比如所有的更新事务都会涉及到某一个表的时候，所有事务都会被分配到同一个 worker 中，就变成单线程复制了。\n\n\n\n##### 按行分发策略\n\n要解决热点表的并行复制问题，就需要一个按行并行复制的方案。\n\n**基本原理**：如果两个事务没有更新相同的行，它们在备库上可以并行执行。显然，这个模式要求 binlog 格式必须是 row。因为statement记录的是sql语句，无法判断更新了哪一行？\n\n实现细节：\n\n- 每一个worker线程，维护一个hashmap，key是【库名+表名+主键值+唯一索引的值】，value是待执行的事务日志的个数\n- 剩下的步骤，同**按表分发策略**\n\n**缺点**：对比按表分发和按行分发这两个方案的话，按行分发策略的并行度更高。不过，如果是要操作很多行的大事务的话，按行分发的策略有两个问题：\n\n- 耗费内存。比如一个语句要删除 100 万行数据，这时候 hash 表就要记录 100 万个项。\n- 耗费 CPU。解析 binlog，然后计算 hash 值，对于大事务，这个成本还是很高的。\n\n**退化**：因为缺点的存在，所以在实现这个策略的时候会设置一个阈值，单个事务如果超过设置的行数阈值（比如，如果单个事务更新的行数超过 10 万行），就暂时退化为单线程模式，退化过程的逻辑大概是这样的：\n\n- coordinator 暂时先 hold 住这个事务；\n- 等待所有 worker 都执行完成，变成空队列；\n- coordinator 直接执行这个事务；\n- 恢复并行模式。\n\n\n\n#### MySQL 5.6 版本的并行复制策略\n\n官方 MySQL5.6 版本，支持了并行复制，只是支持的粒度是**按库并行**。\n\n具体的实现方案和按表分发以及按行分拨的是一样的，只不过hashmap中的key是库名了。\n\n\n\n#### MariaDB 的并行复制策略\n\n**基本原理**：利用了组提交的特性\n\n- 能在同一个组中提交的事务，一定不会更新同一行（因为innodb在更新某一行的时候，会先获取行锁，上一个事务还没有commit的时候，行锁是不会释放的）\n- 主库能并行执行的，备库一定也可以并行执行。\n\n**实现细节**：\n\n- 在一组里面提交的事务，有一个共同的commit_id\n- coordinator每次从relaylog中获取一批事务（具有相同的commit_id）\n- 然后把这一批直接分给worker并发执行\n- 等待worker执行完之后，coordinator在获取下一批事务\n\n**缺点**：虽然是并发了，但是并发度不够，不够在哪里呢？思考一下，在主库上，一组事务的binlog在commit的时候，下一组binlog可能已经开始write了，主库的效率是很高的。而在备库上，coordinator获取一批事务，必须等待这一批执行完，才能执行下一批。效率相比较于主库，就比较慢了。\n\n\n\n#### MySQL 5.7 的并行复制策略\n\n在MariaDB 的并行复制策略出现之后，mysql5.7也出了一个类似的，但是呢，也并没有舍弃mysql5.6中的按库分发策略。mysql5.7提供了一个参数\n\n- slave-parallel-type\n- 配置为 DATABASE，表示使用 MySQL 5.6 版本的按库并行策略；\n- 配置为 LOGICAL_CLOCK，表示的就是类似 MariaDB 的策略（不过是mysql优化后的）。\n\n**mysql对 MariaDB 的策略进行了优化，优化了什么呢？**\n\n在回顾一下MariaDB的并行复制策略的核心是：所有commit的事务，可以并行执行。因为commit的事务一定是不会有锁冲突的。\n\n那么在思考一下mysql的更新流程：\n\n```mermaid\nflowchart LR\n开始((开始))-->undologmem[undolog<br/>mem]\nundologmem-->datamem[data<br/>mem]\ndatamem-->redologprepare[redolog<br/>prepare]\nredologprepare-->binlog[binlog]\nbinlog-->redologcommit[redolog<br/>commit]\n```\n\n可以发现，其实在记录redolog（prepare）的时候，就已经通过了锁校验。具体的关于锁，可以看：mysql的锁从入门到入土\n\n优化点：只要是通过了redolog（prepare）阶段后的一组事务的binlog（组提交），传到备库的时候，这组事务都是可以并行执行的\n\n\n\n\n\n#### MySQL 5.7.22 的并行复制策略\n\n公司生产用的是mysql版本是5.7.19\n\n在 2018 年 4 月份发布的 MySQL 5.7.22 版本里，MySQL 增加了一个新的并行复制策略，基于 WRITESET 的并行复制。\n\n相应地，新增了一个参数 `binlog-transaction-dependency-tracking`，用来控制是否启用这个新策略。这个参数的可选值有以下三种。\n\n- `COMMIT_ORDER`表示的就是前面介绍的，根据同时进入 prepare 和 commit 来判断是否可以并行的策略。\n- `WRITESET`表示的是对于事务涉及更新的每一行，计算出这一行的 hash 值，组成集合 writeset。如果两个事务没有操作相同的行，也就是说它们的 writeset 没有交集，就可以并行。\n- `WRITESET_SESSION`是在 WRITESET 的基础上多了一个约束，即在主库上同一个线程先后执行的两个事务，在备库执行的时候，要保证相同的先后顺序。\n\n**实现细节**：\n\n- msyql对于事务涉及更新的每一行，计算出这一行的 hash 值（是通过“库名 + 表名 + 索引名 + 值”计算出来的），组成集合 writeset。\n- 把这个writeset集合写到binlog中（写在binlog哪里的，不知道，怎么从binlog中获取的，也不知道，这里没扣细节）\n- 备库执行的时候，直接获取一批事务，获取这一批事务的writeset，和当前正在执行的事务的writeset，判断是否有交集。\n- 没有交集，就可以并行。\n\n**优点**：\n\n- 获取writeset的时候，不需要计算，也不需要解析binlog，就可以直接拿到，省CPU\n- 不需要把整个事务的 binlog 都扫一遍才能决定分发到哪个 worker，更省内存；\n- 由于备库的分发策略不依赖于 binlog 内容，所以 binlog 是 statement 格式也是可以的。\n\n**缺点**：\n\n- 对于“表上没主键”和“外键约束”的场景，WRITESET 策略也是没法并行的，也会暂时退化为单线程模型。\n\n\n\n### 思考题\n\n如果主库都是单线程压力模式，在从库追主库的过程中，binlog-transaction-dependency-tracking 应该选用什么参数？\n\n- 首先binlog-transaction-dependency-tracking能选哪些参数？\n  - ORDER_COMMIT\n  - WRITE_SET\n  - WRITE_SET_SESSION\n- 其次看这三个参数分别对应了什么？\n  - ORDER_COMMIT：主要是处于redolog（prepare）状态之后的，组提交的事务，可以并行复制；\n  - WRITE_SET_SESSION：同一个会话中的事务的writeset，即使没有交集，也要顺序执行\n  - WRITE_SET：同一个会话中的事务的writeset，只要没有交集，就可以并行\n\n- 在分析题目\n  - 单线程：所以就没有组提交了，因为都是一个一个提交的。\n  - 单线程：所以是一个会话\n- 结合来看：\n  - 没有组提交，就不能用ORDER_COMMIT，备库会单线程复制。\n  - 是一个会话，所以就不能用WRITE_SET_SESSION，因为也会顺序执行，顺序执行，就成了单线程复制。\n\n\n\n## 主备切换（高可用）\n\n### 为什么要进行主备切换\n\n主备切换有两种场景，一种是主动切换，一种是被动切换。\n\n- 主动切换：人为的发起\n- 被动切换：一般是因为主库出问题了，由 HA 系统发起的。\n\n这也就引出了我们今天要讨论的问题：怎么判断一个主库出问题了？\n\n\n\n### 怎么判断库有问题\n\n\n\n#### select 1\n\n- 只能判断mysql的进程是否存在，无法判断数据库是否可用\n\n- 验证\n\n  - 设置`innodb_thread_concurrency=3`,然后开启4个session，执行下面的语句\n\n  - | session-1                  | session-2                  | session-3                  | session-4                                                    |\n    | -------------------------- | -------------------------- | -------------------------- | ------------------------------------------------------------ |\n    | select sleep(1000) from t; | select sleep(1000) from t; | select sleep(1000) from t; |                                                              |\n    |                            |                            |                            | select 1; (Query OK)<br/>select * from t;(<font color='red'>blocked</font>) |\n\n  - 可以看到select 1会成功，但是查询表会阻塞；\n\n- `innodb_thread_concurrency`表示限制innoDB的并发执行线程数，是指通知运行的线程有多少个。\n- 并发连接数对系统的运行没有多大的影响，有影响的是并发执行数，才会大量占用CPU；具体的可以看：mysql的性能调优从入门到入土\n\n\n\n#### 查询语句\n\n既然`select 1`不行，那我用查询语句总可以吧。\n\n- 新建一张表，比如叫：health_check；\n- 定期执行：select * from health_check;\n- 能执行成功，说明数据库正常\n- 执行不成功，或者超时没返回结果，数据库不正常\n\n但是这个方案，也是不行的，因为当**数据库磁盘满了**的时候，此时数据库不能写入，但是可以正常提供读服务；\n\n\n\n#### 更新语句\n\n既然查询语句，那我用更新语句总可以吧。\n\n常用的做法是：\n\n- 新建一张表，比如叫：health_check\n- 定期执行：update health_check set check_time= now();\n\n这么一看，似乎问题不大，但是注意：主库定期检测了，备库也需要定期检测呀，但是由于主备同步的存在，会导致数据检测有问题，所以要优化一下\n\n- 在health_check表中，加一列，表示当前数据库的server id;\n- 每次检查的时候，只更新自己的server id，就没问题了(因为主备的server id是不同的)\n- update health_check set check_time=now() where server_id = 'xxxx';\n\n但是呢，这个方案，也是不行的，当数据库的磁盘IO性能瓶颈的时候，比如IO使用率已经100%了，注意IO使用率100%并不是说系统不可用，因为此时IO还在继续工作呢，只不过IO使用率100%了，会导致后续过来的更新操作，排队等待而已，对client来说，就是我的sql执行的好慢呀。\n\n那么IO使用率100%了，对于我们的检测语句来说，因为检测语句占用的IO资源非常非常小，所以系统可能会安排执行，这样的话，我们的检测语句正常返回了。但是此时数据库已经达到瓶颈了。\n\n\n\n#### 外部判断的弊端\n\n以上三种判断方法都是外部判断的方式，以上的三种检测方法都有各自的使用场景，但是他们具备同样的一个弊端：\n\n- 判断慢\n\n什么是判断慢，就是系统已经出问题了，但是我们的检测不及时，如果定时检查是10分钟一次，那就要等10分钟之后才能发现问题，而且上面三种方式，还不一定能够覆盖所有的异常场景，所以外部检测的方式，一般只是用来辅助，并不能完全靠它。\n\n\n\n#### 内部判断\n\nMySQL 5.6 版本以后提供的 performance_schema 库，里面记录了很多数据库运行时候的详细统计信息。\n\n在mysql性能调优从土门到入土中，也介绍过 performance_schema库\n\n具体的这里不介绍了，只需要知道这个库中记录的很多超级详细的内容，完全可以实时的检测mysql的运行状态。\n\n但是还是有需要注意的地方\n\n- 如果打开所有的 performance_schema 项，性能大概会下降 10% 左右。所以，我建议你只打开自己需要的项进行统计。\n\n\n\n### 两种主备架构\n\n```mermaid\nflowchart TD\nsubgraph M-S结构\nclientms[client]-->mysqlAms[mysqlA]\nmysqlAms-->mysqlBms[mysqlB<br/>readonly]\nend\nsubgraph 双M结构\nclientmm[client]-->mysqlAmm[mysqlA]\nmysqlAmm-->mysqlBmm[mysqlB<br/>readonly]\nmysqlBmm-->mysqlAmm\nend\n```\n\n\n\n由于主备延迟的存在，所以在主备切换的时候，就相应的有不同的策略。\n\n### 可靠性优先切换策略\n\n下面的切换流程，是在`双M结构`的流程下进行的操作，和`M-S结构`的区别在于：不用修改主备关系了。\n\n- 1、判断备库B上的seconds_behind_master，如果小于某个值（比如5秒），继续下一步，否则继续当前步骤\n- 2、将主库A设置为readonly\n- 3、判断备库B上的seconds_behind_master，直到它变为0为止\n- 4、将备库B设置为可读写\n- 5、将业务请求转发到备库B\n\n在上面的步骤中，我们看到，系统是有不可用时间的。此时主库A和备库B都处于只读状态。\n\n\n\n### 可用性优先切换策略\n\n下面的切换流程，是在`双M结构`的流程下进行的操作，和`M-S结构`的区别在于：不用修改主备关系了。\n\n- 1、将备库B设置为可读写\n- 2、将业务请求转发到备库B，此时原来的主库A就不会有业务请求了\n- 3、判断备库B上的seconds_behind_master，直到它变为0为止\n- 4、将主库A设置为readonly\n\n在上面的步骤中，系统可能会出现数据不一致的情况的，就是A产生的binlog，B还没有执行的时候，就接收到了新的请求，然后主库A的binlog此时传到了备库B上，B在执行binlog，就会导致数据不一致的场景。\n\n\n\n### 公司生产使用的是可靠性优先策略\n\n公司使用的是M-S结构，并且使用的是可靠性优先切换策略。\n\n之前进行过主备切换，DBA通知需要停应用才行的。\n\n<img src=\"mysql的主备从入门到入土.assets/image-20221110201120695.png\" alt=\"image-20221110201120695\" style=\"zoom:67%;\" />\n\n而且主备切换，一般是由专门的HA系统操作的，不用人为的介入，以下的公司的ha系统\n\n<img src=\"mysql的主备从入门到入土.assets/image-20221110195355203.png\" alt=\"image-20221110195355203\" style=\"zoom: 80%;\" />\n\n\n\n\n\n\n\n## 主从切换\n\n### 什么是一主多从\n\n```mermaid\nflowchart TD\nclient-->|write|mysqlA\nmysqlA-.->mysqlA1\nmysqlA1-.->mysqlA\nclient-->|read|mysqlB\nclient-->|read|mysqlC\nclient-->|read|mysqlD\nmysqlA-.->mysqlB\nmysqlA-.->mysqlC\nmysqlA-.->mysqlD\n```\n\nmysqlA是主库\n\nmysqlA1是备库（双M结构）\n\nmysqlB，C，D是从库，从主库mysqlA进行同步，并且B，C，D提供只读能力\n\n以上的架构就是一主多从架构。\n\n\n\n### 一主多从有什么问题\n\n在系统正常运行的时候，没什么问题，主要考虑异常的情况：\n\n- 当主库mysqlA突然宕机了，或者无法提供服务了，怎么办？\n- 那肯定要进行主备切换了，将主库切换到备库mysqlA1（关于主备切换，上面说过了，这种被动切换的场景，就没法保证数据可靠性了，只能强制切换到备库上）\n- 但是主备切换完成之后，还有从库呢？怎么办呢？\n- 之前从库是从`主库mysqlA`进行同步的，现在从库必须要连接`新主库mysqlA1`了。（而这一步，就是主从切换的难点）\n\n\n\n### 一主多从的切换策略\n\n一起看看一个切换系统会怎么完成一主多从的主备切换过程。\n\n一主多从一般有两种切换办法\n\n- 基于位点的同步\n\n```sql\nCHANGE MASTER TO \nMASTER_HOST=$host_name \nMASTER_PORT=$port \nMASTER_USER=$user_name \nMASTER_PASSWORD=$password \nMASTER_LOG_FILE=$master_log_name \nMASTER_LOG_POS=$master_log_pos  \n```\n\n- 基于GTID的切换\n\n```sql\nCHANGE MASTER TO \nMASTER_HOST=$host_name \nMASTER_PORT=$port \nMASTER_USER=$user_name \nMASTER_PASSWORD=$password \nmaster_auto_position=1 \n```\n\n- 参数解释\n\n| 字段                 | 解释                                   | 备注                                 |\n| -------------------- | -------------------------------------- | ------------------------------------ |\n| MASTER_HOST          | 新主库的ip                             |                                      |\n| MASTER_PORT          | 新主库的端口                           |                                      |\n| MASTER_USER          | 新主库的用户名                         |                                      |\n| MASTER_PASSWORD      | 新主库的密码                           |                                      |\n| MASTER_LOG_FILE      | 指定从哪个binlog文件进行同步           | 基于位点的同步才有                   |\n| MASTER_LOG_POS       | 指定从这个binlog文件的哪个位置开始同步 | 基于位点的同步才有                   |\n| master_auto_position | 表示使用GTID协议，使用GTID进行同步     | 基于GTID的同步才有，表示使用GTID协议 |\n\n\n\n#### 基于位点的切换\n\n##### 什么是位点\n\n简单地说，就是一条sql语句在binlog中的位置，叫做位点。\n\n在《mysql日志从入门到入土》一文中，介绍了三种格式的binlog，每一种格式的binlog都会有一个Pos的列，下面以row格式的binlog为例，看一下\n\n```sql\nmysql> show binlog events in 'xxx-bin.000002';\n+----------------+------+----------------+-----------+-------------+--------------------------------------+\n| Log_name       | Pos  | Event_type     | Server_id | End_log_pos | Info                                 |\n+----------------+------+----------------+-----------+-------------+--------------------------------------+\n| xxx-bin.000002 | 5043 | Anonymous_Gtid |         1 |        5122 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS' |\n| xxx-bin.000002 | 5122 | Query          |         1 |        5203 | BEGIN                                |\n| xxx-bin.000002 | 5203 | Table_map      |         1 |        5251 | table_id: 169 (zs.t)                 |\n| xxx-bin.000002 | 5251 | Delete_rows    |         1 |        5299 | table_id: 169 flags: STMT_END_F      |\n| xxx-bin.000002 | 5299 | Xid            |         1 |        5330 | COMMIT /* xid=1924 */                |\n+----------------+------+----------------+-----------+-------------+--------------------------------------+\n74 rows in set (0.00 sec)\n```\n\n其中Pos就是位点。\n\n##### 基于位点的同步\n\n上面已经列出了，使用位点同步的命令，这里在展示一遍\n\n```sql\nCHANGE MASTER TO \nMASTER_HOST=$host_name \nMASTER_PORT=$port \nMASTER_USER=$user_name \nMASTER_PASSWORD=$password \nMASTER_LOG_FILE=$master_log_name \nMASTER_LOG_POS=$master_log_pos  \n```\n\n其中`MASTER_LOG_FILE`指定从哪个binlog文件进行同步，`MASTER_LOG_POS`指定从这个binlog文件的哪个位置开始同步\n\n##### 怎么获取位点\n\n那么在主从切换的时候，怎么获取位点呢？\n\n考虑到切换过程中不能丢数据，所以我们找位点的时候，总是要找一个“**稍微往前**”的，然后再通过判断跳过那些在从库 B 上已经执行过的事务\n\n- 在新主库A1上：等待新主库A1把系统中的relay log全部执行完，为什么新主库A1还会有relay log?(因为新主库之前只是老主库A的一个备库，所以也有relay log)\n\n- 在新主库A1上：执行 show master status 得到当前 新主库A1上最新的 binlog File 和 binlog Position；\n\n  - ```sql\n    mysql> show master status;\n    +-----------------+----------+--------------+------------------+-------------------+\n    | File            | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |\n    +-----------------+----------+--------------+------------------+-------------------+\n    | xxxx-bin.000002 |     8156 |              |                  |                   |\n    +-----------------+----------+--------------+------------------+-------------------+\n    1 row in set (0.00 sec)\n    ```\n\n- 取老主库A故障的时刻 T；\n\n- 用 mysqlbinlog 工具解析 新主库A1 的 binlog File，得到 T 时刻的位点。\n\n  - ```sql\n    mysqlbinlog File --start-datetime=T --stop-datetime=T\n    ```\n\n- 假设我的崩溃时间是：2022-11-08 17:42:33\n\n  - ```sql\n    C:\\Program Files\\MySQL\\MySQL Server 8.0\\bin>mysqlbinlog.exe  -vv \"C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Data\\xxxx-bin.000002\" --start-datetime=\"2022-11-08 17:42:33\" --stop-datetime=\"2022-11-08 17:42:33\"\n    # The proper term is pseudo_replica_mode, but we use this compatibility alias\n    # to make the statement usable on server versions 8.0.24 and older.\n    /*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;\n    /*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;\n    DELIMITER /*!*/;\n    # at 4\n    #221101 14:12:06 server id 1  end_log_pos 125 CRC32 0xe443eab6  Start: binlog v 4, server v 8.0.26 created 221101 14:12:06 at startup\n    # Warning: this binlog is either in use or was not closed properly.\n    ROLLBACK/*!*/;\n    BINLOG '\n    trhgYw8BAAAAeQAAAH0AAAABAAQAOC4wLjI2AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n    AAAAAAAAAAAAAAAAAAC2uGBjEwANAAgAAAAABAAEAAAAYQAEGggAAAAICAgCAAAACgoKKioAEjQA\n    CigBtupD5A==\n    '/*!*/;\n    SET @@SESSION.GTID_NEXT= 'AUTOMATIC' /* added by mysqlbinlog */ /*!*/;\n    DELIMITER ;\n    # End of log file\n    /*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;\n    /*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/;\n    ```\n\n- 其中`end_log_pos 125`这个125就是时刻T崩溃时候的写入的位点。我们使用这个位点就可以了\n\n- 那么最终，就可以确定我们从库的同步命令就是下面这样\n\n  - ```sql\n    CHANGE MASTER TO \n    MASTER_HOST=$host_name \n    MASTER_PORT=$port \n    MASTER_USER=$user_name \n    MASTER_PASSWORD=$password \n    MASTER_LOG_FILE=xxxx-bin.000002\n    MASTER_LOG_POS=125\n    ```\n\n\n\n##### 基于位点同步的问题\n\n但是我们通过 mysqlbinlog 这个命令获取到的 位点 这个值，并不准确。\n\n为什么呢？当老主库A在崩溃前，insert了一条语句，并把这个语句的binlog发给了新主库A1和某一个从库C，然后崩溃了，此时从库C上是有这条记录的，但是我们获取新主库A1的同步位点的时候，获取的位点肯定是在这个insert语句之前的。因为其他从库还需要同步这个insert记录呢。\n\n但是我们的从库C，已经有这条记录的，在同步一次，就会报错：主键冲突。\n\n所以，基于位点的同步一般需要在从库上跳过错误：\n\n- 一种做法是：跳过一个事务。跳过命令的写法是：\n\n  - ```sql\n    set global sql_slave_skip_counter=1;\n    start slave;\n    ```\n\n- 另外一种方式是：通过设置 slave_skip_errors 参数，忽略指定的错误。\n\n\n\n#### 基于 GTID 的切换\n\n\n\n##### 什么是GTID？\n\nGTID 的全称是 Global Transaction Identifier，也就是全局事务 ID，是一个事务在**提交**的时候生成的，是这个事务的唯一标识。它由两部分组成，格式是：\n\n```sql\nGTID=server_uuid:gno\n```\n\n其中：\n\n- server_uuid 是一个实例第一次启动时**自动生成**的，是一个全局唯一的值；不是server_id哦\n- gno 是一个整数，初始值是 1，每次提交事务的时候分配给这个事务，并加 1。\n- 在官网上：GTID=source_id:transaction_id；这里只是为了更容易理解，才写成了：server_uuid:gno\n\n##### 启动GTID模式\n\nGTID模式默认是关闭的，在公司的环境中，是开启GTID模式的\n\n在mysql还没有启动的时候\n\n- 在启动mysql实例的时候，手动设置：`gtid_mode=on`和`enforce_gtid_consistency=on`\n\n在mysql已经启动的时候\n\n- 设置：`gtid_mode=on`\n- 设置：`enforce_gtid_consistency=on`\n- 下面展示了具体的开启步骤\n\n```sql\nmysql> show variables like 'gtid_mode';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| gtid_mode     | OFF   |\n+---------------+-------+\n1 row in set, 1 warning (0.00 sec)\n\nmysql> show variables like 'enforce_gtid_consistency';\n+--------------------------+-------+\n| Variable_name            | Value |\n+--------------------------+-------+\n| enforce_gtid_consistency | OFF   |\n+--------------------------+-------+\n1 row in set, 1 warning (0.00 sec)\n\nmysql> set gtid_mode ='ON';\nERROR 1229 (HY000): Variable 'gtid_mode' is a GLOBAL variable and should be set with SET GLOBAL\nmysql> set global gtid_mode='ON';\nERROR 1788 (HY000): The value of @@GLOBAL.GTID_MODE can only be changed one step at a time: OFF <-> OFF_PERMISSIVE <-> ON_PERMISSIVE <-> ON. Also note that this value must be stepped up or down simultaneously on all servers. See the Manual for instructions.\nmysql> set global gtid_mode='OFF_PERMISSIVE';\nQuery OK, 0 rows affected (0.11 sec)\n\nmysql> set global gtid_mode='ON_PERMISSIVE';\nQuery OK, 0 rows affected (0.03 sec)\n\nmysql> set global gtid_mode='ON';\nERROR 3111 (HY000): SET @@GLOBAL.GTID_MODE = ON is not allowed because ENFORCE_GTID_CONSISTENCY is not ON.\nmysql> set global enforce_gtid_consistency='ON';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> set global gtid_mode='ON';\nQuery OK, 0 rows affected (0.03 sec)\n\nmysql> show variables like 'gtid_mode';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| gtid_mode     | ON    |\n+---------------+-------+\n1 row in set, 1 warning (0.01 sec)\n\nmysql> show variables like 'enforce_gtid_consistency';\n+--------------------------+-------+\n| Variable_name            | Value |\n+--------------------------+-------+\n| enforce_gtid_consistency | ON    |\n+--------------------------+-------+\n1 row in set, 1 warning (0.00 sec)\n```\n\n\n\n##### 怎么查看GTID\n\nGTID是存在binlog中的，所以怎么查看binlog的，就怎么查看GTID\n\n我们知道GTID默认是关闭的，所以下面展示，在关闭GTID模式下，和开启GTID模式下，分别是什么样的\n\n关闭GTID模式下\n\n```sql\nmysql> show binlog events in 'xxx-bin.000002';\n+----------------+------+----------------+-----------+-------------+--------------------------------------+\n| Log_name       | Pos  | Event_type     | Server_id | End_log_pos | Info                                 |\n+----------------+------+----------------+-----------+-------------+--------------------------------------+\n| xxx-bin.000002 | 5043 | Anonymous_Gtid |         1 |        5122 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS' |\n| xxx-bin.000002 | 5122 | Query          |         1 |        5203 | BEGIN                                |\n| xxx-bin.000002 | 5203 | Table_map      |         1 |        5251 | table_id: 169 (zs.t)                 |\n| xxx-bin.000002 | 5251 | Delete_rows    |         1 |        5299 | table_id: 169 flags: STMT_END_F      |\n| xxx-bin.000002 | 5299 | Xid            |         1 |        5330 | COMMIT /* xid=1924 */                |\n+----------------+------+----------------+-----------+-------------+--------------------------------------+\n74 rows in set (0.00 sec)\n```\n\n开启GTID模式下\n\n```sql\nmysql> show binlog events in 'xxx-bin.000005';\n+----------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------+\n| Log_name       | Pos | Event_type     | Server_id | End_log_pos | Info                                                              |\n+----------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------+\n| xxx-bin.000005 |   4 | Format_desc    |         1 |         125 | Server ver: 8.0.26, Binlog ver: 4                                 |\n| xxx-bin.000005 | 125 | Previous_gtids |         1 |         156 |                                                                   |\n| xxx-bin.000005 | 156 | Gtid           |         1 |         235 | SET @@SESSION.GTID_NEXT= '13b96d6b-59ac-11ed-88d6-8c8caa828ba8:1' |\n| xxx-bin.000005 | 235 | Query          |         1 |         317 | BEGIN                                                             |\n| xxx-bin.000005 | 317 | Table_map      |         1 |         365 | table_id: 170 (zs.t)                                              |\n| xxx-bin.000005 | 365 | Update_rows    |         1 |         427 | table_id: 170 flags: STMT_END_F                                   |\n| xxx-bin.000005 | 427 | Xid            |         1 |         458 | COMMIT /* xid=2224 */                                             |\n+----------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------+\n7 rows in set (0.00 sec)\n```\n\n\n\n其中`SET @@SESSION.GTID_NEXT=`后面跟的值，就是GTID的值，那么为啥这俩不一样呢，GTID是怎么生成的呢？下面介绍\n\n\n\n##### GTID是如何生成的\n\n在没有开启GTID模式下，GTID的值，永远都是：`ANONYMOUS`，这个单词的意思是：匿名的;不知姓名的;名字不公开的;不具名的;没有特色的\n\n在 开启GTID 模式下，每个事务都会跟一个 GTID 一一对应。这个时候， GTID 有两种生成方式，而使用哪种方式取决于 session 变量 gtid_next 的值。\n\n```sql\nmysql> show variables like 'gtid_next';\n+---------------+-----------+\n| Variable_name | Value     |\n+---------------+-----------+\n| gtid_next     | AUTOMATIC |\n+---------------+-----------+\n1 row in set, 1 warning (0.00 sec)\n```\n\ngtid_next 是session级别的变量，不是全局global的变量，怎么验证呢，可以用下面的命令\n\n> show variables like 'gtid_next';       <==等价于==>    select @@gtid_next\n\n使用 select 命令，也可以查看变量的值，并且还可以验证当前变量是否是全局变量\n\n```sql\nmysql> select @@gtid_next;\n+-------------+\n| @@gtid_next |\n+-------------+\n| AUTOMATIC   |\n+-------------+\n1 row in set (0.00 sec)\n\nmysql> select @@global.gtid_next;\nERROR 1238 (HY000): Variable 'gtid_next' is a SESSION variable\n```\n\n那么`gtid_next`都有哪些取值呢？\n\n- `AUTOMATIC`：当设置为AUTOMATIC时(默认值)时，系统会自动分配一个GTID，如果事务回滚或者没有写入到binlog文件时则不会分配\n- `具体的GTID值`：可以设置该变量为一个具体的有效的GTID，这时服务器会将该GTID分配给下一个事务，就算该事务没有被写入binlog日志或者为空事务，该GTID也会被分配\n\n\n\n##### GTID集合\n\n在每一个mysql的实例上，都有两个关于GTID的集合：\n\n- `gtid_executed`：是在当前服务器上执行成功的事务的GTID集合\n- `gtid_purged`：是那些已经在当前服务器上提交的，但已经不存在于binlog文件中了（可能是人工删的，也可能是系统自动删的，这里不展开说了）\n\nGTID集合的格式：\n\n```sql\nmysql> select @@gtid_executed;\n+------------------------------------------+\n| @@gtid_executed                          |\n+------------------------------------------+\n| 13b96d6b-59ac-11ed-88d6-8c8caa828ba8:1-3 |\n+------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n\n\n##### GTID 的基本用法\n\n用一个例子，来描述GTID的基本用法\n\n要先开启GTID模式哦，并且设置：`gtid_next=AUTOMATIC`\n\n在实例 X 中创建一个表 t，并初始化一条数据\n\n```sql\n\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB;\n\ninsert into t values(1,1);\n```\n\n执行完之后，这条insert语句，就会记录一个binlog，并且有GTID\n\n```sql\nmysql> show master status;\n+------------------+----------+--------------+------------------+--------------------------------------------+\n| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set                          |\n+------------------+----------+--------------+------------------+--------------------------------------------+\n| xxxxx-bin.000005 |     1349 |              |                  | 13b96d6b-59ac-11ed-88d6-8c8caa828ba8:1-8   |\n+------------------+----------+--------------+------------------+--------------------------------------------+\n1 row in set (0.00 sec)\n\nmysql> show binlog events in 'xxxxx-bin.000005';\n+------------------+------+----------------+-----------+-------------+-------------------------------------------------------------------+\n| Log_name         | Pos  | Event_type     | Server_id | End_log_pos | Info                                                              |\n+------------------+------+----------------+-----------+-------------+-------------------------------------------------------------------+\n| xxxxx-bin.000005 |    4 | Format_desc    |         1 |         125 | Server ver: 8.0.26, Binlog ver: 4                                 |\n| xxxxx-bin.000005 |  125 | Previous_gtids |         1 |         156 |                                                                   |\n| xxxxx-bin.000005 |  156 | Gtid           |         1 |         235 | SET @@SESSION.GTID_NEXT= '13b96d6b-59ac-11ed-88d6-8c8caa828ba8:8' |\n| xxxxx-bin.000005 |  235 | Query          |         1 |         317 | BEGIN                                                             |\n| xxxxx-bin.000005 |  317 | Table_map      |         1 |         365 | table_id: 170 (zs.t)                                              |\n| xxxxx-bin.000005 |  365 | Update_rows    |         1 |         427 | table_id: 170 flags: STMT_END_F                                   |\n| xxxxx-bin.000005 |  427 | Xid            |         1 |         458 | COMMIT /* xid=2224 */                                             |\n+------------------+------+----------------+-----------+-------------+-------------------------------------------------------------------+\n7 rows in set (0.00 sec)\n```\n\n\n\n假设，现在这个实例 X 是另外一个实例 A 的从库，并且此时在实例 A 上执行了下面这条插入语句：\n\n```sql\ninsert into t values(1,1);\n```\n\n并且，这条语句在实例 A 上的 GTID 是 `aaa-bbb-ccc-ddd-eee:10`\n\n那么，实例 X 作为 A 的从库，就要同步这个事务过来执行，显然会出现主键冲突，导致实例 X 的同步线程停止。这时，我们应该怎么处理呢？\n\n处理方法就是，DBA手动执行下面的这个语句序列：\n\n```sql\nset gtid_next='aaa-bbb-ccc-ddd-eee:10';\nbegin;\ncommit;\nset gtid_next=automatic;\nstart slave;\n```\n\n其中，前三条语句的作用，是通过提交一个空事务，把这个 GTID 加到实例 X 的 GTID 集合中。\n\n这样，在次在实例 X 上执行`start slave`开始同步的时候（因为之前主键冲突，实例X上的同步线程停止了），就会跳过这个`aaa-bbb-ccc-ddd-eee:10`了，避免了主键冲突。\n\n在上面的这个语句序列中，start slave 命令之前还有一句 set gtid_next=automatic。这句话的作用是“恢复 GTID 的默认分配行为”，也就是说如果之后有新的事务再执行，就还是按照原来的分配方式。\n\n\n\n\n\n##### 基于GTID的同步\n\n了解了GTID，再来看一下基于GTID的切换，是什么样的？\n\n上面已经列出了 基于 GTID同步的语句，这里在展示一下\n\n```sql\nCHANGE MASTER TO \nMASTER_HOST=$host_name \nMASTER_PORT=$port \nMASTER_USER=$user_name \nMASTER_PASSWORD=$password \nmaster_auto_position=1 \n```\n\n使用GTID进行同步的话，就比较简单了，只需要指定`master_auto_position=1 `就可以了。表示这个主备关系使用的是 GTID 协议。\n\n当主库A挂掉后，此时假设，新主库 A1的 GTID 集合（gtid_executed）记为 set_a1，从库 B 的 GTID 集合（gtid_executed）记为 set_b。接下来，我们就看看现在的主备切换逻辑。\n\n- 实例 B 指定主库 A1，基于主备协议建立连接\n- 实例 B 把 set_b 发给主库 A1\n- 实例 A1算出 set_a 与 set_b 的差集，也就是所有存在于 set_a，但是不存在于 set_b 的 GTID 的集合\n- 实例A1判断这个差集需要的所有 binlog 事务在A1中的binlog是否全都存在（可能人为删除了，可能系统自动删除了，这里不展示说）\n  - 不全都存在：直接返回错误（在基于 GTID 的主备关系里，系统认为只要建立主备关系，就必须保证主库发给备库的日志是完整的。）\n  - 全都存在：A1 从自己的 binlog 文件里面，找出第一个不在 set_b 的事务，发给 B\n- 之后就从这个事务开始，往后读文件，按顺序取 binlog 发给 B 去执行。\n\n\n\n之后这个系统就由新主库 A1写入，主库 A1的自己生成的 binlog 中的 GTID 集合格式是：`server_uuid_of_A1:1-M`\n\n因为之前从库 B 的主库是A，所以之前B的 GTID 集合格式是 `server_uuid_of_A:1-N`\n\n那么把主库切换到从库A1之后 GTID 集合的格式就变成了 `server_uuid_of_A:1-N, server_uuid_of_A1:1-M`\n\n当然，主库 A1之前也是 A 的备库，因此主库 A1和从库 B 的 GTID 集合是一样的。这就达到了我们预期。\n\n```sql\nmysql> select @@gtid_executed;\n+---------------------------------------------+\n| @@gtid_executed                             |\n+---------------------------------------------+\n| server_uuid_of_A:1-N, server_uuid_of_A1:1-M |\n+---------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n\n\n#### 对比位点和GTID同步\n\n| 基于位点的同步                                               | 基于GTID的同步                                               |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| -                                                            | 需要开启GTID模式                                             |\n| 需要从库去主库找位点，而且不准确                             | 使用GTID集合，由**主库**内部判断，很准确                     |\n| 操作复杂，change master还需要指定binlog文件和位点            | change master只需要指向新主库即可                            |\n| 不做日志的完整性判断，基于位点的协议，是由备库决定的，备库指定哪个位点，主库就发哪个位点，不做日志的完整性判断。 | 做日志的完整性判断。在基于 GTID 的主备关系里，系统认为只要建立主备关系，就必须保证主库发给备库的日志是完整的。因此，如果实例 B 需要的日志已经不存在，A’就拒绝把日志发给 B。 |\n\n\n\n## 读写分离（一主多从的应用场景）\n\n\n\n### 什么是读写分离\n\n在写少读多的场景下，写入一般是有主库负责，然后在主库上有很多从库，从库提供只读功能，供业务查询\n\n\n\n### 读写分离的架构\n\n#### 直连架构\n\n客户端（client）主动做负载均衡，这种模式下一般会把数据库的连接信息放在客户端的连接层。也就是说，由客户端来选择后端数据库进行查询。\n\n```mermaid\nflowchart TD\nclient-->|write|mysqlA\nmysqlA-.->mysqlA1\nmysqlA1-.->mysqlA\nclient-->|read|mysqlB\nclient-->|read|mysqlC\nclient-->|read|mysqlD\nmysqlA-.->mysqlB\nmysqlA-.->mysqlC\nmysqlA-.->mysqlD\n```\n\n\n\n#### proxy架构\n\n在 MySQL 和客户端之间有一个中间代理层 proxy，客户端只连接 proxy， 由 proxy 根据请求类型和上下文决定请求的分发路由。\n\n```mermaid\nflowchart TD\nclient-->proxy\nproxy-->|write|mysqlA\nmysqlA-.->mysqlA1\nmysqlA1-.->mysqlA\nproxy-->|read|mysqlB\nproxy-->|read|mysqlC\nproxy-->|read|mysqlD\nmysqlA-.->mysqlB\nmysqlA-.->mysqlC\nmysqlA-.->mysqlD\n```\n\n| 直连结构                                                     | proxy结构                             |\n| ------------------------------------------------------------ | ------------------------------------- |\n| 少了一层 proxy 转发，所以查询性能稍微好一点儿                | 对客户端比较友好                      |\n| 架构简单，排查问题方便                                       | 架构复杂，因为还需要维护proxy的高可用 |\n| 高可用查，在出现主备切换、库迁移等操作的时候，客户端都会感知到，并且需要调整数据库连接信息 |                                       |\n\n\n\n### 读写分离的缺点（过期读）\n\n不论使用哪种架构，你都会碰到我们今天要讨论的问题：由于主从可能存在延迟，客户端执行完一个更新事务后马上发起查询，如果查询选择的是从库的话，就有可能读到刚刚的事务更新之前的状态。\n\n\n\n过期读的产生原因是因为：**主备延迟**。\n\n- 可以通过上文介绍的方法，来避免主备延迟：[为什么会产生主备延迟](#为什么会产生主备延迟)\n  - 复习：为什么会产生主备延迟：备库机器差，大事务，备库压力大（有读业务），并行复制能力\n- 但是呢，主备延迟，并不能100%的避免\n\n主备延迟不能100%的避免，那怎么解决过期读呢？\n\n\n\n### 过期读的解决方案\n\n- 强制走主库方案\n- sleep方案\n- 判断主备无延迟方案（配合semi-sync方案）\n- 等主库位点方案\n- 等GTID方案\n\n\n\n#### 强制走主库方案\n\n将请求分为两类：\n\n- 一类是：必须要拿到最新结果的请求\n- 一类是：可以读到旧数据的请求\n- 对于第一类，强制将其发到主库上。对于第二类，将其发到从库上。\n\n\n\n#### sleep方案\n\n在更新成功，客户端发起查询请求的时候，对这个请求sleep一下，等待主备同步完成，在查询。这样就可以返回最新的结果了。\n\n- 存在的问题：sleep多少是一个问题？\n- 如果sleep短了，去查询，仍然会有过期读；sleep多了，对用户的体验就很不好，感觉接口调用很慢\n\n\n\n#### 判断主备无延迟方案（配合semi-sync）\n\n解决的思路是：主要主备没有延迟了，说明主备上的数据都是一样的了。\n\n那么怎么确定主备无延迟了呢？\n\n- 通过 `show slave status`命令，以下是截取 `show slave status` 结果的部分截图。\n\n- ```sql\n  >show slave status\n  *************************** 1. row ***************************\n                 Slave_IO_State: Waiting for master to send event\n                    Master_Host: sh-dba-mysql-009\n                    Master_User: repl\n                    Master_Port: 3306\n                  Connect_Retry: 10\n                Master_Log_File: mysql-bin.000001\n            Read_Master_Log_Pos: 525\n                 Relay_Log_File: relay.000002\n                  Relay_Log_Pos: 738\n          Relay_Master_Log_File: mysql-bin.000001\n          ..................\n            Exec_Master_Log_Pos: 525\n          ..................\n          Seconds_Behind_Master: 0\n          ..................\n             Retrieved_Gtid_Set: 456f3e13-6000-11e8-8bda-002272a443bb:1-2\n              Executed_Gtid_Set: 456f3e13-6000-11e8-8bda-002272a443bb:1-2\n                  Auto_Position: 1\n          ..................\n  1 row in set (0.00 sec)\n  ```\n\n- 有三种方法判断主备无延迟\n\n  - 第一种：通过`Seconds_Behind_Master`,但是它的单位是秒，如果你觉得精度不够的话，还有下面两个方案\n  - 第二种：通过对比位点的方式\n    - `Master_Log_File`和`Read_Master_Log_Pos`，表示的是读到的主库的最新位点；\n    - `Relay_Master_Log_File`和`Exec_Master_Log_Pos`，表示的是备库执行的最新位点。\n    - 如果上面两组值完全相同，说明主备无延迟。\n  - 第三种：通过对比GTID的方式\n    - `Auto_Position=1` ，表示这对主备关系使用了 GTID 协议。\n    - `Retrieved_Gtid_Set`，是备库收到的所有日志的 GTID 集合；\n    - `Executed_Gtid_Set`，是备库所有已经执行完成的 GTID 集合。\n    - 如果这两个集合相同，说明主备无延迟。\n\n存在的问题：复习一下主备同步的流程：主库-->binlog----(网络)--->备库--->relaylog--->备库执行，主库产生的binlog要通过网络传给备库的，如果在传输的过程中，有一个查询过来了，通过这个方案，判断备库是没有延迟的，但是查出来的数据，仍然是过期读。\n\n**配合 semi-sync**\n\n要解决上面的问题，就要引入半同步复制，也就是 semi-sync replication：\n\n- 主库生成binlog，传给从库\n- 从库收到binlog之后，给主库一个ack\n- 主库收到ack之后，才给客户端返回“事务完成”的确认\n\n也就是说，如果启用了 semi-sync，就表示所有给客户端发送过确认的事务，都确保了备库已经收到了这个日志。\n\n这样，`semi-sync` 配合前面`判断主备无延迟方案`,就能够确定在从库上执行的查询请求，可以避免过期读。\n\n但是，`semi-sync`+`判断主备无延迟方案`只能适用于一主一备的场景。为什么呢？\n\n- 在一主多从的场景，主库只会受到一个从库返回的ack，就会给客户端返回事务提交确认了。\n- 当然，我们也可以设置为收到所有从库返回ack，但是这样，一个sql执行的代价就太大了。\n\n这样**如果查询落在了其他的从库上，还是避免不了过期读**。\n\n还有另一个问题，在业务高峰期，更新频率很快，`判断主备无延迟`可能一直不能成功，就会导致落在从库的查询，一直无法响应。\n\n还有一个问题，事务A在时刻A提交，事务B在时刻B提交，事务C在时刻C提交，由于事务一直在不停的产生和主备同步，这个时候，如果客户端来查询事务A的结果，此时，事务A早就已经完成了，但是由于`主备无延迟`判断不通过，事务A的结果一直无法返回。\n\n**问题：**\n\n- 一主多从的时候，在某些从库执行查询请求会存在过期读的现象；\n- 在更新频繁，持续延迟的情况下，主备延迟判断不通过，可能出现过度等待的问题。\n\n\n\n#### 等主库位点方案\n\n需要使用下面这个sql命令\n\n```sql\nselect master_pos_wait(file, pos, timeout);\n```\n\n\n\n流程\n\n- 客户端发起查询请求，落在任意一个从库上，假设是`从库-C`\n- 在`从库-C`执行查询请求之前，先执行`show master status`得到当前主库执行到的 File 和 Position；\n- 然后在当前`从库-C`上执行：`select master_pos_wait(file, pos, timeout);`\n  - file：是第二步获取到的file\n  - pos：是第二步获取到的Position\n  - timeout：超时时间，单位秒，这个命令在超时时间内没返回，就返回-1\n- 在`从库-C`上的这个命令会返回一个值\n  - NULL：表示备库在同步执行期间，备库同步线程发生异常\n  - -1：超时了，会返回-1\n  - 大于等于0的正整数：表示这个命令执行的时候，`从库-C`已经把主库上的这个日志同步完成了。\n- 如果返回值是 >=0 的正整数，则在这个从库执行查询语句；\n- 否则，到主库执行查询语句。\n\n\n\n如果所有的从库都延迟超过 1 秒了，那查询压力不就都跑到主库上了吗？确实是这样。所以就需要业务开发同学做好限流策略了。\n\n\n\n#### 等GTID 方案\n\n需要使用下面这个sql语句\n\n```sql\n select wait_for_executed_gtid_set(gtid_set, timeout);\n```\n\n流程：\n\n- 客户端发起查询请求，落在任意一个从库上，假设是`从库-C`\n- 在`从库-C`执行查询请求之前，先执行`select wait_for_executed_gtid_set(gtid_set, timeout);`\n  - gtid_set：在前面等位点的方案中，是主动去主库执行 show master status。而5.7.6 版本开始，会把这个事务的 GTID 返回给客户端（需要设置`session_track_gtids`），这样等 GTID 的方案就可以减少一次查询。\n  - timeout：超时时间，单位秒，这个命令在超时时间内没返回，就返回 1\n- 在`从库-C`上的这个命令会返回一个值\n  - 等待，直到这个库执行的事务中包含传入的 gtid_set，返回 0\n  - 超时返回 1\n- 如果返回值是 0，则在这个从库执行查询语句；\n- 否则，到主库执行查询语句。\n\n跟等主库位点的方案一样，等待超时后是否直接到主库查询，需要业务开发同学来做限流考虑。\n\n\n\n#### 思考题\n\n如果使用 GTID 等位点的方案做读写分离，在对大表做 DDL 的时候会怎么样。\n\n- 主库做DDL，典型的大事务，如果持续十分钟，那么从库就是延迟10分钟\n- 在这10分钟内的所有请求，都会全部打到主库上\n- 而主库正在做DDL，更加加重了主库的负担。\n\n解决：\n\n- 业务低峰期操作\n- 使用gh-ots\n","tags":["读写分离","mysql","主备一致","高可用","主备切换","主备延迟","GTID","binlog"],"categories":["JAVA","数据库","MYSQL"]},{"title":"mysql的锁从入门到入土","url":"/note/JAVA/数据库/MYSQL/mysql的锁从入门到入土/","content":"\n# mysql的锁\n\n\n\n---\n\n\n\n根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表锁和行锁三类。\n\n\n\n## 写在前面\n\n\n\n\n\n### 共享锁和独占锁（排它锁）\n\n共享锁：`Shared Locks`，简称`S锁`\n\n独占锁（排它锁）：`Exclusive Locks`简称`X锁`\n\n\n\n### 意向共享锁与意向独占锁\n\n意向共享锁：`Intention Shared Lock`，简称`IS锁`。作用是：当一个事务准备给一条记录加`S锁`的时候，会先给这条记录所在的表加一个`IS锁`\n\n意向独占锁：`Intention Exclusive Lock`，简称`IX锁`。作用是：当一个事务准备在一条记录上加`X锁`的时候，会先给这条记录所在的表加一个`IX锁`\n\n\n\n\n\n### 读锁和写锁\n\n写锁是排它锁（也叫独占锁），意味着其他线程不能读也不能写，当前线程可读写；\n\n读锁是共享锁，意味着其他线程只能读不能写，本线程也不能写；\n\n全局锁只有读锁。\n\n表锁有读锁和写锁。\n\nMDL锁有读锁和写锁。\n\n\n\n\n\n\n\n### 当前读和快照读\n\n在mysql的默认隔离级别RR下\n\n快照读：一般情况下select * from ....where ...  是快照读，不会加锁，有MVCC支持\n\n当前读： `for update`,`lock in share mode`,`update`,`delete`都属于当前读，会加锁\n\n- lock in share mode ： 加的是共享锁（s锁）\n- for update ： 加的是独占锁（排他锁 / x锁）\n- update：加的是独占锁（排他锁 / x锁），update 的加锁语义和 for update 是一致的\n- delete：加的是独占锁（排他锁 / x锁），delete的加锁语义和 for update 是一致的\n\n\n\n## 全局锁\n\n\n\n### 什么是全局锁\n\n顾名思义，全局锁就是对整个数据库实例加锁。\n\n全局锁只有一个读锁。\n\n\n\n### 何时打开 / 关闭全局锁（FTWRL）\n\n打开\n\n- 只能手动打开，命令是 `Flush tables with read lock (FTWRL)`。\n- 打开全局锁之后，整个数据库处于只读状态，可以查询。\n- 之后以下语句会被阻塞：DML（数据的增删改，可以查询）、DDL（包括建表、修改表结构等）和更新类事务的提交语句。\n\n关闭\n\n- unlock tables 可以解除\n- client 断开的时候自动释放\n\n\n\n### 全局锁使用场景（全库逻辑备份）\n\n全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。\n\n\n\n#### 全库逻辑备份\n\n为了避免备份过程中出现数据不一致的情况，所以，在备份的时候需要保证数据一致性。\n\n#### 怎么保证备份时数据一致性\n\n全局锁就是保证数据一致性的方法之一，但是全局锁也有很多缺点：\n\n- 在主库上备份，备份的过程中，整个库处于只读状态，业务停止\n- 在备库上备份，会导致同步过来的`binlog`无法执行，主备延迟\n\n那么，使用一致性视图`readview`也是一个办法，而且不会阻止数据库的更新，但是需要满足两个条件\n\n- 需要引擎支持MVCC\n- 需要数据库隔离级别是RR（readview在RR级别下是第一个查询时开启，直到事务结束，都在一个readview下； 但是RC界别下是每一个查询都会开启readview，这样的话，在备份的时候，仍然是数据不一致的。）\n- MySQL官方自带的逻辑备份工具是 mysqldump。当使用`–single-transaction` 参数的时候，就是使用这种方式\n\n那么，使用`set global readonly=true`呢？确实 readonly 方式也可以让全库进入只读状态，但不建议，主要有两个原因：\n\n- 在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。\n- 在异常处理机制上有差异：使用 FTWRL 命令之后，如果client异常断开，那么 MySQL 会自动释放这个全局锁，整库恢复正常。而将整个库设置为 readonly 之后，如果client发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高\n\n\n\n## 表锁（表锁+MDL锁）\n\nMySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)\n\n\n\n### 表锁\n\n#### 什么是表锁\n\n顾名思义，表锁就是对某张表加锁。\n\n全局锁只有一个读锁，而表锁是有：读锁，和写锁的。\n\n\n\n#### 何时打开/关闭表锁\n\n打开\n\n- 手动打开，表锁的语法是` lock tables … read/write`\n\n关闭\n\n- unlock tables可以解除\n- client 断开的时候自动释放\n\n\n\n#### 表锁的读锁和写锁\n\n全局锁只有一个读锁，而表锁是有：读锁，和写锁的。\n\nlock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。\n\n举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句之后：\n\n- 其他线程写 t1、读写 t2 的语句都会被阻塞。\n- 同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。\n\n\n\n#### 表锁的使用场景\n\n在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。\n\n而对于 InnoDB 这种支持行锁的引擎，一般不用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。\n\n\n\n### 元数据锁（meta data lock，MDL)\n\n\n\n#### 什么是元数据锁\n\nMySQL 5.5 版本中引入了 MDL\n\n执行DML时，加 MDL 读锁\n\n执行DDL时，加 MDL 写锁\n\n\n\n#### 怎么打开/关闭元数据锁\n\nMDL 不需要显式使用，在访问一个表的时候会被自动加上\n\n当对表做DML操作的时候，加 MDL 读锁\n\n当对表做DDL操作的时候，加 MDL 写锁\n\n\n\n#### MDL的使用场景\n\n##### 保证读写的正确性\n\nMDL 的作用是，保证读写的正确性\n\n你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一个字段，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。\n\n\n\n##### ONLINE DDL的实现原理\n\n1. 获取对应要操作表的 MDL写锁\n2. MDL写锁 降级成 MDL读锁\n3. 真正做DDL操作\n4. MDL读锁 升级成 MDL写锁\n5. 释放MDL锁\n\n第一步获取写锁之后，降级为读锁，此时DML可以正常执行（DML需要的是读锁），但是DDL执行不了（DDL需要的是写锁），这样就支持了online ddl\n\n\n\n#### MDL锁的死锁场景\n\n##### 如何安全的给小表加索引\n\n给一个小表加个字段，导致整个库挂了\n\n| session A                          | session B                | session C                                                    | session D                                                    |\n| ---------------------------------- | ------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| begin<br/>select * from t limit 1; |                          |                                                              |                                                              |\n|                                    | select * from t limit 1; |                                                              |                                                              |\n|                                    |                          | alter table t add f int;<br><font color='red'>blocked</font> |                                                              |\n|                                    |                          |                                                              | select * from t limit 1;<br/><font color='red'>blocked</font> |\n\n\n\nsessionA：需要MDL读锁，可以正常执行\n\nsessionB：需要MDL读锁，因为读锁是共享锁，可以正常执行\n\nsessionC：需要MDL写锁，因为sessionA和sessionB还没有提交，MDL读锁还没有释放，所以MDL写锁申请不了，会等待\n\nsessionD：需要MDL读锁，因为sessionC在阻塞，所以sessionD也阻塞了\n\n此时整张表处于不可读写状态，如果这个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。\n\n**事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。**([两阶段锁协议](#两阶段锁协议)，后面会说)\n\n\n\n怎么安全的给小表加字段\n\n- 解决大事务，事务不提交，就会一直占着 MDL 锁\n\n如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，你该怎么做呢？\n\n- 在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。\n\n\n\n\n\n## 行锁与死锁\n\n\n\nMySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。\n\n不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。\n\nInnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。\n\n\n\n### 什么是行锁\n\n顾名思义，行锁就是针对数据表中行记录的锁。\n\n这很好理解，比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。\n\n\n\n### 两阶段锁协议\n\n在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是**两阶段锁协议**。\n\n事务中，行锁是在语句执行时才加上的（[案例十：IN 语句加锁（动态加锁）](#案例十：IN 语句加锁（动态加锁）)），不是事务开始就加上，但释放是统一在事务结束时才释放。\n\n根据这个特性，对于高并发的行记录的操作语句就可以尽可能的安排到最后面，以减少锁等待的时间，提高并发性能\n\n\n\n### 何时加 / 释放行锁\n\n事实上，在事务执行过程中，行锁不会单独加上，因为加锁的最小粒度是 next-key lock（[next-key lock](#next-key lock)）\n\n具体是怎么加的，可以参考：[加锁/释放锁/查看锁规则](#加锁/释放锁/查看锁规则)\n\n但是呢，我们可以简单的理解：在事务中，行锁是在语句执行时才加上的（[案例十：IN 语句加锁（动态加锁）](#案例十：IN 语句加锁（动态加锁）)），如果语句执行的过程中扫描到了行，会给这个行加行锁。\n\n释放是统一在事务结束时才释放\n\n\n\n### 什么是死锁\n\n| session A                               | 事务 B                         |\n| --------------------------------------- | ------------------------------ |\n| begin<br/>update t set k=1 where id =1; | begin;                         |\n|                                         | update t set k=2 where id =2;  |\n| update t set k=3 where id = 2;          |                                |\n|                                         | update t set k=4 where id = 1; |\n\n这时候，事务 A 在等待事务 B 释放 id=2 的行锁，而事务 B 在等待事务 A 释放 id=1 的行锁。 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。\n\n\n\n### 出现死锁怎么办\n\n当出现死锁以后，有两种策略：\n\n- 死锁等待：直接进入等待，直到超时。这个超时时间可以通过参数 `innodb_lock_wait_timeout` 来设置。\n- 死锁检测（推荐）：发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 on，表示开启这个逻辑。\n\n\n\n#### 死锁等待\n\n在 InnoDB 中，`innodb_lock_wait_timeout` 的默认值是 50s\n\n意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。\n\n- 设置比较大的值：对于在线服务来说，这个等待时间往往是无法接受的\n- 设置比较小的值：虽然死锁可以很快解开，但是容易出现误杀，比如正常的锁等待，也会导致线程退出\n\n\n\n#### 死锁检测（推荐）\n\n**开启死锁检测**：`innodb_deadlock_detect`设置为`on`，默认值就是`on`\n\n**死锁检测是怎么检测的**：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待（也就是死锁）。\n\n**死锁检测的例子**：新来的线程F，被锁了后就要检查锁住F的线程（假设为D）是否被锁，如果没有被锁，则没有死锁，如果被锁了，还要查看锁住线程D的是谁，如果是F，那么肯定死锁了，如果不是F（假设为B），那么就要继续判断锁住线程B的是谁，一直走直到发现线程没有被锁（无死锁）或者被F锁住（死锁）才会终止。\n\n**死锁检测的代价**\n\n举个例子：1000个线程并发更新同一行，发生了锁等待，这个时候需要检测是否发生了死锁？\n\n那么死锁检测操作就是 100 万（1000 * 1000）这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。\n\n因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。\n\n\n\n### 行锁的死锁场景\n\n#### 热点行更新的性能问题\n\n热点行更新问题，就是大量的并发线程，更新同一行记录的问题，会由于并发量太大，可能导致死锁的产生，触发死锁检测，耗费大量cpu\n\n**解决方案**\n\n- 如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉\n- 控制并发度：死锁检测的成本高，所以控制并发度，就可以减少死锁检测的性能消耗，同时还能减少死锁的产生\n- 将热点行更新问题拆分为更新多行（具体可以从业务角度考虑）\n\n\n\n## 间隙锁（幻读）\n\n通过幻读的产生，来了解一下间隙锁\n\n\n\n### 幻读\n\n\n\n#### 什么是幻读\n\n结论先行：幻读的产生前提\n\n- 只有在**当前读**的情况下才可能产生（[当前读和快照读](#当前读和快照读)），所以下面的演示，查询语句使用了 `for update`\n- 只有 `insert` 的记录才是幻读，update的不是\n\n\n\n#### 幻读的产生\n\n以下的测试是在InnoDB的默认隔离级别RR下\n\n准备一张表和数据\n\n```sql\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n这个表除了主键 id 外，还有一个普通索引 c，d是普通列没有索引，初始化语句在表中插入了 6 行数据。\n\n\n\n下面这个这个场景，只是假设，是为了引入幻读，然后引出后面要说的`间隙锁`。\n\n这个场景实际上不会产生的，因为有`间隙锁`的存在。\n\n这里假设`间隙锁`不存在\n\n|      | session A                                                    | session B                                            | session C                                           |\n| ---- | ------------------------------------------------------------ | ---------------------------------------------------- | --------------------------------------------------- |\n| T1   | begin;<br/>select * from t where d = 5 for update;<br/>结果：(5,5,5) |                                                      |                                                     |\n| T2   |                                                              | begin;<br/>update t set d=5 where id =0;<br/>commit; |                                                     |\n| T3   | select * from t where d = 5 for update;<br/>结果：(0,0,5),(5,5,5) |                                                      |                                                     |\n| T4   |                                                              |                                                      | begin;<br/>insert into t values(1,1,5);<br/>commit; |\n| T5   | select * from t where d = 5 for update;<br/>结果：(0,0,5),(1,1,5),(5,5,5) |                                                      |                                                     |\n| T6   | commit;                                                      |                                                      |                                                     |\n\n\n\n在 session A 中执行了三次一模一样的查询，由于这些查询语句使用了 for update ，所以都是当前读，并且加上了 x 锁。\n\n- 第一次查询：结果：(5,5,5)，没问题\n- 第二次查询：结果：(0,0,5),(5,5,5)，因为是当前读，所以可以读到 session B 更新后的记录。（但是这不是幻读）\n- 第三次查询：结果：(0,0,5),(1,1,5),(5,5,5)，因为是当前读，所以可以读到 session C 插入的记录。（是幻读）\n\n通过上面的结果看，因为使用了当前读，能查出来这些数据，是没有问题的。\n\n但是真的没有问题吗？如果你觉得上面的步骤，没有问题，那么你就有问题了，接着往下看：看看幻读的问题\n\n\n\n#### 幻读的问题\n\n##### 幻读破坏语义\n\n回过头来，我们来看这个查询语句：`select * from t where d = 5 for update;`，这个查询语句是什么意思？\n\n- 查询 d=5的记录，并且加上了一个x锁；\n\n注意：加的是一个x锁，按照常理考虑，对所有d=5的记录，都应该有个x锁，既然有了x锁，那么针对d=5的记录，就不能再操作了？\n\n那么 session B 的更新（update t set d=5 where id =0;），和 session C 的插入（insert into t values(1,1,5);），都变化了 d=5 的记录，这是怎么回事呢？\n\n这是因为：\n\n- 在第一次执行查询的时候，只给 id=5 这一行记录加上了锁。所以 session B 可以更新 id=0 这一条记录。但是，这样就破坏了第一次查询语句要锁住所有 d=5 的行的加锁声明。\n- session C 也是一样的道理，对 id=1 这一行的插入，也是破坏了要锁住所有 d=5 的行的加锁声明。\n\n这就是破坏了语义。\n\n\n\n##### 幻读导致数据不一致\n\n我们所说的数据一致性，不止是数据库内部数据状态在此刻的一致性，还包含了数据和日志在逻辑上的一致性。\n\n思考下面这个问题，我们执行这个查询语句：`select * from t where d = 5 for update;`\n\n其中字段 d 上是没有索引的，因此这条查询语句会做全表扫描。\n\n那么，其他被扫描到的，但是不满足条件的记录，会不会被加锁呢？\n\n\n\n###### 假设只锁d=5其他记录不加锁\n\n考虑下面这个场景（当然这个场景也是假设的）\n\n先把数据准备一下：\n\n```sql\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n\n\n|      | session A                                                    | session B                                                    | session C                                                    |\n| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| T1   | begin<br/>select * from t where d=5 for update;<br>update t set d = 100 where d = 5; |                                                              |                                                              |\n| T2   |                                                              | begin;<br/>update t set d=5 where id =0;<br/>update t set c=5 where id =0;<br/>commit; |                                                              |\n| T3   | select * from t where d = 5 for update;                      |                                                              |                                                              |\n| T4   |                                                              |                                                              | begin;<br/>insert into t values(1,1,5);<br/>update t set c=5 where id=1;<br/>commit; |\n| T5   | select * from t where d = 5 for update;                      |                                                              |                                                              |\n| T6   | commit;                                                      |                                                              |                                                              |\n\n通过上面的一波执行，看一下数据库的最终结果是什么样的\n\n```sql\n之前：(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n之后：(0,5,5),(1,5,5),(5,5,100),(10,10,10),(15,15,15),(20,20,20),(25,25,25)\n```\n\n这样看，这些数据也没啥问题，但是我们再来看看这时候 binlog 里面的内容。\n\n- session B 的事务先提交，binlog先记录：update t set d=5 where id=0; update t set c=5 where id=0;\n- 然后是 session C 的事务提交了，binlog记录：insert into t values(1,1,5); update t set c=5 where id=1;\n- 最后是 session A 的事务提交了，binlog记录：update t set d = 100 where d = 5;\n- 汇总之后，就是下面这样：\n\n```sql\n//session B\nupdate t set d=5 where id=0; \nupdate t set c=5 where id=0; \n//session C\ninsert into t values(1,1,5);\nupdate t set c=5 where id=1;\n//session A\nupdate t set d=100 where d=5;\n```\n\n然后备库执行上面的binlog之后，结果是什么样的呢？\n\n```sql\n之前：(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n之后：(0,5,100),(1,5,100),(5,5,100),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n到这里，比较一下，你就发现了问题\n\n```sql\n主库：(0,5,5),(1,5,5),(5,5,100),(10,10,10),(15,15,15),(20,20,20),(25,25,25)\n备库：(0,5,100),(1,5,100),(5,5,100),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n这个问题很严重，是不行的。\n\n\n\n所以我们认为，只锁d=5其他记录不加锁，这个假设不合理，要改。\n\n那怎么改呢？我们把扫描过程中碰到的行，也都加上x锁，再来看看执行效果。\n\n\n\n###### 假设给所有扫描到的记录都加锁\n\n\n\n把扫描过程中碰到的行，也都加上写锁，再来看看执行效果。\n\n\n\n考虑下面这个场景（当然这个场景也是假设的）\n\n先把数据准备一下：\n\n```sql\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n\n\n|      | session A                                                    | session B                                                    | session C                                                    |\n| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| T1   | begin<br/>select * from t where d=5 for update;<br>update t set d = 100 where d = 5; |                                                              |                                                              |\n| T2   |                                                              | begin;<br/>update t set d=5 where id =0;<br/><font color='red'>阻塞blocked</font><br/>update t set c=5 where id =0;<br/>commit; |                                                              |\n| T3   | select * from t where d = 5 for update;                      |                                                              |                                                              |\n| T4   |                                                              |                                                              | begin;<br/>insert into t values(1,1,5);<br/>update t set c=5 where id=1;<br/>commit; |\n| T5   | select * from t where d = 5 for update;                      |                                                              |                                                              |\n| T6   | commit;                                                      |                                                              |                                                              |\n| T7   |                                                              | session A释放锁之后，继续执行                                |                                                              |\n\n由于 session A 把所有的行都加了写锁，所以 session B 在执行第一个 update 语句的时候就被锁住了。需要等到 T6 时刻 session A 提交以后，session B 才能继续执行。\n\n通过上面的一波执行，看一下数据库的最终结果是什么样的\n\n```sql\n之前：(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n之后：(0,5,5),(1,5,5),(5,5,100),(10,10,10),(15,15,15),(20,20,20),(25,25,25)\n```\n\n这样看，这些数据也没啥问题，但是我们再来看看这时候 binlog 里面的内容。\n\n- session B 被阻塞了，所以session C是先提交的，binlog先记录：insert into t values(1,1,5);update t set c=5 where id=1;\n- 然后session A执行完了，binlog记录：update t set d = 100 where d = 5;\n- 最后是session B执行完了，binlog记录：update t set d=5 where id =0; update t set c=5 where id =0;\n- 汇总之后，就是下面这样：\n\n```sql\n//session C\ninsert into t values(1,1,5); \nupdate t set c=5 where id=1; \n//session A\nupdate t set d=100 where d=5;\n//session B\nupdate t set d=5 where id=0; /*(0,0,5)*/\nupdate t set c=5 where id=0; /*(0,5,5)*/\n```\n\n然后备库执行上面的binlog之后，结果是什么样的呢？\n\n```sql\n之前：(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n之后：(0,5,5),(1,5,100),(5,5,100),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n到这里，比较一下，你就发现了问题\n\n```sql\n主库：(0,5,5),(1,5,5),(5,5,100),(10,10,10),(15,15,15),(20,20,20),(25,25,25)\n备库：(0,5,5),(1,5,100),(5,5,100),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n虽然 session B的更新的问题解决了，但是 session C 插入的那一条记录，还是不对。这个问题也很严重。\n\n那么为什么呢？我都对所有的记录，都加了锁，为什么还是不行呢？\n\n- 因为我们在加锁的时候，session C还没有执行的，而等 session C执行完之后，session A已经加锁完毕了\n- 这就导致了 id = 1 这一样记录，没有加上锁。\n\n也就是说，即使把所有的记录都加上锁，还是阻止不了新插入的记录，\n\n\n\n### 间隙锁（InnoDB解决幻读）\n\n由于行锁只能锁住行，当我们向两条记录中间新插入记录的时候，会导致幻读的产生。\n\n对于新插入记录这个动作，要更新的是记录之间的“间隙”。\n\n因此，为了解决幻读问题，InnoDB 只好引入新的锁，也就是间隙锁 (Gap Lock)。\n\n\n\n#### 什么是间隙锁（Gap Lock）\n\n顾名思义，间隙锁，锁的就是两个值之间的空隙。比如下面这个表 t，初始化插入了 6 个记录，这就产生了 7 个间隙。\n\n```sql\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n我们插入了 6 个记录，就产生了 7 个间隙\n\n- (-∞,0)\n- (0,5)\n- (5,10)\n- (10,15)\n- (15,20)\n- (25,+∞)\n\n这样，当执行 `select * from t where d=5 for update` 的时候，因为扫描全表；\n\n就不止是给数据库中已有的 6 个记录加上了行锁\n\n还同时加了 7 个间隙锁。这样就确保了无法再插入新的记录。\n\n\n\n#### 何时加 / 释放间隙锁\n\n间隙锁是在RR隔离级别下才会生效的。\n\n如果不想使用间隙锁：可以将隔离级别设置成RC，但是同样的，需要解决可能出现的数据和日志不一致问题，需要把 binlog 格式设置为 row\n\n\n\n事实上，在事务执行过程中，间隙锁不会单独加上，因为加锁的最小粒度是 next-key lock（[next-key lock](#next-key lock)）\n\n具体是怎么加的，可以参考：[加锁/释放锁/查看锁规则](#加锁/释放锁/查看锁规则)\n\n但是呢，我们可以简单的理解：在事务中，间隙锁是在语句执行时才加上的（[案例十：IN 语句加锁（动态加锁）](#案例十：IN 语句加锁（动态加锁）)）\n\n释放是统一在事务结束时才释放\n\n\n\n#### 间隙锁和行锁的互斥关系\n\n\n\n间隙锁与间隙锁之间，不存在任何冲突。\n\n间隙锁与行锁之间，不存在任何冲突。\n\n跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。间隙锁之间都不存在冲突关系。\n\n\n\n\n\n#### 面试：RR隔离级别能解决幻读吗\n\n先说结论：能解决，但不能完全解决。\n\n在快照读的情况下：\n\n- RR隔离级别下，事务开启的时候，就会启动一个readview，是可以解决脏读、不可重复读以及幻读问题的。\n\n- RC隔离级别下，都是当前读，没有快照读，因此无法解决不可重复读以及幻读问题。\n\n在当前读的情况下：\n\n- RR隔离级别下，由于有行锁和间隙锁的存在，当前读也是可以解决脏读、不可重复读以及幻读问题的\n\n- RC隔离级别下，只有行锁，没有间隙锁，因此无法解决不可重复读以及幻读问题。\n\n那么，为什么说RR隔离级别下没有完全解决幻读问题呢？\n\n- 事务1 先快照读，事务2新增了一条数据并提交事务，事务1再当前读。\n- 事务1 先快照读，事务2新增了一条数据并提交事务，事务1对事务2提交的数据进行了修改，事务1再次快照读。\n\n情况1不用说了吧，很好理解。对于情况2， 事务1的更新操作不属于快照读，因此事务1的更新操作是可以生效的，而当前数据会记录最新修改的记录，最新修改的记录为当前事务自己，所以是能看到的。\n\n\n\n\n\n### next-key lock\n\n#### 什么是next-key lock\n\n间隙锁 和 行锁 合称： next-key lock，每个 next-key lock 是前开后闭区间。\n\n比如下面这张表\n\n```sql\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n我们插入了 6 个记录，如果用 select * from t for update 要把整个表所有记录锁起来。\n\n此时产生了：6个行锁，7个间隙锁；7个next-key lock\n\n7个next-key lock 分别是 \n\n- (-∞,0]\n- (0,5]\n- (5,10]\n- (10,15]\n- (15,20]\n- (20, 25]\n- (25, +supremum]     \n  - 这个 `supremum `是啥？\n  - 这是因为 +∞是开区间。所以 InnoDB 给每个索引加了一个不存在的最大值 supremum，这样才符合我们前面说的“都是前开后闭区间”\n\n\n\n\n\n#### 何时加 / 释放next-key lock\n\n\n\n在事务执行过程中，加锁的最小粒度是 next-key lock（[next-key lock](#next-key lock)），具体是怎么加的，可以参考：[加锁/释放锁/查看锁规则](#加锁/释放锁/查看锁规则)\n\n释放是统一在事务结束时才释放\n\n\n\n### 间隙锁的死锁场景\n\n\n\n间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。\n\n\n\n场景：任意锁住一行，如果这一行不存在的话就插入，如果存在这一行就更新它的数据\n\n```sql\n\n-- 准备表和数据\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n看下面这个场景：\n\n| session A                                                    | session B                                                    |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| begin;<br/>select * from t where id = 9 for update;          |                                                              |\n|                                                              | begin<br/>select * from t where id = 9 for update;           |\n|                                                              | insert into t values(9,9,9);<br/><font color='red'>blocked</font> |\n| insert into t values(9,9,9);<br/><font color='red'>ERROR 1231 (40001) : Deadlock found</font> |                                                              |\n\n\n\n- session A 执行 select … for update 语句，由于 id=9 这一行并不存在，因此会加上间隙锁 (5,10);\n- session B 执行 select … for update 语句，同样会加上间隙锁 (5,10)，间隙锁之间不会冲突，因此这个语句可以执行成功；\n- session B 试图插入一行 (9,9,9)，被 session A 的间隙锁挡住了，只好进入等待；\n- session A 试图插入一行 (9,9,9)，被 session B 的间隙锁挡住了。\n- 至此，两个 session 进入互相等待状态，形成死锁。\n\n\n\n间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。\n\n间隙锁是在RR隔离级别下才会生效的。\n\n如果不想使用间隙锁：可以将隔离级别设置成RC，但是同样的，需要解决可能出现的数据和日志不一致问题，需要把 binlog 格式设置为 row（使用statement容易出现数据不一致）\n\n\n\n\n\n### 题外话：隔离级别RC+ROW\n\nmysql 默认的隔离级别是RR，但是一般在大公司内部的数据库，都是使用RC格式，并且statement格式都是：ROW，这是为什么呢？\n\n比如我司用的就是：RC+ROW\n\n```sql\nmysql> show variables like '%isolation%';\n+-----------------------+----------------+\n| Variable_name         | Value          |\n+-----------------------+----------------+\n| transaction_isolation | READ-COMMITTED |\n+-----------------------+----------------+\n1 row in set, 1 warning (0.03 sec)\n\nmysql> show variables like '%binlog_format%';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| binlog_format | ROW   |\n+---------------+-------+\n1 row in set, 1 warning (0.00 sec)\n```\n\n\n\n为什么要使用 RC+ROW呢？\n\n结论先行：为了提升并发和降低死锁产生的概率；同时保证数据的一致性；\n\n- 前面说到，间隙锁的存在会导致锁的范围变大，就更容易导致死锁的产生，比如上面的场景；\n- 而间隙锁是只有在 RR  隔离级别下，才会有的。\n- 所以为了避免使用间隙锁而导致的问题，可以把隔离级别设置成：RC\n- 另外，binlog的格式使用 ROW，是因为默认使用的STATEMENT由于记录的是sql原文，有可能导致主从数据不一致。\n\n\n\n有的朋友发现他们公司就使用的是RC+ROW 的组合。他曾问他们公司的 DBA 说，你为什么要这么配置。DBA 直接答复说，因为大家都这么用呀。\n\n但其实我想说的是，配置是否合理，跟**业务场景**有关，需要具体问题具体分析。但是，如果 DBA 认为之所以这么用的原因是“大家都这么用”，那就有问题了，或者说，迟早会出问题。\n\n#### 问：什么场景，需要RR来保证？\n\n- 一般要求数据一致性的时候，比如备份，比如数据核对校验的场景，金融行业场景。\n\n#### 问：大家都用RC，可是逻辑备份的时候，mysqldump 为什么要把备份线程设置成RR呢？\n\n- 通过一致性视图保证备份时数据的一致性\n- 同时备份的时候不阻塞数据库的DDL\n\n#### 问：在备份期间，备份线程用的是RR，而业务线程用的是RC。同时存在会不会有问题？\n\n- 没问题\n- RR是备份线程单独开启的，只在当前线程内生效\n- 而且由于MVCC的支持，在事务开始的时候，就已经开始记录undolog了\n- 有了回滚段，就能保证备份的时候的数据一致性。\n\n#### 问：这两个不同的隔离级别现象有什么不一样的？\n\n- xxx\n\n#### 问：关于我们的业务，“用RC就够了”这个结论是怎么得到的？\n\n- xxx\n\n如果业务开发和运维团队这些问题都没有弄清楚，那么“没问题”这个结论，本身就是有问题的。\n\n\n\n## 加锁/释放锁/查看锁规则\n\n上面只是介绍了锁，但是什么时候加锁（加锁规则），加了什么锁（查看），什么时候释放（释放锁），我们都还不知道。\n\n适用版本：5.x 系列 <=5.7.24，8.0 系列 <=8.0.13\n\n隔离级别（谈到加锁，必先谈隔离级别）：因为间隙锁在RR隔离级别下才有效，所以下面的加锁规则都是在RR隔离级别下\n\n\n\n### 锁是加在哪儿的\n\n锁就是加在索引上的，这是 InnoDB 的一个基础设定\n\nInnoDB基础设定：锁加在索引上。 如果没有索引，锁加在主键上，主键是天然的唯一索引。\n\n\n\n### 怎么加锁的（加锁规则）\n\n\n\n#### 两个原则，两个优化，一个BUG\n\n我总结的加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。\n\n- 原则一：加锁的基本单位是 `next-key lock`\n- 原则二：查找过程中访问到对象才会加锁\n- 优化一：索引上的等值查询，给**唯一索引**加锁的时候，next-key lock 退化为 行锁\n- 优化二：索引上的等值查询，向右遍历且最后一个值不满足等值条件的时候，next-key lock 退化为 间隙锁\n- BUG一：**唯一索引**上的范围查询，会访问到不满足查询条件的第一个值为止。\n\n\n\n#### 案例一：主键索引等值查询\n\n案例数据\n\n```sql\n-- 准备表和数据\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n\n\n只给出现象，试着分析一下现象产生的原因\n\n| session A                                       | session B                                                    | session C                                                    |\n| ----------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| begin;<br/>update t set d = d + 1 where id = 7; |                                                              |                                                              |\n|                                                 | insert into t values (8,8,8);<br/><font color = 'red'>blocked</font> |                                                              |\n|                                                 |                                                              | update t set d = d + 1 where id = 10;<br/><font color = 'gree'>ok</font> |\n\n\n\n关键词：update语句，主键索引，主键记录不存在\n\n- 主键记录这一行存在的时候是行锁（优化一），这一行不存在，那就是间隙锁啦。\n\n\n\n\n\n#### 案例二：普通索引等值查询（覆盖索引的优化）\n\n案例数据\n\n```sql\n-- 准备表和数据\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n\n\n| session A                                                   | session B                                                    | session C                                                    |\n| ----------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| begin;<br/>select id from t where c = 5 lock in share mode; |                                                              |                                                              |\n|                                                             | update t set d = d + 1 where id = 5;<br/><font color = 'gree'>ok</font> |                                                              |\n|                                                             |                                                              | insert into t values (7,7,7);<br/><font color = 'red'>blocked</font> |\n\n\n\n- 关键词：lock in share mode，只查id是覆盖索引，c是普通索引\n- 因为用到了覆盖索引，所以不会锁主键索引；\n- c是普通索引，而且是=5的查询，锁  ( 0 , 5 ] \n- c是普通索引，还要继续向右查询，锁 (5,10] , 退化成  (5,10)，所以7插不进去\n\n\n\n**覆盖索引的优化**\n\n数据行加读锁，如果查询字段使用了覆盖索引，访问到的对象只有普通索引，并没有访问到主键索引，则不会锁主键索引。\n\n如果没有使用覆盖索引，且当前查询是for update ,update 和 delete 都是当前读，则会回表查询，访问到主键索引，这样主键索引也会加锁。\n\n- lock in share mode 只锁覆盖索引\n-  for update 就不一样了。 执行 for update 时，系统会认为你接下来要更新数据，因此会顺便给主键索引上满足条件的行加上行锁。\n\n\n\n#### 案例三：主键索引范围查询\n\n\n\n案例数据\n\n```sql\n-- 准备表和数据\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n\n\n对于主键查询，考虑下面两个语句，是一样的吗？\n\n```sql\n\nmysql> select * from t where id=10 for update;\nmysql> select * from t where id>=10 and id<11 for update;\n```\n\n它们的查询结果是一样的，等价，但是不完全等价。\n\n| session A                                                    | session B                                                    | session C                                                    |\n| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| begin;<br/>select * from t where id>=10 and id<11 for update; |                                                              |                                                              |\n|                                                              | insert into t values (8,8,8);<br/><font color = 'gree'>ok</font><br/>insert into t values (13,13,13);<br/><font color = 'red'>blocked</font> |                                                              |\n|                                                              |                                                              | update t set d = d + 1 where id = 15;<br/><font color = 'red'>blocked</font> |\n\n\n\n- 关键词：主键索引，范围查询， for update\n- 因为是 >= 10，访问到了10，所以是：( 5, 10 ] , 对10来说，是等值查询，退化成 锁10的行锁。\n- 因为是范围查询，继续向右找，找到15，锁 ( 10 , 15 ] 这里不会退化，因为这是范围查询，不是等值查询，只有等值查询才会退化。 （但是在后续的版本中，8.0.19版本锁定区域已经是（10，15））\n\n\n\n\n\n#### 案例五：唯一索引范围查询（BUG）\n\n\n\n案例数据\n\n```sql\n-- 准备表和数据\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n\n\n| session A                                                    | session B                                                    | session C                                                    |\n| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| begin;<br/>select * from t where id > 10 and id <= 15 for update; |                                                              |                                                              |\n|                                                              | update t set d = d + 1 where id = 20;<br/><font color = 'red'>blocked</font> |                                                              |\n|                                                              |                                                              | insert into t values (16,16,16);<br/><font color = 'red'>blocked</font> |\n\n\n\n\n\n- id > 10，就不会访问到 10\n- id <= 15 , 访问到了 15，所以 锁 ( 10 , 15 ] ，因为bug的存在，会继续向右查找一个记录， 锁 ( 15, 20 ] \n- 8.0.25，这个bug已经被修复，但是只修复了主键（主键也是唯一索引），唯一索引还没有修复\n\n\n\n#### 案例六：普通索引等值查询\n\n案例数据\n\n```sql\n-- 准备表和数据\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25),(30,10,30);\n```\n\n\n\n| session A                              | session B                                                    | session C                                                    |\n| -------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| begin;<br/>delete from t where c = 10; |                                                              |                                                              |\n|                                        | insert into t values (12,12,12);<br/><font color = 'red'>blocked</font> |                                                              |\n|                                        |                                                              | update t set d = d + 1 where c = 15;<br/><font color = 'gree'>ok</font> |\n\n\n\n要想搞明白这个，需要先知道索引是有序的，所以上面这些数据，实际上索引的顺序是（普通索引的叶子节点存的是主键值）：\n\n```sql\n字段c：\t0\t5\t10\t10\t15\t20\t25\n主键：\t\t0\t5\t10\t30\t15\t20\t25\n```\n\n\n\n- 关键词：等值查询，普通索引，delete语句\n- delete 语句：加锁语义和 for update 是一样的，和update也是一样的。\n- c = 10 , 且 c是普通索引，遍历到第一个 c = 10，锁 ( 5 5,      10 10 ] \n- 索引上的等值查询，会向右遍历，遍历到第二个 c=10，锁 ( 5 5,      10 30 ] \n- 索引上的等值查询，会向右遍历，遍历到c=15，此时锁： ( 5 5,      15 15 ] ， 不满足查询条件，退化成间隙锁：锁 ( 5 5,      15 15 )\n- 所以最终锁的范围就是：  ( 5 5,      15 15 )\n\n\n\n#### 案例七：limit 语句加锁\n\n案例数据\n\n```sql\n-- 准备表和数据\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25),(30,10,30);\n```\n\n\n\n| session A                                      | session B                                                    |\n| ---------------------------------------------- | ------------------------------------------------------------ |\n| begin;<br/>delete from t where c = 10 limit 2; |                                                              |\n|                                                | insert into t values (12,12,12);<br/><font color = 'gree'>ok</font> |\n\n\n\n- 关键词：等值查询，普通索引，delete语句，有limit\n- delete 语句：加锁语义和 for update 是一样的，和update也是一样的。\n- c = 10 , 且 c是普通索引，遍历到第一个 c = 10，锁 ( 5 5,      10 10 ] \n- 索引上的等值查询，会向右遍历，遍历到第二个 c=10，锁 ( 5 5,      10 30 ] ，此时满足了limit的条件，不在向后遍历\n- 所以最终锁的范围就是：  ( 5 5,      10 30 ] \n\n\n\n#### 案例八：一个死锁的例子（验证next-key lock =间隙锁+行锁）\n\n\n\n| session A                                                    | session B                                                    |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| begin;<br/>select id from t where c = 10 lock in share mode; |                                                              |\n|                                                              | update t set d = d + 1 where c = 10;<br/><font color = 'red'>blocked</font> |\n| insert into t values(8,8,8);                                 |                                                              |\n|                                                              | <font color = 'red'>ERROR 1213(40001):Deadlock found when trying to get lock;try restaring transaction</font> |\n\n\n\n- 关键词：c有普通索引，覆盖索引查询，等值查询，lock in share mode当前读\n- session A 的 select 语句会加锁： ( 5 , 10 ]  和 ( 10 , 15 )\n- session B 的 update 语句也要在索引 c 上加  ( 5 , 10 ] ，进入锁等待；\n- session A 的 insert 语句需要再插入 (8,8,8) 这一行，被 session B 的间隙锁锁住。由于出现了死锁，InnoDB 让 session B 回滚。\n\n\n\n那么问题来了，**session B 的 next-key lock 不是还没申请成功吗？**\n\n其实是这样的，session B 的“加 next-key lock(5,10] ”操作，实际上分成了两步，先是加 (5,10) 的间隙锁，加锁成功（间隙锁与间隙锁之间没有加锁冲突）；然后加 c=10 的行锁，这时候才被锁住的。\n\n也就是说，我们在分析加锁规则的时候可以用 next-key lock 来分析。但是要知道，具体执行的时候，是要分成间隙锁和行锁两段来执行的。\n\n\n\n#### 案例九：范围查询ORDER BY排序加锁\n\n案例数据\n\n```sql\n-- 准备表和数据\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n\n\n| session A                                                    | session B                                                    | session C                                                    |\n| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| begin;<br/>select * from t where c >= 15 and c <= 20 order by c desc for update; |                                                              |                                                              |\n|                                                              | insert into t values (11,11,11);<br/><font color = 'red'>blocked</font> |                                                              |\n|                                                              |                                                              | insert into t values (6,6,6);<br/><font color = 'red'>blocked</font> |\n\n\n\n题外话：\n\n下面俩sql有啥区别？\n\n```sql\nselect * from t where c >= 15 and c <= 20 order by c desc for update;\nselect * from t where c >= 15 and c <= 20 order by c desc lock in share mode;\n```\n\n- 在上面语句中，没有区别；但是在下面的语句中会有区别\n\n```sql\nselect id from t where c >= 15 and c <= 20 order by c desc for update;\nselect id from t where c >= 15 and c <= 20 order by c desc lock in share mode;\n```\n\n- 当使用覆盖索引的时候：\n- for update 会对主键索引也加锁。\n- lock in share mode 不会对主键加锁。\n- 这是 [覆盖索引的优化](#案例二：非唯一索引等值查询（覆盖索引的优化）)。 \n\n回过来，接着看这个思考题\n\n- 关键词：for update，范围查询，用到了普通索引，没有用覆盖索引\n- 由于有 order by c desc，所以索引c的扫描顺序是： 25->20->15->10->5->0\n- 当遍历到 c = 20 时，满足查询条件，锁 ( 15 , 20 ]\n- 然后 c 不是唯一索引，所以还会继续向右扫描，直到遇到 25,又加一个 next-key lock (20,25]，不过 25 不满足条件，退化为间隙锁(20,25)。\n- 继续遍历下一个 c = 15，满足查询条件，锁 ( 10 , 15 ]\n- 继续遍历下一个 c = 10，不满足查询条件，锁 ( 5 , 10 ]\n- 因为 c 不是唯一索引，也不会用到 “两个优化” 原则，所以在索引c上最终锁的范围就是： ( 10 , 25 ]\n- 因为是for update，会对扫描到的行也加锁。锁：c=20、c=15、c=10 这三行加三个行锁。\n- 最终锁的范围就是：（ 5，25），\n\n\n\n\n\n#### 案例十：IN 语句加锁（动态加锁）\n\n案例数据\n\n```sql\n-- 准备表和数据\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n\n\n| session A                                                    |\n| ------------------------------------------------------------ |\n| begin;<br/>select id from t where c in(5,20,10) lock in share mode; |\n\n\n\n- 关键词：in语句，覆盖索引\n- in 语句的查询，mysql默认会对 in  中的数据，进行升序排序。\n- 所以这个语句其实是：`begin;<br/>select id from t where c in(5,10,20) lock in share mode;`\n- 查找c=5的时候，加锁 ( 0 , 5 ]，因为c不是唯一索引，继续向右遍历，锁 ( 5, 10 ]，退化成 ( 5 , 10 )\n- 查找c=10的时候，加锁 ( 5, 10 ]，因为c不是唯一索引，继续向右遍历，锁 ( 10, 15 ]，退化成 ( 10 , 15 )\n- 查找c=20的时候，加锁 ( 15, 20 ]，因为c不是唯一索引，继续向右遍历，锁 ( 20, 25 ]，退化成 ( 20 , 25 )\n- 所以最终范围是：( 0 , 5 ]   ( 5, 10 ]   ( 10 , 15 )    ( 15, 20 ]   ( 20 , 25 )  \n- 简单的说就是：( 0 ，25 ) 中去掉 15 的行锁\n\n**动态加锁**\n\n你可能会说，这个加锁范围，不就是从 (5,25) 中去掉 c=15 的行锁吗？为什么这么麻烦地分段说呢？\n\n因为我要跟你强调这个过程：这些锁是“在执行过程中一个一个加的”，而不是一次性加上去的。\n\n这就是动态加锁。执行到哪里，就加锁加到哪里。\n\n\n\n#### 案例十一：一个死锁的例子（验证动态加锁）\n\n案例数据\n\n```sql\n-- 准备表和数据\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n\n\n| session A                                                    | session B                                                    |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| begin;<br/>select id from t where c in(5,20,10) lock in share mode; |                                                              |\n|                                                              | select id from t where c in(5,20,10) order by c desc for update; |\n\n\n\n现象：当上面两个语句并发执行的时候，会导致死锁，分析为什么？\n\n- 关键词：in 语句，order by 语句，覆盖索引\n- session A上的语句，in 语句的查询，mysql默认会对 in  中的数据，进行升序排序。\n- 所以sessionA上的语句，等价于：`select id from t where c in(5,10,20) lock in share mode;`\n- 而sessionB上的语句，由于指定了排序，in 中的数据顺序会按照执行的排序进行排。\n- 所以sessionB上的语句，等价于：`select id from t where c in(20,10,5) order by c desc for update;`\n- 由于是**动态加锁**：sessionA上的加锁顺序是：( 0 , 5 ]   ( 5, 10 ]   ( 10 , 15 )    ( 15, 20 ]   ( 20 , 25 )  \n- 由于是**动态加锁**：sessionB上的加锁顺序是：( 20 , 25 )   ( 15, 20 ]   ( 10 , 15 )    ( 5, 10 ]    ( 0 , 5 ]\n- 也就是说，这两条语句要加锁相同的资源，但是加锁顺序相反。当这两条语句并发执行的时候，就可能出现**死锁**。\n\n\n\n所以：\n\n- 由于锁是一个个加的（动态加锁），要避免死锁，对同一组资源，要按照尽量相同的顺序访问。\n- 在发生死锁的时刻，innodb会选择回滚成本更小（占用资源最少）的语句进行回滚： for update 语句占有的资源比 lock in share mode 要多\n\n\n\n\n\n### 释放锁\n\nRR隔离级别遵守两阶段锁协议，所有加锁的资源，都是在事务提交或者回滚的时候才释放的。\n\n1、间隙锁只发生于RR隔离级别下\n\n2、RR隔离级别下遵守两阶段提交，事务结束才释放锁 \n\n3、RC隔离级别 没有间隙锁\n\n4、RC隔离级别 语句执行完就释放“不满足条件的行”的行锁，而不是在事务结束的时候才释放\n\n\n\n\n\n### 查看锁\n\n在`information_scheme`库中有两张表\n\n- `innodb_locks`：该表中会记录一些锁信息：\n  - 如果一个事务想要获取某个锁但未获取到，该锁信息将被记录。\n  - 如果一个事务因为获取到了某个锁，但是这个锁阻塞了别的事务的话，该锁信息会被记录。\n  - 正常获取锁释放锁，没有阻塞的情况，在这张表中，不会被记录。\n\n- `innodb_lock_waits`：表明当前系统中因为等待哪些锁而让事务进入阻塞状态。\n\n还可以使用`SHOW ENGINE INNODB STATUS`中有两个部分：\n\n- `TRANSACTIONS`这一部分表示当前系统中，每个事务加了哪些锁。\n- `LATEST DETECTED DEADLOCK`这一部分是表示当系统中出现死锁的时候，记录的最后一次死锁的信息\n\n\n\n\n\n\n\n## 死锁\n\n在上面介绍其他锁的时候，穿插着介绍了很多死锁的场景，这里汇总一下\n\n\n\n### 什么是死锁\n\n- [什么是死锁](#什么是死锁)\n\n### 出现死锁怎么办\n\n- [出现死锁怎么办](#出现死锁怎么办)\n\n#### 死锁等待\n\n- [死锁等待](#死锁等待)\n\n#### 死锁检测\n\n- [死锁检测（推荐）](#死锁检测（推荐）)\n\n### 死锁的出现场景\n\n#### MDL锁的死锁场景\n\n- [MDL锁的死锁场景](#MDL锁的死锁场景)\n  - 如何安全的给小表加索引\n\n#### 行锁的死锁场景\n\n- [行锁的死锁场景](#行锁的死锁场景)\n  - 热点行更新的性能问题\n\n#### 间隙锁的死锁场景\n\n- [间隙锁的死锁场景](#间隙锁的死锁场景)\n  - 任意锁住一行，如果这一行不存在的话就插入，如果存在这一行就更新它的数据\n\n#### next-key lock 的死锁场景\n\n- [案例八：一个死锁的例子（验证next-key lock =间隙锁+行锁）](#案例八：一个死锁的例子（验证next-key lock =间隙锁+行锁）)\n  - 加锁的粒度是next-key lock，但是实际上间隙锁和行锁是分开执行的\n\n### 怎么查看死锁\n\n通过一个简单的死锁，来分析死锁现场\n\n准备数据\n\n```sql\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n按照下面的步骤语句\n\n| session A                                                    | session B                                                    |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| begin;<br/>update t set d = 55 where c = 5;                  |                                                              |\n|                                                              | begin<br/>update t set d = 1010 where c = 10;                |\n| update t set d = 101010 where c = 10;<br/><font color='red'>blocked</font> |                                                              |\n|                                                              | update t set d = 555 where c = 5;<br/><font color='red'>ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction</font> |\n| <font color='gree'>Query OK</font>                           |                                                              |\n\n\n\n执行命令：`show engine innodb status;`,会返回好多好多信息。\n\n其中有一段是：LATEST DETECTED DEADLOCK，表示的就是：记录的最后一次死锁信息（innoDB只会记录最后一次的死锁信息）。\n\n摘录如下：\n\n```sql\n------------------------\nLATEST DETECTED DEADLOCK\n------------------------\n2022-12-15 20:57:44 0x216c\n*** (1) TRANSACTION:\nTRANSACTION 1829, ACTIVE 63 sec starting index read\nmysql tables in use 1, locked 1\nLOCK WAIT 5 lock struct(s), heap size 1128, 4 row lock(s), undo log entries 1\nMySQL thread id 11, OS thread handle 22644, query id 21 localhost ::1 root updating\nupdate t set d = 101010 where c = 10\n\n*** (1) HOLDS THE LOCK(S):\nRECORD LOCKS space id 7 page no 5 n bits 80 index c of table `xx`.`t` trx id 1829 lock_mode X\nRecord lock, heap no 3 PHYSICAL RECORD: n_fields 2; compact format; info bits 0\n 0: len 4; hex 80000005; asc     ;;\n 1: len 4; hex 80000005; asc     ;;\n\n\n*** (1) WAITING FOR THIS LOCK TO BE GRANTED:\nRECORD LOCKS space id 7 page no 5 n bits 80 index c of table `xx`.`t` trx id 1829 lock_mode X waiting\nRecord lock, heap no 4 PHYSICAL RECORD: n_fields 2; compact format; info bits 0\n 0: len 4; hex 8000000a; asc     ;;\n 1: len 4; hex 8000000a; asc     ;;\n\n\n*** (2) TRANSACTION:\nTRANSACTION 1830, ACTIVE 28 sec starting index read\nmysql tables in use 1, locked 1\nLOCK WAIT 5 lock struct(s), heap size 1128, 4 row lock(s), undo log entries 1\nMySQL thread id 12, OS thread handle 14380, query id 22 localhost ::1 root updating\nupdate t set d = 555 where c = 5\n\n*** (2) HOLDS THE LOCK(S):\nRECORD LOCKS space id 7 page no 5 n bits 80 index c of table `xx`.`t` trx id 1830 lock_mode X\nRecord lock, heap no 4 PHYSICAL RECORD: n_fields 2; compact format; info bits 0\n 0: len 4; hex 8000000a; asc     ;;\n 1: len 4; hex 8000000a; asc     ;;\n\n\n*** (2) WAITING FOR THIS LOCK TO BE GRANTED:\nRECORD LOCKS space id 7 page no 5 n bits 80 index c of table `xx`.`t` trx id 1830 lock_mode X waiting\nRecord lock, heap no 3 PHYSICAL RECORD: n_fields 2; compact format; info bits 0\n 0: len 4; hex 80000005; asc     ;;\n 1: len 4; hex 80000005; asc     ;;\n\n*** WE ROLL BACK TRANSACTION (2)\n```\n\n\n\n我们来看看其中的关键信息：\n\n这个结果分成三部分\n\n- (1) TRANSACTION，是第一个事务的信息\n- (2) TRANSACTION，是第二个事务的信息\n- WE ROLL BACK TRANSACTION (2)，是最终的处理结果，表示回滚了第二个事务。\n\n其他可以读取到的信息，这里就不在一一展示了。\n\n\n\n\n\n## 锁等待（间隙锁范围变大）\n\n### 什么是锁等待\n\n通俗的说，就是语句A的执行需要锁，但是锁此时被语句B持有，所以，语句A想要正常执行，就需要等待语句B释放锁；\n\n\n\n### 锁等待的出现场景\n\n#### delete语句导致的锁等待场景\n\n案例数据\n\n```sql\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n\n\n| session A                                                    | session B                                                    |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| begin;<br/>select * from t where id > 10 and id <= 15 for update; |                                                              |\n|                                                              | delete from t where id = 10;<br/><font color='gree'>ok</font> |\n|                                                              | insert into t values (10,10,10);<br/><font color='red'>blocked</font> |\n\n\n\n- session A的语句锁的范围是：（ 10，15 ] （ 15， 20 ）\n- session B 执行delelte，因为10没有被锁，所以可以删掉。\n- 那么执行sessionB执行insert 10的时候，为什么发生了锁等待，导致阻塞呢？\n- 要想知道为什么，我们就要查看锁的信息，分析为什么产生锁等待。\n  - 结论先贴出来：session A 原本锁的范围是：（ 10，15 ] （ 15， 20 ），当 session B 的delete语句执行之后，锁的范围自动变成了：（ 5，15 ] （ 15， 20 ）\n\n\n\n#### update语句导致的锁等待场景\n\n案例数据\n\n```sql\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n\n\n| session A                                                  | session B                                                    |\n| ---------------------------------------------------------- | ------------------------------------------------------------ |\n| begin;<br/>select c from t where c > 5 lock in share mode; |                                                              |\n|                                                            | update t set c = 1 where c = 5;<br/><font color='gree'>ok</font> |\n|                                                            | update t set c = 5 where c = 1;<br/><font color='red'>blocked</font> |\n\n\n\n- session A 的语句是覆盖索引，锁的范围是：索引c上的 (5,10]、(10,15]、(15,20]、(20,25]和 (25,supremum]\n- session B 的一个更新语句，可以分为两步\n  - 插入 c = 1， id=5 的记录： c=1没有被锁，可以执行\n  - 删除 c = 5，id=5 的记录：c=5没有被锁，可以执行\n- 这个语句执行完之后，锁的范围会发生变化：(1,10]、(10,15]、(15,20]、(20,25]和 (25,supremum]\n- session B的第二个更新语句，可以分为两步\n  - 插入  c=5，id=5 的记录：因为c=5已经被锁了，所以阻塞了。\n  - 插入 c=1，id=5 的记录：\n\n\n\n### 怎么查看锁等待\n\n执行命令：`show engine innodb status;`,会返回好多好多信息。\n\n其中有一段是：TRANSACTIONS，里面包含了锁信息，我们可以通过分析这部分，得到为什么产生锁等待。\n\n摘录如下：\n\n```sql\n------------\nTRANSACTIONS\n------------\nTrx id counter 1871\nPurge done for trx's n:o < 1870 undo n:o < 0 state: running but idle\nHistory list length 0\nLIST OF TRANSACTIONS FOR EACH SESSION:\n---TRANSACTION 283993830666896, not started\n0 lock struct(s), heap size 1128, 0 row lock(s)\n---TRANSACTION 283993830664568, not started\n0 lock struct(s), heap size 1128, 0 row lock(s)\n---TRANSACTION 283993830663792, not started\n0 lock struct(s), heap size 1128, 0 row lock(s)\n---TRANSACTION 1870, ACTIVE 9 sec inserting\nmysql tables in use 1, locked 1\nLOCK WAIT 2 lock struct(s), heap size 1128, 1 row lock(s)\nMySQL thread id 12, OS thread handle 14380, query id 79 localhost ::1 root update\ninsert into t values (10,10,10)\n------- TRX HAS BEEN WAITING 9 SEC FOR THIS LOCK TO BE GRANTED:\nRECORD LOCKS space id 8 page no 4 n bits 80 index PRIMARY of table `xx`.`t` trx id 1870 lock_mode X locks gap before rec insert intention waiting\nRecord lock, heap no 5 PHYSICAL RECORD: n_fields 5; compact format; info bits 0\n 0: len 4; hex 8000000f; asc     ;;\n 1: len 6; hex 00000000073a; asc      :;;\n 2: len 7; hex 82000000860137; asc       7;;\n 3: len 4; hex 8000000f; asc     ;;\n 4: len 4; hex 8000000f; asc     ;;\n\n------------------\n---TRANSACTION 1866, ACTIVE 54 sec\n2 lock struct(s), heap size 1128, 1 row lock(s)\nMySQL thread id 11, OS thread handle 22644, query id 74 localhost ::1 root\n```\n\n\n\n在死锁日志里：\n\n- lock_mode X waiting：表示next-key lock； \n\n- lock_mode X locks rec but not gap：是只有行锁； \n\n- locks gap before rec：是只有间隙锁；\n\n通过分析以上日志：\n\n- 我们知道，由于 delete 操作把 id=10 这一行删掉了，原来的两个间隙 (5,10)、(10,15）变成了一个 (5,15)。\n- 也就是说：\n  - session A 原本锁的范围是：（ 10，15 ] （ 15， 20 ）\n  - 当 session B 的delete语句执行之后，锁的范围自动变成了：（ 5，15 ] （ 15， 20 ）\n- 这也就是：为什么 session B 的insert语句无法插入的原因了。\n\n\n\n\n\n## 主键自增锁\n\n\n\n### 什么是主键的自增锁\n\n我们知道主键是连续自增的，主键自增的实现逻辑是：\n\n- 每个表会有一个自增id（可以通过`show create table xx`语句查看，`AUTO_INCREMENT`表示的就是自增id）\n- 每个 insert 语句都会获取自增id作为自己的主键id，然后会把 自增id+1 在写到 `AUTO_INCREMENT`中\n- 接下来真正执行insert语句\n\n那么问题来了，如果是一个并发的 insert 语句，是有可能获取到相同的 自增id的，这样就会导致 insert 语句执行失败（主键冲突）。\n\n所以 mysql 为了避免两个事务申请到相同的自增 id，肯定要加锁，然后顺序申请。\n\n这里所加的锁，就是 主键自增锁。\n\n自增 id 锁并不是一个事务锁，而是**每次申请完就马上释放**，以便允许别的事务再申请\n\n\n\n### 主键自增锁的发展历史\n\nMySQL 5.0 ：自增锁的范围是语句级别。也就是说，如果一个语句申请了一个表自增锁，这个锁会等语句执行结束以后才释放。显然，这样设计会影响并发度。\n\nMySQL 5.1.22 ：引入了一个新策略，新增参数` innodb_autoinc_lock_mode`，默认值是 1。\n\n- 设置为0：表示采用之前 MySQL 5.0 版本的策略，即语句执行结束后才释放锁；\n- 设置为1：\n  - 普通 insert 语句，自增锁在申请之后就马上释放\n  - 类似 insert … select 这样的批量插入数据的语句，自增锁还是要等语句结束后才被释放；\n- 设置为2：所有的申请自增主键的动作都是申请后就释放锁\n- 在公司中，这个参数被设置为：2 ，配合binlog_format=ROW可以解决数据一致性的问题\n\n\n\n### insert语句的锁\n\n\n\n#### insert into ... values( ... )\n\n\n\n#### insert into ... select ...\n\n\n\n生产环境中，尽量不要使用，如果必须使用，一定要在后面的select中，加上where条件。\n\n因为这个语句在执行过程中，会对所有扫描到的记录加锁。\n\n这些被加锁的记录，在这个语句执行完之前，是执行不了的。\n\n\n\n#### insert into ... on duplicate key update ...\n\n插入一条记录，如果主键冲突了，则执行更新。\n\n\n\n\n\n## 残留的问题\n\ninsert语句中有很多的锁，比如：插入意向锁，本文没有细化；后续补充。。\n\n锁的内存结构，一个锁加上的时候，在内存中，是一个怎样的存在。。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["mysql","锁"],"categories":["JAVA","数据库","MYSQL"]},{"title":"redis使用lua脚本删除匹配的key","url":"/note/JAVA/数据库/REDIS/redis使用lua脚本删除匹配的key/","content":"\n# redis使用lua脚本删除匹配的key\n\n---\n\n\n\n## 问题\n\n最近生产环境的redis内存不足了，问题是因为redis的key的过期时间设置的太长了，当初设置了30天过期，但是在实际的运行中，发现redis每天内存占用率以5%的速度增长，当时正值封板期间，不能发布代码解决问题，所以使用了lua脚本来删除过期时间小于10天的数据\n\n\n\n## redis的配置\n\n>  3主3备(2G内存/分片)\n\n\n\n## 脚本\n\n脚本如下\n\n```redis\neval \"local function scanAndDel(cursor,loop_times,count) local loops = 0 repeat local res = redis.call('scan', cursor, 'match', '99*','COUNT', count) if (res ~= nil and #res >= 0) then redis.replicate_commands() cursor = tonumber(res[1]) loops = loops + 1 local ks = res[2] local key_num = #ks for i = 1, key_num, 1 do local k = tostring(ks[i]) local ttl_res = redis.call('ttl',k) local surplus_time = tonumber(ttl_res) if (surplus_time < 1296000) then redis.call('del', k) end end end until (cursor <= 0 or loops >= loop_times) return cursor end return scanAndDel(0,1000,1000)\" 0\n```\n\n对其中的lua脚本格式化之后如下：\n\n```lua\nlocal function scanAndDel(cursor,loop_times,count)\n    local loops = 0\n    repeat local res = redis.call('scan', cursor, 'match', '99*','COUNT', count)\n        if (res ~= nil and #res >= 0)\n        then\n            redis.replicate_commands()\n            cursor = tonumber(res[1])\n            loops = loops + 1\n            local ks = res[2]\n            local key_num = #ks\n            for i = 1, key_num, 1\n            do\n                local k = tostring(ks[i])\n                local ttl_res = redis.call('ttl',k)\n                local surplus_time = tonumber(ttl_res)\n                if (surplus_time < 1296000) then\n                    redis.call('del', k)\n                end\n            end\n        end\n    until (cursor <= 0 or loops >= loop_times)\n    return cursor\nend\nreturn scanAndDel(0,1000,1000)\n```\n\n脚本的逻辑\n\n- 定义一个方法`scanAndDel`三个参数的含义如下\n  - scan命令的游标\n  - 循环次数\n  - 每一次循环scan多少条记录\n\n\n\n\n\n## 缺陷\n\n上面的脚本会循环1000次，一次扫描1000个key，总共就是1千万个key，在实际的生产运行中，大概3S左右会执行完。\n\n```lua\n> eval \"local function scanAndDel(cursor,loop_times,count) local loops = 0 repeat local res = redis.call('scan', cursor, 'match', '99*','COUNT', count) if (res ~= nil and #res >= 0) then redis.replicate_commands() cursor = tonumber(res[1]) loops = loops + 1 local ks = res[2] local key_num = #ks for i = 1, key_num, 1 do local k = tostring(ks[i]) local ttl_res = redis.call('ttl',k) local surplus_time = tonumber(ttl_res) if (surplus_time < 1296000) then redis.call('del', k) end end end until (cursor <= 0 or loops >= loop_times) return cursor end return scanAndDel(0,1000,1000)\" 0\n15471832\n> eval \"local function scanAndDel(cursor,loop_times,count) local loops = 0 repeat local res = redis.call('scan', cursor, 'match', '99*','COUNT', count) if (res ~= nil and #res >= 0) then redis.replicate_commands() cursor = tonumber(res[1]) loops = loops + 1 local ks = res[2] local key_num = #ks for i = 1, key_num, 1 do local k = tostring(ks[i]) local ttl_res = redis.call('ttl',k) local surplus_time = tonumber(ttl_res) if (surplus_time < 1296000) then redis.call('del', k) end end end until (cursor <= 0 or loops >= loop_times) return cursor end return scanAndDel(0,1000,1000)\" 0\n6702980\n```\n\n但是随后而来的就是 **告警**\n\n```java\n\t\n2022-11-14T14:05:19.616+0800|ERROR|consumer-task-AGING_WARN_SCAN_INFO~~AGING_EYE_WARN_SCAN_INFO_CONSUMER-1|com.xxxx.fw.aging.eye.service.AgingCollectServiceImpl||collectLastScanSite error code 9901010001 siteId 11500 exeption io.lettuce.core.RedisCommandTimeoutException: Command timed out after 3 second(s)com.xxxx.framework.cacheproxy.CPException: io.lettuce.core.RedisCommandTimeoutException: Command timed out after 3 second(s)\nat com.xxxx.framework.cacheproxy.redis.RedisCache.call(RedisCache.java:1110)\nat com.xxxx.framework.cacheproxy.redis.RedisCache.setString(RedisCache.java:133)\nat com.xxxx.fw.aging.eye.service.AgingCollectServiceImpl.collectLastScanSite(AgingCollectServiceImpl.java:270)\nat com.xxxx.fw.aging.eye.service.AgingCollectServiceImpl.processScan(AgingCollectServiceImpl.java:263)\nat com.xxxx.fw.aging.eye.service.AgingCollectServiceImpl$$FastClassBySpringCGLIB$$79e7af70.invoke(<generated>)\nat org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)\nat org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:685)\nat com.xxxx.fw.aging.eye.service.AgingCollectServiceImpl$$EnhancerBySpringCGLIB$$c5b443aa.processScan(<generated>)\nat com.xxxx.fw.aging.eye.kafka.AgingScanConsumer.onMessageProcess(AgingScanConsumer.java:24)\nat com.xxxx.fns.kafka.base.AbstractKafkaCompensateListener.onMessage(AbstractKafkaCompensateListener.java:37)\nat com.xxxx.kafka.api.consume.MessageConverListener.onMessage$original$NTmK7AJg(MessageConverListener.java:27)\nat com.xxxx.kafka.api.consume.MessageConverListener.onMessage$original$NTmK7AJg$accessor$rIclUBpU(MessageConverListener.java)\nat com.xxxx.kafka.api.consume.MessageConverListener$auxiliary$eJDIueqp.call(Unknown Source)\nat org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstMethodsInter.intercept(InstMethodsInter.java:86)\nat com.xxxx.kafka.api.consume.MessageConverListener.onMessage(MessageConverListener.java)\nat com.xxxx.kafka.api.client.KafkaConsumer24$ConsumeTask.run(KafkaConsumer24.java:299)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\nat java.lang.Thread.run(Thread.java:748)\nCaused by: io.lettuce.core.RedisCommandTimeoutException: Command timed out after 3 second(s)\nat io.lettuce.core.ExceptionFactory.createTimeoutException(ExceptionFactory.java:51)\nat io.lettuce.core.LettuceFutures.awaitOrCancel(LettuceFutures.java:114)\nat io.lettuce.core.cluster.ClusterFutureSyncInvocationHandler.handleInvocation(ClusterFutureSyncInvocationHandler.java:123)\nat io.lettuce.core.internal.AbstractInvocationHandler.invoke(AbstractInvocationHandler.java:80)\nat com.sun.proxy.$Proxy153.setex(Unknown Source)\nat com.xxxx.framework.cacheproxy.redis.RedisCache.lambda$setString$9(RedisCache.java:137)\nat com.xxxx.framework.cacheproxy.redis.RedisCache.call(RedisCache.java:1108)\n... 20 common frames omitted\n```\n\n告警信息中可以看出，我在通过lua脚本删除key的过程中，应用的redis连接不上了。\n\n究其原因，是因为lua脚本执行的时候，会阻塞其他的命令。\n\n\n\n### 测试\n\n在单实例中， 新建一个 key ， 然后执行一个需要消耗长时间的lua脚本\n\n```lua\neval \"local a=redis.call('hget','test1','name') local b=1 repeat b=b+1 until(b>100000000000000) return a\" 0\n```\n\n然后开启另一个终端， 执行查询操作 `keys *`\n\n```text\nlocal:0>keys *\n\"BUSY Redis is busy running a script. You can only call SCRIPT KILL or SHUTDOWN NOSAVE.\"\n```\n\nredis 提示 有脚本正在运行， 只能使用 `SCRIPT KILL` 或 `SHUTDOWN NOSAVE` 命令终止脚本运行。\n\n### 总结\n\nlua 确实可以增强 redis 的操作，方便在代码中写循环语句操作 redis， 同时要注意在 单实例的 redis 中， 要评估 lua 脚本的开销， 最好是在 redis 集群中使用 redis， 则 lua 不影响 redis 的读写性能","tags":["redis","lua"],"categories":["JAVA","数据库","REDIS"]},{"title":"idea激活","url":"/note/TOOLS/idea/idea激活/","content":"\n# idea激活\n\n\n\n激活使用的是无限试用的方法\n\n下载插件包：[ide-eval-resetter-2.1.6.zip](idea激活.assets/ide-eval-resetter-2.1.6.zip)\n\n直接拖到IDEA或者DataGrip中安装\n\n安装之后，在Help中就可以看到，点击就可以重置\n\n<img src=\"idea激活.assets/image-20221011163201374.png\" alt=\"image-20221011163201374\" style=\"zoom:80%;\" />","tags":["idea破解"],"categories":["TOOLS","idea"]},{"title":"时效项目","url":"/note/PROJECT/时效项目/","content":"\n\n\n# 时效项目\n\n## 背景\n\n再快递的行业中，一个快递单号（称之为运单），在整个生命流程中，可能会产生各种各样的异常：比如丢失，退件，滞留等等各种；在这些异常中，有一种异常叫做时效异常；\n\n**什么是时效异常？**当我们在网上购买商品之后，都希望自己的商品能尽快送到自己手中，但是快递的流转是有自己的规则的，比如必须揽收，必须过分拨，派送，签收等等。作为快递公司，也希望能够尽快把货物送到用户手中，所以快递公司会对站点有时效性的要求：比如12H内揽收，发运在24H内需要到分拨，需要在24H内派送完成等等；如果站点完成不了这些要求，快递公司会对站点罚款。那么当这些单子不满足要求的时候，就会产生异常。我们叫做时效异常；\n\n**需要做什么？**时效项目做得是监听丰网所有的运单，从下单开始监控，监控运单的整个生命周期中的所有扫描信息，会不会产生时效异常。目前我们定义的时效异常有12种；每一个运单在下单之后，到签收的过程中，可能会产生0个或者多个异常；\n\n| **异常类型**      | **判定规则**                    |\n| ----------------- | ------------------------------- |\n| 即将超时待揽收    | 下单后12h没有揽收               |\n| 超时待揽收        | 下单后24h没有揽收               |\n| 即将发运超时      | 揽收后12h没有到分拨             |\n| 发运超时          | 揽收后24h没有到分拨             |\n| 末端网点未到件    | 末端分拨发件后12h没有到末端网点 |\n| 即将派签超时      | 派件后，12h没有签收             |\n| 派签超时(24H-48H) | 派件后，24h没有签收             |\n| 派签超时(48H-72H) | 派件后，48h没有签收             |\n| 派签超时(72H以上) | 派件后，72h没有签收             |\n| 入库24H-48H未取   | 入库后，12h没有取出             |\n| 入库48H-72H未取   | 入库后，48h没有取出             |\n| 入库超72H未取     | 入库后，72h没有取出             |\n\n## 难点\n\n请求量大：目前丰网的订单量在500万，扫描量每天亿级别；平均每分钟的吞吐量在7万，高峰期吞吐量在15万；\n\n数据量大：订单量每天500万左右，需要存储3个月的数据：500万 * 90天 = 45亿的数据\n\n实时性高：要求产生时效异常之后，网点立马跟进处理，所以实时性要求很高；出现异常，立马感知到；\n\n变化频繁：由于政策的变化，时效异常的考核可能不同，比如暴雨，疫情，政策等原因，部分时效异常的判定规则会有变化；\n\n\n\n## 数据量\n\n#### 订单量\n\n- 高峰期每分钟70K左右\n\n![image-20220822170909082](时效项目.assets/image-20220822170909082.png)\n\n### 扫描量\n\n- 高峰期每分钟180K左右\n\n![image-20220822171224234](时效项目.assets/image-20220822171224234.png)\n\n\n\n## 技术栈\n\n1. sfboot（基础springboot 2.2.2.RELEASE）\n\n1. kafka\n\n1. 本地缓存：caffeine\n\n1. 中间件缓存：redis\n\n1. JexlEngine 执行表达式（hutool 工具包）\n\n1. mycat\n\n1. 延迟队列：redission\n\n## 架构\n\n\n\n\n\n\n\n## 配置\n\n开始是4C8G（最大堆6G）；后面升级为6C8G（最大堆6G）\n\n![image-20220906143903876](时效项目.assets/image-20220906143903876.png)\n\n数据库\n\n初始数据库【已废弃】\n\n![image-20220906143740394](时效项目.assets/image-20220906143740394.png)\n\n最新的数据库\n\n![image-20220906143812882](时效项目.assets/image-20220906143812882.png)\n\n\n\n\n\n## 系统设计\n\n\n\n#### 数据库\n\n\n\n\n\n##### 整体UML图\n\n\n\n\n\n##### 表概念说明\n\n描述每张表的作用是什么\n\n| 表名                 | 含义                        | 描述                                                         |\n| -------------------- | --------------------------- | ------------------------------------------------------------ |\n| ag_aging_object      | 引擎模块-时效对象定义表     | 时效对象的定义                                               |\n| ag_aging_status      | 引擎模块-运单状态定义表     | 表示当前时效对象关注的运单状态，不同的时效对象关注的状态可能不一样 |\n| ag_operate_action    | 引擎模块-操作动作定义表     | 消费运单和扫描，映射成引擎关注的动作：下单，网点类型扫描等   |\n| ag_aging_factor      | 引擎模块-时效因子定义表     | 定义因子，一般是基础表中的字段                               |\n| ag_operate_element   | 引擎模块-操作要素定义表     | 加减乘除比较符的定义                                         |\n| ag_status_action     | 引擎模块-操作动作状态表     | 定义哪些动作可能产生哪些状态：比如下单产生待揽收；           |\n| ag_aging_rule        | 引擎模块-运单状态规则表     | 运单状态的判定规则 组合                                      |\n| ag_aging_rule_detail | 引擎模块-运单状态规则详情表 | 每一个规则的详细判定逻辑                                     |\n| ag_aging_warn_rule   | 引擎模块-时效异常规则表     | 异常的判定规则                                               |\n\n\n\n\n\n#### Kafka\n\n\n\n##### kafka流转图\n\n\n\n\n\n\n\n#### 流程\n\n\n\n##### 一个运单的状态判断流程\n\n\n\n##### 一个异常的产生流程\n\n\n\n\n\n\n\n\n\n#### 引擎的配置\n\n\n\n##### 引擎-运单状态的判定规则\n\n\n\n| **运单状态** | **规则判定** | **运单状态的产生时间** | **运单状态的产生站点** |\n| ------------ | ------------ | ---------------------- | ---------------------- |\n| 待揽收       |              | ORDER_TIME             | SEND_SITE              |\n| 已揽收       |              | REC_TIME               | REC_SITE               |\n| 始发分拨到件 |              | FIRST_CENTER_ARR_TIME  | FIRST_CENTER           |\n| 始发分拨发件 |              | FIRST_CENTER_SEND_TIME | FIRST_CENTER           |\n| 末端分拨到件 |              | END_CENTER_ARR_TIME    | END_CENTER             |\n| 末端分拨发件 |              | END_CENTER_SEND_TIME   | END_CENTER             |\n| 末端网点到件 |              | END_SITE_TIME          | END_SITE               |\n| 派件中       |              | DISP_TIME              | DISP_SITE              |\n| 已入库       |              | BOX_IN_TIME            | BOX_IN_SITE            |\n| 已取消       |              | CANCEL_TIME            |                        |\n| 已签收       |              | SIGN_TIME              | SIGN_SITE              |\n| 转寄         |              | TRANSFER_TIME          |                        |\n| 退回         |              | RETURN_TIME            |                        |\n| 换单/作废    |              | CHANGE_INVALID_TIME    |                        |\n\n\n\n\n\n\n\n## 问题\n\n### kafka扫描消费积压\n\n现象：\n\n原因：扫描量太大，应用处理不完\n\n解决办法：多线程（线程不能开太多，会导致cpu升高，同时线程太多，也会有部分线程堵塞，因为查库慢，只能等着），分TOPIC（本质上还是多个线程处理），升级CPU\n\n### 应用CPU升高\n\n现象：\n\n原因：线程数加的太多了\n\n解决办法：\n\n### 数据库CPU升高\n\n现象：有job再跑，十分钟升高一次； 数据量大，读库不稳定，快的时候5ms，慢的时候200ms；\n\n解决办法：\n\n### 数据延迟\n\n初版的时候，因为kafka积压，会导致运单状态变更不及时，即使运单已经签收了， 在kafka中堵了4小时，此时产生了原本不应该产生的异；\n\n原因：kafka堵了。kafka堵的原因是因为数据库层面性能瓶颈；暂未解决\n\n解决办法：将延迟队列中的数据，扔到扫描的kafka后面去；\n\n### 运单基础信息表合并\n\n现象：经过上面的多次优化，对一个扫描的处理，最终只能稳定在20-30ms左右，通过arthas查看，基本是卡在对库的操作上；\n\n解决办法：升级mycat层的cpu（升级后8C16G 16个分片8C16G），但是效果不好；最终新购买了一个mycat集群（16C32G，32个分片4C8G）\n\n\n\n- 将原来的5张表缩减为1张表（非必要字段由下游业务自己填充）\n- 运单信息由上游通过kafka扔给下游，避免下游在多一次查库\n- 更新数据之前，判断所有的字段是否相同，如果相同，就不在操作数据库（减少一次数据库的更新）\n\n\n\n\n\n\n\n数据量大的时候，cpu会升高\n\n![image-20221024153613762](时效项目.assets/image-20221024153613762.png)\n\n这个时候，去看应用的那个线程占用的cpu比较高？\n\n![image-20221024153657174](时效项目.assets/image-20221024153657174.png)\n\n可以看到是消费运单状态的消费者，占用cpu比较多。用的是arthas的thread命令。\n\n优化办法：在当前的配置下，这个部署单元，一秒钟消费的能力是有限的，如果上游给100W的数据，我们1分钟内可以消费100W的数据，但是代价是cpu升到90%，为了保证我们的CPU不超过80%，就不要再1分钟内消费这么多的数据，可以通过限制入口的扫描量，来控制。\n\n可以通过滑动时间窗口的算法来限制。\n\n\n\n\n\n","tags":["项目","时效"],"categories":["PROJECT"]},{"title":"promethues和granafa配置","url":"/note/JAVA/监控/promethues和granafa配置/","content":"\n\n\n## promethues添加新的配置：\n\n修改promethues.yml文件\n\n<img src=\"promethues和granafa配置.assets/image-20220902160951197.png\" alt=\"image-20220902160951197\"  />\n\n\n\n比如我要添加一个kafka的监控，使用kafka-expoter: 进行如下配置：\n\n<img src=\"promethues和granafa配置.assets/image-20220902161001220.png\" alt=\"image-20220902161001220\" style=\"zoom:80%;\" />\n\n\n\n首次启动：\n\n```sh\nnohup ./prometheus --config.file=prometheus.yml &\n```\n\n\n\n热部署（只加载配置文件，不重启promethues的进程）\n\n- 找到promethues的进程ID： lsof -i:9090\n\n- kill -HUP pid\n\n- 观察日志，或者刷新promethus的页面，可以看到新的配置被加载\n\n\n\n<img src=\"promethues和granafa配置.assets/image-20220902161056730.png\" alt=\"image-20220902161056730\" style=\"zoom:80%;\" />\n\n\n\n<img src=\"promethues和granafa配置.assets/image-20220902161108613.png\" alt=\"image-20220902161108613\" style=\"zoom:80%;\" />\n\n\n\n使用kafka-expoter指定端口号： nohup ./kafka_exporter --kafka.server=10.13.70.4:9092 --web.listen-address=:9309 &\n\n\n\n","tags":["promethues","granafa"],"categories":["JAVA","监控"]},{"title":"oracle创建索引的一些规范","url":"/note/JAVA/数据库/ORACLE/oracle创建索引的一些规范/","content":"\n\n\n1、表的主键、外键必须有索引；\n\n2、数据量超过300的表应该有索引；\n\n3、经常与其他表进行连接的表，在连接字段上应该建立索引；\n\n4、经常出现在Where子句中的字段，特别是大表的字段，应该建立索引；\n\n5、索引应该建在选择性高的字段上；\n\n6、索引应该建在小字段上，对于大的文本字段甚至超长字段，不要建索引；\n\n7、复合索引的建立需要进行仔细分析；尽量考虑用单字段索引代替：\n\nA、正确选择复合索引中的主列字段，一般是选择性较好的字段；\n\nB、复合索引的几个字段是否经常同时以AND方式出现在Where子句中？单字段查询是否极少甚至没有？如果是，则可以建立复合索引；否则考虑单字段索引；\n\nC、如果复合索引中包含的字段经常单独出现在Where子句中，则分解为多个单字段索引；\n\nD、如果复合索引所包含的字段超过3个，那么仔细考虑其必要性，考虑减少复合的字段；\n\nE、如果既有单字段索引，又有这几个字段上的复合索引，一般可以删除复合索引；\n\n8、频繁进行数据操作的表，不要建立太多的索引；\n\n9、删除无用的索引，避免对执行计划造成负面影响；\n\n以上是一些普遍的建立索引时的判断依据。一言以蔽之，索引的建立必须慎重，对每个索引的必要性都应该经过仔细分析，要有建立的依据。 因为太多的索引与不充分、不正确的索引对性能都毫无益处：在表上建立的每个索引都会增加存储开销，索引对于插入、删除、更新操作也 会增加处理上的开销。另外，过多的复合索引，在有单字段索引的情况下，一般都是没有存在价值的；相反，还会降低数据增加删除时的性 能，特别是对频繁更新的表来说，负面影响更大","tags":["oracle","索引"],"categories":["JAVA","数据库","ORACLE"]},{"title":"oracle查询表的字段名类型注释","url":"/note/JAVA/数据库/ORACLE/oracle查询表的字段名类型注释/","content":"\n\n\n\n\n\n\n```sql\nselect a.COLUMN_ID, a.TABLE_NAME, a.COLUMN_NAME, a.DATA_TYPE, a.DATA_LENGTH, a.NULLABLE,b.COMMENTS from (\n\nSELECT COLUMN_ID, TABLE_NAME, COLUMN_NAME, DATA_TYPE, DATA_LENGTH, NULLABLE\n  FROM ALL_TAB_COLUMNS\n WHERE TABLE_NAME = '这里改成表名') a left join (\n\n\nselect TABLE_NAME,COLUMN_NAME,COMMENTS\nfrom user_col_comments\nwhere Table_Name='这里改成表名') b on a.COLUMN_NAME = b.COLUMN_NAME order by a.COLUMN_ID asc ;\n\n```\n\n","tags":["oracle","表结构"],"categories":["JAVA","数据库","ORACLE"]},{"title":"单精度与双精度是什么意思，有什么区别？","url":"/note/JAVA/数据库/MYSQL/单精度与双精度是什么意思有什么区别/","content":"\n# 单精度与双精度是什么意思，有什么区别？\n\n---\n\n\n\n- 单精度是这样的格式：1位符号，8位指数，23位小数（总共占32位）\n- 所以在mysql中，float占用4个字节（32位）\n\n\n\n<img src=\"单精度与双精度是什么意思有什么区别.assets/image-20220902113054136.png\" alt=\"image-20220902113054136\" style=\"zoom:80%;\" />\n\n\n\n- 双精度是这样的格式：1位符号，11位指数，52位小数（总共占64位）\n- 所以在mysql中，double占用8个字节（64位）\n\n<img src=\"单精度与双精度是什么意思有什么区别.assets/image-20220902113110508.png\" alt=\"image-20220902113110508\" style=\"zoom:80%;\" />\n\n\n\n- 那请问单精度为什么叫单精度，精度代表的是什么？单和双针对的又是谁？\n\n根据IEEE754的规范，要表达的数字占32位是个基准，称为“单”，32位的整数倍即为“几倍精度”。所以还有“半精度”、“双精度”、“四倍精度”、“任意精度”等。\n\n<img src=\"https://pic1.zhimg.com/v2-08470f89b22d899d31b192cf67c78624.png\" alt=\"preview\" style=\"zoom:80%;\" />\n\n\n\n\n\n","tags":["mysql","单精度","双精度"],"categories":["JAVA","数据库","MYSQL"]},{"title":"mysql如何给大表加索引","url":"/note/JAVA/数据库/MYSQL/mysql给大表加索引/","content":"\n\n\n\n\n# mysql如何给大表加索引\n\n\n\n\n\n给大表加索引、加字段属于DDL（数据定义语言）操作，任何对MySQL大表的DDL操作都值得警惕，不然很可能会引起锁表，报错`Waiting for meta data lock`，造成业务崩溃。那么如何对大表进行加索引操作？\n\n\n\n## 早期DDL原理\n\n\n\n再谈如何对加大表加索引前，先谈一下MySQL DDL操作为什么会锁表？对于这个问题，需要先了解一下MySQL5.6.7之前的早期DDL原理。\n\n早期DDL操作分为`copy table`和`inplace`两种方式。\n\n### copy table 方式\n\n1. 创建与原表相同的**临时表**，并在临时表上执行DDL语句\n2. **锁原表，不允许DML（数据操作语言），允许查询**\n3. 将原表中数据逐行拷贝至临时表（过程没有排序）\n4. 原表升级锁，禁止读写，即原表暂停服务\n5. rename操作，将临时表重命名原表\n\n### inplace 方式\n\nfast index creation，仅支持索引的创建跟删除\n\n1. 创建**frm**（表结构定义文件）临时文件\n2. **锁原表，不允许DML（数据操作语言），允许查询**\n3. 根据聚集索引顺序构建新的索引项，按照顺序插入新的索引页\n4. 原表升级锁，禁止读写，即原表暂停服务\n5. rename操作，替换原表的frm文件\n\n\n\n### copy方式  VS inplace 方式？\n\ninplace 方式相对于 copy 方式来说，inplace 不会生成临时表，不会发生数据拷贝，所以**减少了I/O资源占用**。\n\ninplace 只适用于**索引的创建与删除**，不适用于其他类的DDL语句。\n\n不论是早期copy还是早期inplace方式的DDL，都会进行**锁表操作，不允许DML操作，仅允许查询**。\n\n知道了DDL的机制，下面就了解一下“如何对大表进行加索引操作”！\n\n\n\n## 方案一：“影子策略”\n\n\n\n此方法来自《高性能MySQL》一书中的方案。\n\n### 方案思路\n\n1. 创建一张与原表（tb）结构相同的新表（tb_new）\n2. 在新表上创建索引\n3. 重命名原表为其他表名（tb => tb_tmp），新表重命名为原表名（tb_new => tb），此时新表（tb）承担业务\n4. 为原表（tb_tmp）新增索引\n5. 交换表，新表改回最初的名称（tb => tb_new），原表改回最初的名称（tb_tmp => tb），原表（tb）重新承担业务\n6. 把新表数据导入原表（即把新表承担业务期间产生的数据和到原表中）\n\n### 如何实践\n\nSQL实现：\n\n```sql\n# 以下sql对应上面六步\n\ncreate table tb_new like tb;\n\nalter table tb_new add index idx_col_name (col_name);\n\nrename table tb to tb_tmp, tb_new to tb;\n\nalter table tb_tmp add index idx_col_name (col_name);\n\nrename table tb to tb_new, tb_tmp => tb;\n\ninsert into tb (col_name1, col_name2) select col_name1, col_name2 from tb_new;\n```\n\n### 有哪些问题\n\n步骤3之后，新表改为原表名后（tb）开始承担业务，步骤3到结束之前这段时间的新产生的数据都是存在新表中的，但是如果有业务对老数据进行修改或删除操作，那将无法实现，所以步骤3到结束这段时间可能会产生数据（更新和删除）丢失。\n\n\n\n## 方案二：pt-online-schema-change\n\nPERCONA提供若干维护MySQL的小工具，其中 pt-online-schema-change（简称pt-osc）便可用来相对安全地对大表进行DDL操作。\n\npt-online-schema-change 方案利用三个触发器（DELETE \\ UPDATE \\ INSERT触发器）解决了“影子策略”存在的问题，让新老表数据同步时发生的数据变动也能得到同步。\n\n### 工作原理\n\n1. 创建一张与原表结构相同的新表\n2. 对新表进行DDL操作（如加索引）\n3. 在原表上创建3个触发器（DELETE\\UPDATE\\INSERT），用来原表复制到新表时（步骤4）的数据改动时的同步\n4. 将原表数据以数据块（chunk）的形式复制到新表\n5. 表交换，原表重命名为old表，新表重命名原表名\n6. 删除旧表，删除触发器\n\n### 如何使用\n\n见[使用 pt-online-schema-change 工具不锁表在线修改 MySQL 表结构](https://link.segmentfault.com/?enc=LZyogq6Wt0yUIXj9hpxKBw%3D%3D.mTz7mGjIRe%2BqR0GQwjJJbmtqPnn6lrupLe5wCR%2FIfh53ZSk4VgqnepCKNNljhT7ZB7O05511WulBpY8nkzJzgQ5naGBmq5SEkORgF1Gherdcemq6mzYwFFWwOCZsjHDu3xySOppPU4%2Fa7DEPqjky2ZHYHysFX9DSq%2FMBxf%2B28uE%3D)一文\n\n### 问题疑惑\n\n见[pt-online-schema-change的原理解析与应用说明-问题解答](https://link.segmentfault.com/?enc=vdd59uzwtDTSyc6GGmW28g%3D%3D.GmAUReMDuDxsUXZZAz4TtsBNrADSH0I%2F7ue01BhFOnLOdi5ov0BegypEZQNbysVYFPaVXcTD2ShJmVMAT3L4bA%3D%3D)\n\n\n\n## 方案三：ONLINE DDL\n\nMySQL5.6.7 之前由于DDL实现机制的局限性，常用“影子策略”和 pt-online-schema-change 方案进行DDL操作，保证相对安全性。在 MySQL5.6.7 版本中新推出了 Online DDL 特性，支持“无锁”DDL。5.7版本已趋于成熟，所以在5.7之后可以直接利用 ONLINE DDL特性。\n\n对于 ONLINE DDL 下的 inplace 方式，分为了 `rebuild table` 和 `no-rebuild table`。\n\n\n\n### Online DDL执行阶段\n\n大致可分为三个阶段：初始化、执行和提交\n\n#### Initialization阶段\n\n此阶段会使用MDL读锁，禁止其他并发线程修改表结构\n服务器将考虑存储引擎能力、语句中指定的操作以及用户指定的ALGORITHM 和 LOCK选项，确定操作期间允许的并发数\n\n#### Execution阶段\n\n此阶段分为两个步骤 Prepared and Executed\n此阶段是否需要MDL写锁取决于Initialization阶段评估的因素，如果需要MDL写锁的话，仅在Prepared过程会短暂的使用MDL写锁\n其中最耗时的是Excuted过程\n\n#### Commit Table Definition阶段\n\n此阶段会将MDL读锁升级到MDL写锁，此阶段一般较快，因此独占锁的时间也较短\n用新的表定义替换旧的表定义(如果rebuild table)\n\n### ONLINE DDL 过程\n\n1. 获取对应要操作表的 MDL（metadata lock）写锁\n2. MDL写锁 降级成 MDL读锁\n3. 真正做DDL操作\n4. MDL读锁 升级成 MDL写锁\n5. 释放MDL锁\n\n在第3步时，DDL操作时是不会进行锁表的，可以进行DML操作。但可能在拿DML写锁时锁住，见文章[MySQL · 源码阅读 · 白话Online DDL](https://link.segmentfault.com/?enc=yKSiYtlA%2B%2FDHBQARytIswA%3D%3D.NUd8BeocCVMVlmgOSLJOnJqrdCy9I5PIEfxDJPad8PVd%2FSgo7PKv6xKNLtLSHj4Y)\n\n\n\n\n\n\n\n\n\n\n\n","tags":["索引","mysql"],"categories":["JAVA","数据库","MYSQL"]},{"title":"mysql的数据宽度与数据长度","url":"/note/JAVA/数据库/MYSQL/mysql的数据宽度与数据长度/","content":"\n\n\n# mysql的数据宽度与数据长度\n\n---\n\n\n\n## 前言\n\n在mysql的表定义中\n\n```sql\nCREATE TABLE `ag_aging_warn_clean` (\n\t`id` BIGINT (20) NOT NULL AUTO_INCREMENT,\n\t`status_code` VARCHAR (255) DEFAULT NULL COMMENT '状态code',\n\t`money` float (5,2) DEFAULT NULL,\n\tPRIMARY KEY (`id`),\n\tKEY `ag_aging_warn_clean` (`status_code`)\n) ENGINE = INNODB AUTO_INCREMENT = 66 DEFAULT CHARSET = utf8mb4;\n\n\n```\n\n会有字段长度的定义，比如下面这种\n\n- bigint(20)\n- varchar(255)\n- float(5,2)\n\n括号外面的我们知道是数据类型，括号里面的是什么意思呢？\n\n\n\n## 正文\n\n说到这个，就要说到mysql的数据类型了，跟着我一步一步走，在mysql中数据类型大致分为五类（下面列的比较全）：\n\n\n\n### mysql中数据类型\n\n- 整数类型：BIT、BOOL、TINY INT、SMALL INT、MEDIUM INT、 INT、 BIG INT\n- 浮点数类型：FLOAT、DOUBLE、DECIMAL\n- 字符串类型：CHAR、VARCHAR、TINY TEXT、TEXT、MEDIUM TEXT、LONGTEXT、TINY BLOB、BLOB、MEDIUM BLOB、LONG BLOB\n- 日期类型：Date、DateTime、TimeStamp、Time、Year\n- 其他数据类型：BINARY、VARBINARY、ENUM、SET、Geometry、Point、MultiPoint、LineString、MultiLineString、Polygon、GeometryCollection等\n\n然后针对每一个数据类型，它的长度和范围是不一样的（只列出常用的数据类型，全的可以参考 ： https://m.php.cn/article/460317.html）\n\n### mysql中各数据类型及字节长度\n\n| 数据类型     | 字节长度 | 范围或用法                                                   |\n| ------------ | -------- | ------------------------------------------------------------ |\n| Bit          | 1        | 无符号[0,255]，有符号[-128,127]，**备注**：BIT和BOOL布尔型都占用1字节 |\n| TinyInt      | 1        | 整数[0,255]                                                  |\n| SmallInt     | 2        | 无符号[0,65535]，有符号[-32768,32767]                        |\n| MediumInt    | 3        | 无符号[0,2^24-1]，有符号[-2^23,2^23-1]]                      |\n| Int          | 4        | 无符号[0,2^32-1]，有符号[-2^31,2^31-1]                       |\n| BigInt       | 8        | 无符号[0,2^64-1]，有符号[-2^63 ,2^63 -1]                     |\n| Float(M,D)   | 4        | 单精度浮点数。**备注**：这里的D是精度，如果D<=24则为默认的FLOAT，如果D>24则会自动被转换为DOUBLE型。 |\n| Double(M,D)  | 8        | 双精度浮点。**备注**：关于精度可以参考：[单精度与双精度是什么意思有什么区别.md](单精度与双精度是什么意思有什么区别.md) |\n| Decimal(M,D) | M+1或M+2 | 未打包的浮点数，用法类似于FLOAT和DOUBLE，天缘博客提醒您如果在ASP中使用到Decimal数据类型，直接从数据库读出来的Decimal可能需要先转换成Float或Double类型后再进行运算。 |\n| Date         | 3        | 以YYYY-MM-DD的格式显示，比如：2009-07-19                     |\n| Date Time    | 8        | 以YYYY-MM-DD HH:MM:SS的格式显示，比如：2009-07-19 11：22：30 |\n| TimeStamp    | 4        | 以YYYY-MM-DD的格式显示，比如：2009-07-19                     |\n| Time         | 3        | 以HH:MM:SS的格式显示。比如：11：22：30                       |\n| Year         | 1        | 以YYYY的格式显示。比如：2009                                 |\n| Char(M)      | M        | 定长字符串。                                                 |\n| VarChar(M)   | M        | 变长字符串，要求M<=255                                       |\n| Binary(M)    | M        | 类似Char的二进制存储，特点是插入定长不足补0                  |\n| VarBinary(M) | M        | 类似VarChar的变长二进制存储，特点是定长不补0                 |\n| Tiny Text    | Max:255  | 大小写不敏感                                                 |\n| Text         | Max:64K  | 大小写不敏感                                                 |\n| Medium Text  | Max:16M  | 大小写不敏感                                                 |\n| Long Text    | Max:4G   | 大小写不敏感                                                 |\n| TinyBlob     | Max:255  | 大小写敏感                                                   |\n| Blob         | Max:64K  | 大小写敏感                                                   |\n| MediumBlob   | Max:16M  | 大小写敏感                                                   |\n| LongBlob     | Max:4G   | 大小写敏感                                                   |\n| Enum         | 1或2     | 最大可达65535个不同的枚举值                                  |\n| Set          | 可达8    | 最大可达64个不同的值                                         |\n\n\n\n### 整数型的可选属性\n\n观察第三列【范围或用法】，可以看到，整数型根据数据类型的不同，存储的数值大小也不同，而且还有`无符号`的概念，那么这俩是什么意思呢？\n\n其实在mysql中，整数型的可选属性有三个：\n\n- M   ： 宽度(在0填充的时候才有意义，否则不需要指定)\n- unsigned   ： 无符号类型(非负)\n- zerofill   ：  0填充,(如果某列是zerofill，那么默认就是无符号)，如果指定了zerofill只是表示不够M位时，用0在左边填充，如果超过M位，只要不超过数据存储范围即可\n\n那么这三个有什么用的？我们来一个一个看：\n\n当我们设置了`bigint(5)`的时候，数据库中存储了 -1,-12,-123,-12345,-123456,1,12,123,12345,123456，分别是什么效果？\n\n要分设置了 zerofill（0填充）的情况，和不设置 zerofill（0填充） 的情况\n\n- 不设置zerofill（0填充）的情况，如下图\n\n<img src=\"mysql的数据宽度与数据长度.assets/image-20220902144000573.png\" alt=\"image-20220902144000573\" style=\"zoom:80%;\" />\n\n\n\n- 设置了zerofill（0填充）的情况，如下图\n  - 当使用*zerofill* 时，默认会自动加unsigned（无符号）属性\n\n<img src=\"mysql的数据宽度与数据长度.assets/image-20220902144158868.png\" alt=\"image-20220902144158868\" style=\"zoom:80%;\" />\n\n看完了 zerofill（零填充）的效果之后，我们接着来看 unsigned （无符号）\n\n- 无符号没什么可以说的，以bigint为例\n- 无符号[0,2^64-1]，有符号[-2^63 ,2^63 -1]\n- 设置无符号之后，负数是无法插入的，网上一些傻逼博客说可以插入，只不过展示0，全是放屁\n\n<img src=\"mysql的数据宽度与数据长度.assets/image-20220902153330878.png\" alt=\"image-20220902153330878\" style=\"zoom:80%;\" />\n\n问：bigint(5)和bigint(20)有什么区别？\n\n- 在数据插入方面，没有任何区别；两者占用的空间都是8字节；\n- 只不过展示方面不一样，见上\n\n\n\n### 字符串型的宽度\n\n上面了解了 整数型 的【宽度】，引出了无符号和零填充的概念；那么针对字符串呢？\n\n- 字符串不支持【无符号】和【零填充】\n- 但是字符串支持【宽度】\n\n比如下面这种： varchar(255)，其中的255表示什么含义呢？\n\n- 表示这个列只能存储255个字节，是真正的长度限制。\n\n\n\n\n\n","tags":["单精度","双精度"],"categories":["JAVA","数据库","MYSQL"]},{"title":"mysql的学习网站","url":"/note/JAVA/数据库/MYSQL/mysql的学习网站/","content":"\n\n\n\n\n数据库内核报告： http://mysql.taobao.org/monthly/\n\n<img src=\"mysql的学习网站.assets/image-20220901193533934.png\" alt=\"image-20220901193533934\" style=\"zoom:50%;\" />\n\n\n\n\n\n","tags":["mysql","学习","网站"],"categories":["JAVA","数据库","MYSQL"]},{"title":"mysql排序字段相同导致结果不一致","url":"/note/JAVA/数据库/MYSQL/mysql排序字段相同导致结果不一致/","content":"\n\n\n## mysql排序字段相同导致结果不一致\n\n### 前言\n\n最近公司业务有这么一个功能：对5000条数据进行批量导入，然后通过表格的形式展示在前端，需要根据创建时间进行排序。\n\n所以，很简单的我们会想到\n\n```mysql\nORDER BY gmt_create DESC;\n```\n\n但是在实际的使用中，我发现一个问题，就是当时间相同的时候，这个排序是不确定的，是随机的。\n\n我们看一个例子：\n\n- 首先这两条sql语句是一样的，只不过一个查询 `*` 一个只查询`id`。那么理所当然的：他们的结果应该是一样的。\n\n```mysql\nSELECT * FROM customer WHERE employee_id=39 AND assigned=1 and status=0 ORDER BY gmt_last_transfer DESC ;\n\nSELECT id FROM customer WHERE employee_id=39 AND assigned=1 and status=0 ORDER BY gmt_last_transfer DESC ;\n```\n\n\n\n<img src=\"mysql排序字段相同导致结果不一致.assets/image-20220831203859308.png\" alt=\"image-20220831203859308\" style=\"zoom:50%;\" />\n\n\n\n<img src=\"mysql排序字段相同导致结果不一致.assets/image-20220831203918310.png\" alt=\"image-20220831203918310\" style=\"zoom:50%;\" />\n\n\n\n我们可以看到两次的查询结果是不一样的。\n\n### 正文\n\n### 为什么结果不一样\n\n查阅了Goole和相关资料，大概总结了这种情况的原因。其实发生这种现象是“故意”设计的。\n\n如果没有指定ORDER BY语句，则SQL Server（或任何RDBMS）不保证以特定顺序返回结果。\n\n有些人认为，如果没有指定order by子句，行总是以聚簇索引顺序或物理磁盘顺序返回。\n\n然而，这是不正确的，因为在查询处理期间可以改变行顺序的许多因素，例如并行的HASH连接是更改行顺序的操作符的一个很好的例子。\n\n如果指定ORDER BY语句，SQL Server将对行进行排序，并按请求的顺序返回。\n\n但是，如果该顺序不是确定性的，即可能有重复的值，则在每个具有相同值的组中，由于与上述相同的原因，该顺序是“随机的”。\n\n### 那么怎么保证顺序唯一呢？\n\n- 不使用可能重复的字段进行排序，即不用时间排序，而使用主键进行排序。\n\n- 但是在某些必须使用时间排序，应该怎么办呢？\n\n- 这个时候就需要使用多字段排序的功能。\n\n- 所谓的多字段排序，其实也很简单。\n\nmysql多个字段排序：\n\n```mysql\nselect * from table order by id desc,name desc;\n```\n\n多字字段排序只需要添加多个排序条件，并且每个排序的条件之前用逗号分开。\n\n> order by id desc,name desc; \n\n表示先按照id降序排序，再按照name降序排序。\n\n同理：\n\n> order by id desc,name asc;\n\n 表示先按照id降序排序，再按照name升序排序。\n\n","tags":["mysql"],"categories":["JAVA","数据库","MYSQL"]},{"title":"+号在java中作用","url":"/note/JAVA/数据库/MYSQL/mysql中+号的作用/","content":"\n\n\n## +号在java中作用\n\n- 作为运算符，当作用于数值型的时候，是运算符\n\n- 作为连接符，当作用于字符串的时候，是连接符\n\n## +号在mysql中的作用\n\n### 不能作为连接符\n\n```mysql\nselect first_name+last_name from user order by id desc;\n\n\n期望的结果：\n-------------\n颛孙鹏程\n张三\n李四\n\n\n\n实际的结果\n-------------\n0\n0\n0\n```\n\n\n\n为什么会这样呢？ 看下面就知道了。\n\n### 只能作为运算符\n\n\n\n#### 两个操作数都为数值型\n\n当两个操作数都为数值型的时候，做加法运算。\n\n```mysql\nselect 100+20;\n\n结果\n-----------\n120\n```\n\n\n\n\n\n#### 一个操作数为数值型，另一个是字符型\n\n当一个操作数为数值型，另一个是字符型的时候，会试图将字符型转换成数值型：\n\n- 如果转换成功，然后在做加法运算。\n\n- 如果转换失败，会默认转换成0，然后在做加法运算。\n\n```mysql\nselect 'zhuansun'+20;\n结果\n-----------\n20\n\n\n\nselect '120'+20;\n结果\n-----------\n140\n```\n\n\n\n#### 两个操作数都是字符型\n\n当两个操作数都是字符型的时候，会将字符型默认转换成0，然后在做加法运算。这也就解释了我们之前说的：为什么不能作为连接符。\n\n```mysql\nselect 'zhuansun'+'wenqing';\n\n结果\n-----------\n0\n```\n\n\n\n\n\n#### 一个操作数为null，另一个操作数为字符型或者数值型。\n\n当两个操作数任意一方为null的时候，不管另一个是什么，结果都是NULL。\n\n```mysql\nselect null+20;\n\n结果\n-----------\nNULL\n```\n\n\n\nNULL值与任何值连接，结果都是NULL\n\n```mysql\nselect null+100;\n\n结果：\n--------\nNULL\n\n\n# 假设score可能为null\nselect CONCAT(first_name,',',last_name,',',socre) from user order by id desc;\n\n结果\n----------\n颛孙,鹏程,98\n张,三,90\nnull\n李,四,87\nnull\nnull\n\n```\n\n那么在第二个查询语句结果中出现null的原因是因为score是null，然后与 first_name 和 last_name 进行连接，不管这两个值有没有值，结果都是null。\n\n","tags":["mysql","连接符"],"categories":["JAVA","数据库","MYSQL"]},{"title":"mysql中的交集差集并集","url":"/note/JAVA/数据库/MYSQL/mysql中的交集差集并集/","content":"\n\n\n# mysql中的交集差集并集\n\n\n\n## 前言\n\n首先要知道，mysql并不直接支持。\n\n也就是没有什么 INTERSECT，EXCEPT等等等等（oracle中有）。\n\n那么mysql怎么取交集并集和差集呢？\n\n\n\n## 正文\n\n那么mysql怎么取交集并集和差集呢？\n\n注意： mysql仅仅支持 UNION 和 UNION ALL 这两个操作。\n\n所以，我们只能通过其他的手段对两个结果进行取交集差集和并集。\n\n### 并集\n\n可以直接使用 UNION 和 UNION ALL 这两个操作。\n\nUNION 和 UNION ALL 运算：将查询的返回组合成一个结果\n\n- UNION  合并结果并且将重复的内容取唯一\n- UNION ALL  合并结果并且没有过滤重复内容\n\n\n\n### 交集\n\n使用 UNION ALL 配合 GROUP BY 和 HAVING 进行查询\n\n- 两个集合的结构要一致,对应的字段数,字段类型\n- 将两个集合用 UNION ALL 关键字合并,这里的结果是有重复的所有集\n- 将上面的所有集 GROUP BY id\n- 最后 HAVING COUNT(id)=1,等于1的意思是只出现了一次,所以这个是差集,如果等于2,那么就是交集\n\n使用方法：\n\n```mysql\nSELECT a.* FROM ((结果集1) UNION ALL (结果集2)) a GROUP BY xxx HAVING COUNT(xxx) = 2;\n```\n\n\n\n还有一个方法，就是使用 IN ， 这个方法应该都可以理解。就不过多解释了。\n\n(结果集1) WHERE xxx IN (结果集2) ;\n\n注意：使用 IN 当数据量很大的时候，效率比较差\n\n可以参考： https://blog.csdn.net/mine_song/article/details/70184072\n\n\n\n### 差集\n\n使用 UNION ALL 配合 GROUP BY 和 HAVING 进行查询\n\n- 两个集合的结构要一致,对应的字段数,字段类型\n- 将两个集合用 UNION ALL 关键字合并,这里的结果是有重复的所有集\n- 将上面的所有集 GROUP BY id\n- 最后 HAVING COUNT(id)=1,等于1的意思是只出现了一次,所以这个是差集,如果等于2,那么就是交集\n\n使用方法：\n\n```mysql\nSELECT a.* FROM ((结果集1) UNION ALL (结果集2)) a GROUP BY xxx HAVING COUNT(xxx) = 1;\n```\n\n\n\n还有一个方法，就是使用 NOT IN ，\n\n(结果集1) WHERE xxx NOT IN (结果集2) ;\n\n注意：使用 NOT IN 当数据量很大的时候，效率比较差\n\n这个方法应该都可以理解。就不过多解释了。可以参考： https://blog.csdn.net/mine_song/article/details/70184072","tags":["mysql","交集","并集","差集"],"categories":["JAVA","数据库","MYSQL"]},{"title":"mysql建表的utf8和utf8mb4有什么区别","url":"/note/JAVA/数据库/MYSQL/mysql建表的utf8和utf8mb4有什么区别/","content":"\n\n\n## mysql建表的utf8和utf8mb4有什么区别\n\n### 前言\n\n最近自己搞了一个项目，买了一套腾讯云的数据库，很便宜。但是配置也很低。勉强够用吧。\n\n然后在创建数据库的过程中，观察到了一个现象，mysql内置的编码集包含了utf8和utf8mb4，于是做了一下比较。\n\n### 正文\n\n可以简单的理解 utf8mb4 是目前最大的一个字符编码,支持任意文字.\n\n### 为什么会有UTF8MB4？\n\n既然utf8应付日常使用完全没有问题，那为什么还要使用utf8mb4呢?低版本的MySQL支持的utf8编码，最大字符长度为 3 字节，如果遇到 4字节的字符就会出现错误了。三个字节的 UTF-8 最大能编码的 Unicode 字符是0xFFFF，也就是Unicode中的基本多文平面（BMP）。也就是说，任何不在基本多文平面的 Unicode字符，都无法使用MySQL原有的 utf8字符集存储。这些不在BMP中的字符包括哪些呢？最常见的就是Emoji 表情（Emoji 是一种特殊的 Unicode 编码，常见于 ios 和 android手机上），和一些不常用的汉字，以及任何新增的 Unicode 字符等等。\n\n### UTF-8编码\n\n理论上讲， UTF-8 格式使用一至六个字节，最大能编码 31 位字符。最新的 UTF-8规范只使用一到四个字节，最大能编码21位，正好能够表示所有的 17个 Unicode平面。关于UTF编码，请阅读《常见编码总结》一文。\n\n而utf8 则是 Mysql 早期版本中支持的一种字符集，只支持最长三个字节的UTF-8字符，也就是 Unicode 中的基本多文本平面。这可能是因为在MySQL发布初期，基本多文种平面之外的字符确实很少用到。而在MySQL5.5.3版本后，要在 Mysql 中保存 4 字节长度的 UTF-8 字符，就可以使用utf8mb4字符集了。例如可以用utf8mb4字符编码直接存储emoj表情，而不是存表情的替换字符。为了获取更好的兼容性，应该总是使用 utf8mb4 而非 utf8，事实上，最新版的phpmyadmin默认字符集就是utf8mb4。诚然，对于 CHAR类型数据，使用utf8mb4 存储会多消耗一些空间。\n\n### 那么utf8mb4比utf8多了什么的呢?\n\n多了emoji编码支持.\n\n如果实际用途上来看,可以给要用到emoji的库或者说表,设置utf8mb4.\n\n比如评论要支持emoji可以用到.\n\n建议普通表使用utf8 如果这个表需要支持emoji就使用utf8mb4\n\n- 新建mysql库或者表的时候还有一个排序规则\n\n- utf8_unicode_ci比较准确\n\n- utf8_general_ci速度比较快。\n\n通常情况下 utf8_general_ci的准确性就够我们用的了，在我看过很多程序源码后，发现它们大多数也用的是utf8_general_ci，所以新建数据库时一般选用utf8_general_ci就可以了如果是utf8mb4那么对应的就是 utf8mb4_general_ci，utf8mb4_unicode_ci\n\n","tags":["mysql","utf8","utf8mb4"],"categories":["JAVA","数据库","MYSQL"]},{"title":"mycat从入门到入土","url":"/note/JAVA/数据库/MYCAT/mycat从入门到入土/","content":"\n\n\n\n\n# Mycat常用的分片规则\n\n# MyCat的核心功能\n\n## 分库分表\n\n## Sql路由\n\n## 结果集聚合\n\n我们考虑这样的场景，分片表orders分布到两个数据库节点上，这时候，我们执行以下sql:\n\n```sql\nselect count(1) from orders;\n```\n\n\n\nsql同时发送到两个节点执行，肯定会得到两个count结果，如果没有经过结果集汇聚，我们得到的会是两行count结果，如下所示:\n\n假设两个节点的数据条数分别为100和200。\n\n```sql\n+-----------+\n| count(1) |\n+-----------+\n|  100  |\n+-----------+\n|  200  |\n+-----------+\n```\n\n\n\n经过结果集汇聚后得到的才是正确的，我们想要的结果:\n\n```sql\n+-----------+\n| count(1) |\n+-----------+\n|  300  |\n+-----------+\n```\n\n\n\n对于结果集汇聚，有两种方案\n\n- 第一种是交给应用层去处理，比如count操作，那么只要对得到的结果进行累加即可，但是，如果考虑更复杂的汇聚呢？需要应用层做的太多太多了。\n\n- 第二种方案，也是最优的方案：为了最大化减少应用层的复杂性，我们考虑在mycat这一层实现结果集汇聚处理。\n\n更复杂的结果集汇聚包括sum、avg、group by、order by、limit等\n\n## 读写分离\n\n# MyCat的核心概念\n\n## 逻辑库（schema）\n\n这个概念对应于mysql数据库里面的database ，逻辑库的定义在schema.xml配置文件里面。\n\n## 逻辑表(table)\n\n逻辑表概念对应于mysql数据库里面的table。逻辑表的定义在schema.xml配置文件里面。逻辑表有以下几种分类:\n\n### 全局表\n\n- 全局表：无须对数据进行切分，在所有的分片上都保存一份数据\n\n- 使用场景：数据量小，变动小，使用频繁，这种表适合于 Mycat 全局表\n\n- 查询：Mycat 在 Join 操作中，业务表与全局表进行 Join 聚合会优先选择相同分片内的全局表 join，避免跨库 Join\n\n- 插入：在进行数据插入操作时， mycat 将把数据分发到全局表对应的所有分片执行，在进行数据读取时候将会随机获取一个节点读取数据。\n\n- 缺点：1.4版本前，Mycat 没有做各个分片的全局表的数据一致性检查\n\n### 分片表\n\n- 分片表分布在两个或者更多个分片节点上，数据的分布依赖于分片规则\n\n### 普通表\n\n- 数据只存在一个特定分片节点上，不分片，也不做副本保存(全局表)，实际上对应一个普通的mysql表。\n\n### ER表\n\n- 具有父子关系的分片表，子表的记录与所关联的父表记录存放在同一个分片节点上，即子表依赖于父表，通过表分组(Table Group)保证两个表的join不会是跨库join。表分组是解决跨分片数据join难题的一种折中方案。\n\n## 分片节点(dataNode)\n\n数据切分以后，一个分片表被分到不同的分片数据库上面，每个表分片所在的database(mysql上的概念)就是分片节点——dataNode。dataNode的定义在database.xml配置文件里面。\n\n## 节点主机(dataHost)\n\n节点主机dataHost对应实际的数据库连接，其中可以定义读库readHost和写库writeHost，用于实现读写分离。一个dataHost至少包括一个writeHost，可以包含若干个readHost，也可以没有readHost。dataHost的定义在database.xml配置文件里面。\n\n## 分片规则(rule)\n\n分片规则作用在分片表之上，是实现数据切分、sql路由的关键点。sf-mycat里面内置多种分片规则，包括前面所说的取模(mod)、哈希取模(hash-mod)、范围分区(range)等等。用户可以根据特定业务需求选取合适的分片规则来规划表数据的存储。分片规则的定义在rule.xml配置文件里面，并在schema.xml文件里面被使用。\n\n# MyCat的注意点（限制）\n\n（不支持的SQL，删除的SQL可能导致的问题？？）\n\nMycat本身为Java程序，无法承载大量数据的汇总分析操作，且Mycat以及业界大多数分布式产品都对分布式事务的强一致性都没有很好的解决方案。在涅槃项目中，分布式数据库中间件Mycat的定位是来解决大吞吐量业务系统，且主要为基于分片字段的原子性的应用调用。因此在研发规范上有如下要求：\n\n- 所有查询都带上分片字段条件，避免全分片扫描\n\n- 同一事务中的操作都落在同一个分片上，避免分布式事务\n\nMycat中并没有真正实现分布式事务，而是弱XA事务，不能保证事务commit阶段的完全一致性，需要应用去避免。\n\n扩容：分布式数据库的扩容方案是跟分片方法息息相关的，扩容方案的好坏，关键在于是否需要数据迁移，对于分片方法为 *哈希取模* 的数据，是随机分布在各个节点的，没有规律。扩容之后路由结果会发生改变，没有好的扩容方案，必须做数据迁移，将数据全量导出后再重新导入。因此在设计阶段应该合理预估的节点数量并预留冗余分片。\n\nmycat如何保证表结构一致的： 分表以后，表的数据必须落到多个数据节点上，这时候表定义也就有了多份。虽然我们可以通过sf-mycat这个总入口执行DDL进行表的创建以及表定义的修改，以达到分表在各个节点上的定义达到一致。但是，因为一些人为操作，可能在开发阶段临时修改表结构定义，并且不是通过sf-mycat总入口修改。那么，这时候有可能产生表结构定义不一致。假设在分表上创建了一个用于优化查询的索引，而后在开发阶段某个分片上的表的这个索引被人为修改，导致这个节点上的查询无法走正常的索引优化而影响执行效率，那么这个时候将拖慢整个sql的执行，甚至于引发一些不可控的场景。","tags":["mycat","分库分表","分片","读写分离"],"categories":["JAVA","数据库","MYCAT"]},{"title":"mycat的14中分片规则","url":"/note/JAVA/数据库/MYCAT/mycat的14中分片规则/","content":"\n\n\n# mycat的14中分片规则\n\n# 参考文章\n\nhttps://zhuanlan.zhihu.com/p/359262439\n\n# 环境准备\n\n- mycat 有 三个实例（三个分片）\n\n- 创建一个数据库：partition_db\n\n```sql\ncreate database partition_db DEFAULT CHARACTER SET utf8mb4;\n```\n\n\n\n# 分片规则\n\n## 1、取模分片\n\n```xml\n<tableRule name=\"mod-long\"> \n    <rule>\n        <columns>id</columns> \n        <algorithm>mod-long</algorithm> \n    </rule> \n</tableRule> \n\n<function name=\"mod-long\" class=\"io.mycat.route.function.PartitionByMod\"> \n    <property name=\"count\">3</property> \n</function>\n```\n\n\n\n配置说明：\n\n| 属性      | 描述                             |\n| --------- | -------------------------------- |\n| columns   | 标识将要分片的表字段             |\n| algorithm | 指定分片函数与function的对应关系 |\n| class     | 指定该分片算法对应的类           |\n| count     | 数据节点的数量                   |\n\n效果：\n\n用主键ID对3取模，数据会均匀的分散在3个分片上；\n\n问题：如果count配置了2，是什么情况？\n\n## 2、范围分片\n\n根据指定的字段及其配置的范围与数据节点的对应情况， 来决定该数据属于哪一个分片 ， 配置如下：\n\n```xml\n<tableRule name=\"auto-sharding-long\"> \n    <rule>\n        <columns>id</columns> \n        <algorithm>rang-long</algorithm> \n    </rule> \n</tableRule> \n\n<function name=\"rang-long\" class=\"io.mycat.route.function.AutoPartitionByLong\"> \n    <property name=\"mapFile\">autopartition-long.txt</property> \n    <property name=\"defaultNode\">0</property> \n</function>\n```\n\n\n\nautopartition-long.txt 配置如下：\n\n```xml\n# range start-end ,data node index \n# K=1000,M=10000. \n0-500M=0 \n500M-1000M=1 \n1000M-1500M=2\n```\n\n\n\n含义为 ： \n\n- 0 - 500 万之间的值 ， 存储在0号数据节点 ； \n\n- 500万 - 1000万之间的数据存储在1号数据节点 ；\n\n- 1000万 - 1500 万的数据节点存储在2号节点 ；\n\n配置说明:\n\n| 属性        | 描述                                                         |\n| ----------- | ------------------------------------------------------------ |\n| columns     | 标识将要分片的表字段                                         |\n| algorithm   | 指定分片函数与function的对应关系                             |\n| class       | 指定该分片算法对应的类                                       |\n| mapFile     | 对应的外部配置文件                                           |\n| type        | 默认值为0 ; 0 表示Integer , 1 表示String                     |\n| defaultNode | 默认节点 默认节点的所用:枚举分片时,如果碰到不识别的枚举值, 就让它路由到默认节点 ; 如果没有默认值,碰到不识别的则报错 。 |\n\n## 3、枚举分片\n\n通过在配置文件中配置可能的枚举值, 指定数据分布到不同数据节点上, 本规则适用于按照**省份或状态拆分数据**等业务。\n\n配置如下:\n\n```xml\n<tableRule name=\"sharding-by-intfile\"> \n    <rule>\n        <columns>status</columns> \n        <algorithm>hash-int</algorithm> \n    </rule> \n</tableRule> \n\n<function name=\"hash-int\" class=\"io.mycat.route.function.PartitionByFileMap\"> \n    <property name=\"mapFile\">partition-hash-int.txt</property> \n    <property name=\"type\">1</property> \n    <property name=\"defaultNode\">0</property> \n</function>\n```\n\n\n\npartition-hash-int.txt，内容如下 :\n\n```xml\n待处理=0 \n处理中=1 \n已完成=2\n```\n\n\n\n含义为：\n\n- status=待处理的落在0号分片\n\n- status=处理中的落在1号分片\n\n- status=已完成的落在2号分片\n\n配置说明:\n\n| 属性        | 描述                                                         |\n| ----------- | ------------------------------------------------------------ |\n| columns     | 标识将要分片的表字段                                         |\n| algorithm   | 指定分片函数与function的对应关系                             |\n| class       | 指定该分片算法对应的类                                       |\n| mapFile     | 对应的外部配置文件                                           |\n| type        | 默认值为0 ; 0 表示Integer , 1 表示String                     |\n| defaultNode | 默认节点 ; 小于0 标识不设置默认节点 , 大于等于0代表设置默认节点 ; 默认节点的所用:枚举分片时,如果碰到不识别的枚举值, 就让它路由到默认节点 ; 如果没有默认值,碰到不识别的则报错 。 |\n\n## 4、范围取模\n\n该算法为先进行范围分片, 计算出分片组 , 再进行组内求模。\n\n配置如下\n\n```xml\n<tableRule name=\"auto-sharding-rang-mod\"> \n    <rule>\n        <columns>id</columns> \n        <algorithm>rang-mod</algorithm> \n    </rule> \n</tableRule> \n\n<function name=\"rang-mod\" class=\"io.mycat.route.function.PartitionByRangeMod\"> \n    <property name=\"mapFile\">autopartition-range-mod.txt</property> \n    <property name=\"defaultNode\">0</property> \n</function>\n```\n\n\n\nautopartition-range-mod.txt配置格式\n\n```xml\n#range start-end , data node group size \n0-500M=1 \n500M-2000M=2\n```\n\n\n\n含义为：\n\n- 0 - 500 万之间的值 ， 存储在一个分片节点上；\n\n- 500万到2000万之间的值，存储在两个分片节点上；\n\n- 然后拿到分片节点数量之后，在对分片节点数量进行取模，得到具体的存储分片； \n\n配置说明:\n\n| 属性        | 描述                                                         |\n| ----------- | ------------------------------------------------------------ |\n| columns     | 标识将要分片的表字段名                                       |\n| algorithm   | 指定分片函数与function的对应关系                             |\n| class       | 指定该分片算法对应的类                                       |\n| mapFile     | 对应的外部配置文件                                           |\n| defaultNode | 默认节点 ; 未包含以上规则的数据存储在defaultNode节点中, 节点从0开始 |\n\n**优点**： 综合了范围分片和求模分片的优点。 分片组内使用求模可以保证组内的数据分布比较均匀，分片组之间采用范围分片可以兼顾范围分片的特点。\n\n**缺点**： 在数据范围时固定值（非递增值）时，存在不方便扩展的情况，例如将 dataNode Group size 从 2 扩展为 4 时，需要进行数据迁移才能完成 ；\n\n## 5、固定分片hash\n\n该算法类似于十进制的求模运算，但是为二进制的操作，例如，取 id 的二进制低 10 位 与 1111111111 进行**位 & 运算**。\n\n```xml\n最小值的情况：\n    0 0 0 0 0 0 0 0 0 0 \n&   1 1 1 1 1 1 1 1 1 1 \n---------------------------------\n    0 0 0 0 0 0 0 0 0 0       =   0\n\n最大值的情况\n    1 1 1 1 1 1 1 1 1 1 \n&   1 1 1 1 1 1 1 1 1 1 \n----------------------------------\n    1 1 1 1 1 1 1 1 1 1       =   2^10-1 = 1023\n```\n\n\n\n配置如下 ：\n\n```xml\n<tableRule name=\"sharding-by-long-hash\"> \n    <rule>\n        <columns>id</columns> \n        <algorithm>func1</algorithm> \n    </rule> \n</tableRule> \n\n<function name=\"func1\" class=\"org.opencloudb.route.function.PartitionByLong\"> \n    <property name=\"partitionCount\">2,1</property> \n    <property name=\"partitionLength\">256,512</property> \n</function>\n```\n\n\n\n含义是：\n\n- 在示例中配置的分片策略，希望将数据水平分成3份，前两份各占 25%，第三份占 50%。\n\n- 示例数据：\n\n| id开始   | id结束    | 数据节点 |\n| -------- | --------- | -------- |\n| 1        | 255       | dn1      |\n| 256      | 511       | dn1      |\n| 512      | 1023      | dn2      |\n| 1024     | 1024+255  | dn1      |\n| 1024+256 | 1024+511  | dn2      |\n| 1024+512 | 1024+1023 | dn3      |\n\n配置说明:\n\n| 属性            | 描述                             |\n| --------------- | -------------------------------- |\n| columns         | 标识将要分片的表字段名           |\n| algorithm       | 指定分片函数与function的对应关系 |\n| class           | 指定该分片算法对应的类           |\n| partitionCount  | 分片个数列表，是一个数组         |\n| partitionLength | 分片范围列表，是一个数组         |\n\n约束 :\n\n- 分片长度 : 默认最大2^10 , 为 1024 ;\n\n- partitionCount, partitionLength的数组长度必须是一致的 ;\n\n- 1024 = sum((count[i]*length[i]))\n\n优点： 这种策略比较灵活，可以均匀分配也可以非均匀分配，各节点的分配比例和容量大小由 partitionCount 和 partitionLength 两个参数决定\n\n缺点：和取模分片类似。\n\n看不懂，看这个：https://blog.csdn.net/zhou920786312/article/details/122427650\n\n\n\n## 6、取模范围\n\n该算法先进行取模，然后根据取模值所属范围进行分片。\n\n**注意 : 取模范围算法只能针对于数字类型进行取模运算 ; 如果是字符串则无法进行取模分片 ;**\n\n配置如下:\n\n```xml\n<tableRule name=\"sharding-by-pattern\"> \n    <rule>\n        <columns>id</columns> \n        <algorithm>sharding-by-pattern</algorithm> \n    </rule> \n</tableRule> \n\n<function name=\"sharding-by-pattern\" class=\"io.mycat.route.function.PartitionByPattern\"> \n    <property name=\"mapFile\">partition-pattern.txt</property> \n    <property name=\"defaultNode\">0</property> \n    <property name=\"patternValue\">96</property> \n</function>\n```\n\n\n\npartition-pattern.txt配置如下:\n\n```xml\n0-32=0 \n33-64=1 \n65-96=2\n```\n\n\n\n含义是：\n\n- 在mapFile配置文件中\n\n- **1-32即代表id%96后的分布情况**。\n\n- 如果在1-32, 则在分片0上 ; \n\n- 如果在33-64, 则在分片1上 ; \n\n- 如果在65-96, 则在分片2上。\n\n配置说明:\n\n| 属性         | 描述                                                       |\n| ------------ | ---------------------------------------------------------- |\n| columns      | 标识将要分片的表字段                                       |\n| algorithm    | 指定分片函数与function的对应关系                           |\n| class        | 指定该分片算法对应的类                                     |\n| mapFile      | 对应的外部配置文件                                         |\n| defaultNode  | 默认节点 ; 如果id不是数字, 无法求模, 将分配在defaultNode上 |\n| patternValue | 求模基数                                                   |\n\n**优点**：可以自主决定取模后数据的节点分布\n\n**缺点**：dataNode 划分节点是事先建好的，需要扩展时比较麻烦。\n\n\n\n\n\n## 7、字符串hash求模范围\n\n与取模范围算法类似, 该算法支持**数值**、**符号**、**字母取模**，首先截取长度为 prefixLength 的子串，在对子串中每一个字符的 **ASCII 码**求和，然后对求和值进行**取模运算**（sum%patternValue），就可以计算出子串的分片数。\n\n配置如下：\n\n```xml\n<tableRule name=\"sharding-by-prefixpattern\"> \n    <rule>\n        <columns>id</columns> \n        <algorithm>sharding-by-prefixpattern</algorithm> \n    </rule> \n</tableRule> \n\n<function name=\"sharding-by-prefixpattern\" class=\"io.mycat.route.function.PartitionByPrefixPattern\"> \n    <property name=\"mapFile\">partition-prefixpattern.txt</property> \n    <property name=\"prefixLength\">5</property> \n    <property name=\"patternValue\">96</property> \n</function>\n```\n\n\n\npartition-prefixpattern.txt配置如下:\n\n```xml\n# range start-end ,data node index \n# ASCII \n# 48-57=0-9 \n# 64、65-90=@、A-Z \n# 97-122=a-z \n###### first host configuration \n0-32=0 \n33-64=1 \n65-96=2\n```\n\n\n\n含义是：\n\n- 对id先截取prefixLength 长度的数据\n\n- 然后对截取后的数据的 每一位 获取ASCII 码\n\n- 然后把所有的ASCII码求和\n\n- 求和之后对patternValue 取模\n\n- 然后对取模的结果在 partition-prefixpattern.txt 中进行范围的匹配\n\n比如：\n\n```xml\n字符串 : \n    gf89f9a \n\n截取字符串的前5位进行ASCII的累加运算 : \n    g - 103 \n    f - 102 \n    8 - 56 \n    9 - 57 \n    f - 102 \n    \n    sum求和 : 103 + 102 + + 56 + 57 + 102 = 420 \n    求模 : 420 % 96 = 36\n```\n\n\n\n配置说明:\n\n| 属性         | 描述                                                         |\n| ------------ | ------------------------------------------------------------ |\n| columns      | 标识将要分片的表字段                                         |\n| algorithm    | 指定分片函数与function的对应关系                             |\n| class        | 指定该分片算法对应的类                                       |\n| mapFile      | 对应的外部配置文件                                           |\n| prefixLength | 截取的位数; 将该字段获取前prefixLength位所有ASCII码的和, 进行求模sum%patternValue ,获取的值，在通配范围内的即分片数 ; |\n| patternValue | 求模基数                                                     |\n\n**优点**：可以自主决定取模后数据的节点分布\n\n**缺点**：｀dataNode｀ 划分节点是事先建好的，需要扩展时比较麻烦。\n\n## 8、应用代码指定\n\n由运行阶段由应用自主决定路由到那个分片 , 直接根据字符子串（必须是数字）计算分片号 , 配置如下 :\n\n```xml\n<tableRule name=\"sharding-by-substring\"> \n    <rule>\n        <columns>id</columns> \n        <algorithm>sharding-by-substring</algorithm> \n    </rule> \n</tableRule> \n\n<function name=\"sharding-by-substring\" class=\"io.mycat.route.function.PartitionDirectBySubString\"> \n    <property name=\"startIndex\">0</property> <!-- zero-based --> \n    <property name=\"size\">2</property> \n    <property name=\"partitionCount\">3</property> \n    <property name=\"defaultPartition\">0</property> \n</function>\n```\n\n\n\n含义是：\n\n- 对id从startIndex 开始截取，截取 size 个长度，比如id=756432，截取之后就是75,\n\n- 然后75不在partitionCount 中，75 >3了，所以放在默认分片 defaultPartition 0中\n\n配置说明\n\n| 属性             | 描述                                                         |\n| ---------------- | ------------------------------------------------------------ |\n| columns          | 标识将要分片的表字段                                         |\n| algorithm        | 指定分片函数与function的对应关系                             |\n| class            | 指定该分片算法对应的类                                       |\n| startIndex       | 字符子串起始索引                                             |\n| size             | 字符长度                                                     |\n| patternValue     | 分区（分片）数量                                             |\n| defaultPartition | 默认分片，当截取之后的获得分片号不在分片数量内时，使用默认分片 |\n\n## ??? 9、字符串hash解析\n\n取字符串中的指定位置的子字符串, 进行hash算法， 算出分片 ， \n\n配置如下\n\n```xml\n<tableRule name=\"sharding-by-stringhash\"> \n    <rule>\n        <columns>user_id</columns> \n        <algorithm>sharding-by-stringhash</algorithm> \n    </rule> \n</tableRule> \n\n<function name=\"sharding-by-stringhash\" class=\"io.mycat.route.function.PartitionByString\"> \n    <property name=\"partitionLength\">512</property> <!-- zero-based --> \n    <property name=\"partitionCount\">2</property> \n    <property name=\"hashSlice\">0:2</property> \n</function>\n```\n\n\n\n含义是：\n\n- 对一个字符串，截取（0,2）的长度\n- 对截取后的字符串，进行hash\n- hash后的结果，按照partitionLength和partitionCount进行分片\n\n配置说明:\n\n| 属性            | 描述                                                         |\n| --------------- | ------------------------------------------------------------ |\n| columns         | 标识将要分片的表字段                                         |\n| algorithm       | 指定分片函数与function的对应关系                             |\n| class           | 指定该分片算法对应的类                                       |\n| partitionLength | hash取模基数； length * count = 1024 （非出性能考虑）        |\n| partitionCount  | 分区数                                                       |\n| hashSlice       | hash运算符，根据字符串的hash运算；<br/> 0 代表str.length()<br/>-1 代表str.length() - 1<br/>大于0 代表数字自身<br/>可以理解为：substring(start,end) |\n\n## 10、一致性hash\n\n\n\n\n\n\n\n## 11、日期分片\n\n按照日期来分片\n\n```xml\n<tableRule name=\"sharding-by-date\"> \n    <rule>\n        <columns>create_time</columns> \n        <algorithm>sharding-by-date</algorithm> \n    </rule> \n</tableRule> \n\n<function name=\"sharding-by-date\" class=\"io.mycat.route.function.PartitionByDate\"> \n    <property name=\"dateFormat\">yyyy-MM-dd</property> \n    <property name=\"sBeginDate\">2020-01-01</property> \n    <property name=\"sEndDate\">2020-12-31</property> \n    <property name=\"sPartionDay\">10</property> \n</function>\n```\n\n\n\n配置说明:\n\n| 属性        | 描述                                                         |\n| ----------- | ------------------------------------------------------------ |\n| columns     | 标识将要分片的表字段                                         |\n| algorithm   | 指定分片函数与function的对应关系                             |\n| class       | 指定该分片算法对应的类                                       |\n| dateFormat  | 日期格式                                                     |\n| sBeginDate  | 开始日期                                                     |\n| sEndDate    | 结束日期，如果配置了结束日期，则表示数据到达了这个日期所在的分片之后，会重头开始再次插入 |\n| sPartionDay | 分区天数，默认是10，表示10天的数据存在一个分区               |\n\n注意：配置规则的表的dataNode的分片，必须和分片规则数量一致\n\n例如 2020-01-01 到 2020-12-31 ，每10天一个分片，一共需要37个分片。\n\n## 12、单月小时\n\n单月内按照小时拆分, 最小粒度是小时 , 一天最多可以有24个分片, 最小1个分片, 下个月从头开始循环, 每个月末需要手动清理数据。\n\n配置如下 ：\n\n```xml\n<tableRule name=\"sharding-by-hour\"> \n    <rule>\n        <columns>create_time</columns> \n        <algorithm>sharding-by-hour</algorithm> \n    </rule> \n</tableRule> \n\n<function name=\"sharding-by-hour\" class=\"io.mycat.route.function.LatestMonthPartion\"> \n    <property name=\"splitOneDay\">24</property> \n</function>\n```\n\n\n\n配置说明:\n\n| 属性        | 描述                             |\n| ----------- | -------------------------------- |\n| columns     | 标识将要分片的表字段             |\n| algorithm   | 指定分片函数与function的对应关系 |\n| class       | 指定该分片算法对应的类           |\n| splitOneDay | 一天切分的分片数                 |\n\n## 13、自然月分片\n\n使用场景为按照月份列分区, 每个自然月为一个分片。\n\n 配置如下:\n\n```xml\n<tableRule name=\"sharding-by-month\"> \n    <rule>\n        <columns>create_time</columns> \n        <algorithm>sharding-by-month</algorithm> \n    </rule> \n</tableRule> \n\n<function name=\"sharding-by-month\" class=\"io.mycat.route.function.PartitionByMonth\"> \n    <property name=\"dateFormat\">yyyy-MM-dd</property> \n    <property name=\"sBeginDate\">2020-01-01</property> \n    <property name=\"sEndDate\">2020-12-31</property> \n</function>\n```\n\n\n\n配置说明:\n\n| 属性       | 描述                                                         |\n| ---------- | ------------------------------------------------------------ |\n| columns    | 标识将要分片的表字段                                         |\n| algorithm  | 指定分片函数与function的对应关系                             |\n| class      | 指定该分片算法对应的类                                       |\n| dateFormat | 日期格式                                                     |\n| sBeginDate | 开始日期                                                     |\n| sEndDate   | 结束日期，如果配置了结束日期，当到达这个月份之后，会重复从开始分片插入 |\n\n## 14、日期范围hash\n\n其思想和范围取模分片一样，先根据日期进行范围分片求出分片组，再根据时间hash使得短期内数据分布的更均匀 ;\n\n配置如下：\n\n```xml\n<tableRule name=\"range-date-hash\"> \n    <rule>\n        <columns>create_time</columns> \n        <algorithm>range-date-hash</algorithm> \n    </rule> \n</tableRule> \n\n<function name=\"range-date-hash\" class=\"io.mycat.route.function.PartitionByRangeDateHash\"> \n    <property name=\"dateFormat\">yyyy-MM-dd HH:mm:ss</property> \n    <property name=\"sBeginDate\">2020-01-01 00:00:00</property> \n    <property name=\"groupPartionSize\">6</property> \n    <property name=\"sPartionDay\">10</property> \n</function>\n```\n\n\n\n配置说明:\n\n| 属性             | 描述                                   |\n| ---------------- | -------------------------------------- |\n| columns          | 标识将要分片的表字段                   |\n| algorithm        | 指定分片函数与function的对应关系       |\n| class            | 指定该分片算法对应的类                 |\n| dateFormat       | 日期格式，符合java标准                 |\n| sBeginDate       | 开始日期，与 dateFormat 指定的格式一致 |\n| groupPartionSize | 每组的分片数量                         |\n| sPartionDay      | 代表多少天为一组                       |\n\n优点 : 可以避免扩容时的数据迁移，又可以一定程度上避免范围分片的热点问题\n\n注意 : 要求日期格式尽量精确些，不然达不到局部均匀的目的","tags":["mycat","分片"],"categories":["JAVA","数据库","MYCAT"]},{"title":"doris分区的基本操作","url":"/note/JAVA/数据库/DORIS/doris分区的基本操作/","content":"\n\n\n```sql\n\n查看某一个表的分区\nshow partitions from fw_sign_sum_di;\n\n删除分区\nalter table fw_sign_sum_di drop partition p20211230;\n\n添加分区：分区要按照顺序加\nalter table fw_sign_sum_di ADD PARTITION p20211230 VALUES LESS THAN (\"2021-12-31\");\n\n添加分区，可以用下面的代码生成脚本\n\n#         SimpleDateFormat sdf = new SimpleDateFormat(\"yyyyMMdd\");\n#         SimpleDateFormat sdf2 = new SimpleDateFormat(\"yyyy-MM-dd\");\n#         Date currentDate = new Date(1643385599000L);\n#         for (int i = 1; i < 500; i++) {\n#             currentDate =  DateUtil.getAfterDays(currentDate,1);\n#             StringBuilder sb = new StringBuilder(\"alter table fw_billstat_sum_di ADD PARTITION p\")\n#                 .append(sdf.format(currentDate)).append(\" \")\n#                 .append(\"VALUES LESS THAN ('\")\n#                 .append(sdf2.format(DateUtil.getAfterDays(currentDate,1)))\n#                 .append(\"');\");\n#             System.out.println(sb.toString());\n#         }\n\n\n```\n\n","tags":["doris"],"categories":["JAVA","数据库","DORIS"]},{"title":"mongo的基本查询语法","url":"/note/JAVA/数据库/MONGO/mongo的基本查询语法/","content":"\n\n\n\n\n```sql\ndb.in_board.find({}).count(true)\ndb.in_board.find({\"fwDisp\":\"true\",\"dispTime\":{$gte:\"1638806400000\",$lt:\"1638892799000\"}}).count(true)\n\n\ndb.in_board.find({\"fwDisp\":\"true\",\"signTime\":{$gte:\"1638806400000\",$lt:\"1638892799000\"}}).count(true)\ndb.in_board.find({\"fwDisp\":\"true\",\"dispTime\":{$gte:\"1638806400000\",$lt:\"1638892799000\"},\"signTime\":{$ne:null},\"signTime\":{$lte:\"this.latestSignTime\"}}).count(true)\ndb.in_board.find({\"fwDisp\":\"true\",\"dispTime\":{$gte:\"1638720000000\",$lt:\"1638806399000\"},\"signTime\":{$ne:null},\"signTime\":{$lte:\"this.latestSignTime\"}}).sort({\"_id\": 1})\n\ndb.in_board.find({\"fwDisp\":\"true\",\"dispTime\":{$gte:\"1638892800000\",$lt:\"1638979199000\"},\"dispSettleSiteId\":\"99618\"}).count(true)\ndb.in_board.find({\"fwDisp\":\"true\",\"dispTime\":{$gte:\"1638892800000\",$lt:\"1638979199000\"},\"dispSettleSiteId\":\"99618\",\"signTime\":{$ne:null},\"signTime\":{$lte:\"this.latestSignTime\"}}).count(true)\n\n\n```\n\n\n\n\n\n","tags":["mongo"],"categories":["JAVA","数据库","MONGO"]},{"title":"【mybatis】解决mybatis-plus的分页问题","url":"/note/JAVA/SSM三大框架/【mybatis】解决mybatis-plus的分页问题/","content":"\n\n\n\n\n---\n\n## 现象\n\nmybatis-plus的分页插件：\n\n- 默认只支持一页500条；\n- 但凡是在代码中指定分页大于500的，统一当做500处理；\n\n\n\n## 怎么解决\n\n\n\n```java\n@Configuration\npublic class MybatisPlusConfig extends MybatisPlusBaseConfig {\n\n    @Bean\n    public PaginationInterceptor paginationInterceptor() {\n        PaginationInterceptor paginationInterceptor = new PaginationInterceptor();\n        // 设置最大单页限制数量，默认 500 条，-1 不受限制\n        paginationInterceptor.setLimit(50000L);\n        return paginationInterceptor;\n    }\n\n}\n```\n\n\n\n在顺丰的框架中（sf-boot）：直接按照上面这样配置，会报错:\n\n- 因为`com.sf.boot.base.config.MybatisPlusBaseConfig`已经把这个分页插件注册进来了，我们自己在注册一遍，就会出现问题；\n\n```java\nError starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.\n2022-08-25T19:22:41.054+0800|ERROR|main|org.springframework.boot.diagnostics.LoggingFailureAnalysisReporter||\n\n***************************\nAPPLICATION FAILED TO START\n***************************\n\nDescription:\n\nThe bean 'performanceInterceptor', defined in class path resource [com/sf/boot/base/config/MybatisPlusBaseConfig.class], could not be registered. A bean with that name has already been defined in class path resource [com/sf/fns/nas/amp/config/MybatisPlusConfig.class] and overriding is disabled.\n\nAction:\n\nConsider renaming one of the beans or enabling overriding by setting spring.main.allow-bean-definition-overriding=true\n```\n\n\n\n怎么解决呢？\n\n- 方法一：开启spirng的bean重写配置\n  - 不太靠谱，因为两个地方同时定义了一个bean ，使用bean的覆盖重写其实在某种情况下是很可能出事的。\n\n```\nspring.main.allow-bean-definition-overriding=true\n```\n\n- 【目前我采用的】方法二：启动类中排除`com.sf.boot.base.config.MybatisPlusBaseConfig`\n\n```java\n@EnableFeignClients(basePackages = {\"com.xx.xxx.xxx\"})\n@RestController\n@EnableTransactionManagement\n@MapperScan(\"com.xx.xx.xx.xx.mapper\")\n@SpringBootApplication(exclude = MybatisPlusBaseConfig.class)\npublic class AppApplication {\n    public static void main(String[] args) {\n        SpringApplication app = new SpringApplication(AppApplication.class);\n        app.setBannerMode(Banner.Mode.OFF);\n        app.run(args);\n    }\n}\n\n```\n\n\n\n## 源码原因\n\n<img src=\"【mybatis】解决mybatis-plus的分页问题.assets/image-20220825111404053.png\" alt=\"image-20220825111404053\" style=\"zoom:80%;\" />\n\n<img src=\"【mybatis】解决mybatis-plus的分页问题.assets/image-20220825111444219.png\" alt=\"image-20220825111444219\" style=\"zoom:80%;\" />\n\n\n\n","tags":["mybatis-plus","分页"],"categories":["JAVA","SSM三大框架"]},{"title":"mybatis从入门到入土","url":"/note/JAVA/SSM三大框架/【mybatis】Mybatis从入门到入土/","content":"\n\n\n# mybatis从入门到入土\n\n---\n# 1 环境准备\n## 1.1 下载源码导入IDEA\n\n先下载这三个项目：我是fork到自己仓库了，也可以从mybatis官方仓库下载：[https://github.com/mybatis](https://github.com/mybatis)\n\n> [https://github.com/zhuansun/mybatis-3](https://github.com/zhuansun/mybatis-3)\n\n> [https://github.com/zhuansun/spring](https://github.com/zhuansun/spring)\n\n> [https://github.com/zhuansun/parent](https://github.com/zhuansun/parent)\n\nmybatis-3：mybatis源码项目\n\nspring：mybatis与spring集成使用的，方便mybatis集成spring\n\nparent：mybatis源码所依赖的基础依赖\n\n将这三个项目导入到IDEA，导入方式不是直接打开文件夹，而是新建空项目，然后添加module（可以参考《Mybatis3源码深度解析 1.4节》）：\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/12Qv-Ty5zzBk7QuJvMls9YcQJ1sdFDpF6dZ3G-izeN0.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n项目接口如下：\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/qptRYBgkk0ZUvKQ4jdiTI6mZ-hE3SQ5K1DhAE5OEEFs.png\" alt=\"image\" style=\"zoom: 150%;\" />\n\n## 1.2 HSQLDB数据库简介\n* **是什么**\n* 是纯java语言写的关系型数据库管理系统\n* **怎么用**\n* 运行方式有两种：单独server部署（独立部署）或者内存模式运行（嵌入到应用中）；\n* 数据保存有两种方式：内存或者磁盘；\n* **为什么要用**\n* 因为Mybatis源码使用了HSQLDB作为单元测试使用的数据库，所以为了学习源码，需要了解。\n* **使用示例**\n* 1、准备好sql文件；\n* 2、在项目中引入hsqldb的依赖\n* 3、在代码中就可以直接使用\n\n# 2 JDBC知识准备\n## 2.1 JDBC是什么？\n* java语言提供的访问关系型数据的接口，或者说是规范；\n\n## 2.2 JDBC怎么用?\n* 一般来说分为四个步骤\n* 1、与数据源建立连接；\n* 2、执行SQL语句；\n* 3、检索SQL执行结果；\n* 4、关闭连接\n### * **与数据源建立连接**\n* 简单的来说，就是说去Connection对象：JDBC提供了Connection接口，用来表示与底层数据源的连接；\n* 获取Connection对象的方式一：DriverManager\n\n```java\nConnection connection = DriverManager.getConnection(\"jdbc:hsqldb:mem:mybatis\",\"sa\", \"\")\n```\n* 获取Connection对象的方式二：DataSource\n\n```java\n// 创建DataSource实例\nDataSource dataSource = new UnpooledDataSource(\"org.hsqldb.jdbcDriver\",\"jdbc:hsqldb:mem:mybatis\",\"sa\", \"\");\n// 获取Connection对象\nConnection connection = dataSource.getConnection();\n```\n### * **执行SQL语句**\n* JDBC获取到Connection对象后，可以设置事务或者创建Statement，PreparedStatement，CllableStatement对象；\n\n* Statement对象可以理解为SQL语句的执行器，比如调用Statement中的executeQuery方法执行查询，executeUpdate方法执行更新；\n* Statement执行之后，可以通过Statement接口提供的getResultSet获取查询结果集，或者通过getUpdateCount获取更新影响的行数；\n\n### * **检索SQL执行结果**\n* 执行SQL之后会有结果集，比如Statement使用getResultSet得到结果集，我们使用ResultSet这个接口来接收这些返回值；\n* 获取到返回值之后，使用ResultSet提供的各种get方法可以拿到结果；\n### * **关闭连接**\n* 关闭连接\n\n## 2.3 JDBC核心类解读\n### Connection\n* 是什么？一个Connection对象表示通过JDBC驱动与数据源建立的连接，这里的数据源可以是关系型数据库，文件系统或者是其他通过JDBC访问的数据。\n* 特性：使用JDBC API的应用程序，可能需要维护多个Connection对象，一个Connection对象可能访问多个数据源，也可能访问单个数据源。**？？？？没看懂**\n* 获取Connection的方式有两种：**一种是通过DriverManager，一种是通过DateSource（主流都是用这个，推荐）；**\n* 具体相关：包括JDBC驱动类型，DriverManager类，Driver接口，DataSource接口等；\n* JDBC**驱动类型**\n* JDBC驱动类型不是我们所说mysql-connection-java，而是更为底层的驱动类型方式，常见的mysql-connection-java这个jar包只是其中一个驱动类型的具体实现；\n* JDBC-ODBC Bridge Driver ：应用程序->JDBC API->JDBC驱动->ODBC驱动<-通信协议->数据库（桥接影响性能，不推荐）\n* Native API Driver：应用程序->JDBC API->JDBC驱动->特定客户端/特定链接库<-通信协议->特定数据库（特定数据库使用特定链接库，不能跨平台）\n* JDBC-Net Driver：应用程序->JDBC API->JDBC驱动<-通信协议->服务器<-通信协议->数据库（中间使用了服务器转发，影响性能），微软有一款产品再用。\n* Native Protocol Driver：应用程序->JDBC API->JDBC驱动-><-通信协议->数据库（直接使用java开发JDBC某一个数据库的驱动，直接使用该驱动访问数据库，少了转换，性能好，推荐）比如常见的mysql-connection-java.jar 以及oracle的等等，都是使用这个方式。\n* **Driver接口**\n* 所有的驱动都需要实现Driver接口，并且实现一个静态代码块。\n\n```java\npublic class AcmeJdbcDriver implements java.sql.Driver {\nstatic {\njava.sql.DriverManager.registerDriver(new AcmeJdbcDriver());\n}\n...\n}\n```\n* 静态代码块的作用是：在类加载的时候，注册当前驱动的实例；\n* 因为类加载的时候就会注册驱动，所以我们使用JDBC操作数据库之前都要先加载驱动\n\n```Plain Text\n Class.forName(\"org.hsqldb.jdbcDriver\");\n```\n* 加载驱动类目前有两种方式，一种是Class.forName() 一种是通过SPI机制；\n* **DriverAction接口**\n* 可以解除注册，从DriverManager中移除已经注册的驱动，知道就行了。一般是驱动开发人员需要关注；\n* **DriverManager类**\n* Driver接口是驱动，有一些实现类，这些实现类在实例化的时候，会将自己注册到DriverManager中；\n* DriverManager有两个重要的方式：registerDriver() 用来注册驱动实例的。一个是getConnection()用来获取数据库Connection对象的。\n* DriverManager的getConnection，会根据传进来的URL，解析，然后通过URL拿到具体的驱动实现类，最后通过具体的驱动实现类链接到对应的数据库。\n* **小结：三者之间的关系**\n\n```java\n//Driver调用DriverManager注册\npublic class XXXDriver implements Driver{\n    static void registerMe() {\n        DriverManager.registerDriver(new XXXDriver);\n    }\n    Connection connect(String url, java.util.Properties info) throws SQLException{\n        //具体实现\n    }\n}\n//应用程序调用DriverManager获取Connection连接\npublic MainTest{\n    public static void main(String[] args){\n        Connection connection = DriverManager.getConnection(...);\n    }\n}\n//DriverManager调用Driver获取Connection连接\npublic class Drivermanager{\n    //DriverManager的源码\n    public static Connection getConnection(...) throws SQLException {\n        //最终调用的是Driver接口中的connect方法\n        return driver.connect();\n    }\n}\n```\n* **DataSource接口**\n* 和DriverManager对比\n\n|      | DriverManager                                                | DataSource                                                   |\n| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n|      | 需要在代码中硬编码指定驱动；Class.forName(....)，如果需要修改的时候，需要修改业务代码 | 使用配置的方式，同时使用JNDI暴露服务，通过逻辑名称可以简单的获取到DataSource对象，从而获得Connection； 另外当修改数据源的时候，只需要修改配置，不需要对应用代码进行修改。提高了应用程序的可移植性 |\n|      | 每获取一次Connection对象，都需要与数据库建立一次连接，使用完之后，需要关闭连接。 | DataSource接口支持数据库连接池和分布式事务。连接池通过对连接的复用而不是新建一个物理连接来显著地提高程序的效率。从而适用于任务繁忙、负担繁重的企业级分布式事务。 |\n|      | 参考文献：[http://www.voidcn.com/article/p-vwtcyipy-rn.html](http://www.voidcn.com/article/p-vwtcyipy-rn.html) | 参考文献：[http://www.voidcn.com/article/p-vwtcyipy-rn.html](http://www.voidcn.com/article/p-vwtcyipy-rn.html) |\n\n* 大部分开源框架使用的都是DataSource接口；\n* 关闭Connection对象\n* 使用完之后需要显示的关闭；\n* close() 用于关闭connection对象\n* isClosed() 判断连接是否关闭\n* isValid() 判断连接是否有效\n\n### Statement\nStatement是一个接口，有两个比较重要的子接口：PreparedStatement和CallableStatement。\n\nStatement接口定义了执行SQL语句的方法，不支持参数输入；\n\nPreparedStatement接口中增加了设置SQL参数的方法；设值字后，再次设值需要注意，可能需要clear之后才行。\n\nCallableStatement接口继承自PreparedStatement，增加了调用存储过程以及检索存储过程调用结果的方法。\n\n* **问题一：Statement接口中boolean execute(String sql, String columnNames\\[\\]) throws SQLException;这个方法的第二个入参的含义是什么？？**\n\n表示可以被用于检索！！去你妈的。。这是人能听懂的话吗？一步一步的往下看：\n\n首先要知道，在Statement中，以execute(..)方法为例，总共有四个方法，其中有三个方法，提到了可以被用于检索：**【注意：只有当sql是INSERT的时候，第二个字段才会生效，如果是UPDATE后者是DELETE语句，第二个参数填了也没用】**\n\n![image](【mybatis】Mybatis从入门到入土.assets/7_2hrcGsoVHlxvOYILE01hMteeiMPRDWx1WojqRqa6s.png)\n\n对于这三个方法，我们来看看第二个字段分别是什么含义？（摘录自java8api）\n\n```Plain Text\nautoGeneratedKeys - 一个常数，表示使用方法getGeneratedKeys是否应使自动生成的密钥可用于getGeneratedKeys; 以下常数之一： Statement.RETURN_GENERATED_KEYS或Statement.NO_GENERATED_KEYS\n```\n```Plain Text\ncolumnIndexes - 插入行中的列的索引数组，应该可用于通过调用方法进行 getGeneratedKeys\n```\n```Plain Text\ncolumnNames - 插入行中列的名称数组，应该可用于通过调用方法进行 getGeneratedKeys\n```\n瞅瞅，这说的是人话吗！！那么接下来用人话说就是：当我们希望拿到INSERT语句的返回结果（返回结果可以是主键，可以是表中某一个字段）的时候，就指定第二个参数，这样获取到resultSet的时候，就可以拿到我们期望的返回结果。这就叫做**可用于检索**！还是不懂，看个例子就懂了。\n\n例子1：不指定检索字段，是什么样子的\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/yh0upAZS2UmhemvNqE9x10Sqbq5C9xv3yUbszNTb2Y4.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n例子2：指定autoGeneratedKeys检索\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/K2JlTQsnkVc0_tujJUfD9j77ppode0Z28voacv5dShk.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n例子3：\n\n* 不贴代码了\n* 当我们指定检索为：\n\n```Plain Text\nstmt.executeUpdate(sql,Statement.NO_GENERATED_KEYS);\n```\n的时候，和没指定检索是一样的效果；\n\n例子4：\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/peIsrngrofKF5hmFox_31n-jIK03_WatS4LrQMvmwy8.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n总结：到这里，可能有点眉目了，但是还是不知道检索是什么？我们再来接着看指定columnIndexes和columnNames的时候，是什么样子的。\n\n例子5：\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/CTZdEsMJD22gk-eO4eaolRQ81UHZ8jwyrbkBfSqI0lI.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n例子6：\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/LdXnJySFue9S29nD9QMJ8TRGbc5MP7Zaa-K0lIzTZ0s.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n例子7：\n\n* 使用columnIndexes和columnNames的时候，如果超了（指定了3个索引，但是get4个），还是会报错的；\n* 当指定是columnNames的时候，除了可以通过getXXX(数字)的形式，也可以通过getXXX(\"name\")的形式；\n\n总结：当我们希望拿到INSERT语句的返回结果（返回结果可以是主键，可以是表中某一个字段）的时候，就指定第二个参数，这样获取到resultSet的时候，就可以拿到我们期望的返回结果。这就叫做**可用于检索**\n\n* **问题二：CallableStatement到底是干嘛的？什么是存储过程？什么是IN，什么是OUT，什么是INOUT？**\n\n想解答这个问题，需要知道什么是存储过程，都没有学习，也不知道是个啥。\n\n### ResultSet\nResultSet简单的说，就是SQL执行的结果，掌握下面三点：\n\nResultSet的类型：游标是否是可以滚动和修改是否对数据库敏感\n\nResultSet的并行性：resultSet是否可以读写\n\nResultSet的可保持性：事务结束后是否要关闭resultSet\n\n* ResultSet的类型：\n* TYPE\\_FORWARD\\_ONLY（默认）：游标只能向前\n* TYPE\\_SCROLL\\_INSENSITIVE：游标可向前向后，也可指定；ResultSet数据的修改对数据库不敏感（不敏感的意思就是ResultSet的修改不会影响到数据库中的记录）；\n* TYPE\\_SCROLL\\_SENSITIVE：游标可向前向后，也可指定；ResultSet数据库的修改对数据库敏感；\n* 这三个类型，在创建Statement的时候，可以指定\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/MvWGhLnUmWRbrV8Smj9H7lUHrTZx8e41t5wRwKdh7yk.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n* ResultSet的并行性\n* CONCUR\\_READ\\_ONLY：为resultSet设置这种属性后，只能从ResultSet中读取数据；修改会报错；\n* CONCUR\\_UPDATABLE：为resultSet设置这种属性后，可读可写；\n* 这两个属性，也是在创建Statement的时候，可以指定的（同ResultSet的类型一样，都是可以指定的）\n* ResultSet的可保持性\n* HOLD\\_CURSOURS\\_OVER\\_COMMIT：调用connection的commit方法后，不关闭当前事务创建的resultSet。\n* CLOSE\\_CURSOURS\\_AT\\_COMMIT：会关闭；好处就是会提高系统的性能。\n* 默认的可保持性，取决于驱动的具体实现。\n* 修改ResultSet对象\n* 并行性为CONCUR\\_UPDATABLE的ResultSet可以使用ResultSet接口中提供的方法对其进行更新，包括更新行，删除行，在驱动的支持下，还可以插入行；\n* 关闭ResultSet对象\n\n### DataBaseMetaData\n* 简单的说，就是用来提供底层数据源的相关的信息；比如获取数据源的信息，获取数据源是否支持某一特性，获取数据源的限制等等；\n* 创建DataBaseMetaData对象：通过connection对象创建的。\n* 获取数据源的基本信息\n\n```Plain Text\n@Test\n    public void testDbMetaData() {\n        try {\n            Class.forName(\"org.hsqldb.jdbcDriver\");\n            // 获取Connection对象\n            Connection conn = DriverManager.getConnection(\"jdbc:hsqldb:mem:mybatis\",\n                    \"sa\", \"\");\n            DatabaseMetaData dmd = conn.getMetaData();\n            System.out.println(\"数据库URL:\" + dmd.getURL());\n            System.out.println(\"数据库用户名:\" + dmd.getUserName());\n            System.out.println(\"数据库产品名:\" + dmd.getDatabaseProductName());\n            System.out.println(\"数据库产品版本:\" + dmd.getDatabaseProductVersion());\n            System.out.println(\"驱动主版本:\" + dmd.getDriverMajorVersion());\n            System.out.println(\"驱动副版本:\" + dmd.getDriverMinorVersion());\n            System.out.println(\"数据库供应商用于schema的首选术语:\" + dmd.getSchemaTerm());\n            System.out.println(\"数据库供应商用于catalog的首选术语:\" + dmd.getCatalogTerm());\n            System.out.println(\"数据库供应商用于procedure的首选术语:\" + dmd.getProcedureTerm());\n            System.out.println(\"null值是否高排序:\" + dmd.nullsAreSortedHigh());\n            System.out.println(\"null值是否低排序:\" + dmd.nullsAreSortedLow());\n            System.out.println(\"数据库是否将表存储在本地文件中:\" + dmd.usesLocalFiles());\n            System.out.println(\"数据库是否为每个表使用一个文件:\" + dmd.usesLocalFilePerTable());\n            System.out.println(\"数据库SQL关键字:\" + dmd.getSQLKeywords());\n            IOUtils.closeQuietly(conn);\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n```\n## 2.4 JDBC事务\n* 事务边界：事务的开启是JDBC驱动或者数据库决定的，不能再代码中显示的开启；有些框架可能支持显示开始事务，至少JDBC不支持；什么时候开启事务，是SQL:2003规范决定的。\n* 事务提交：关闭一个事务有两种方式，一种是自动提交（默认开启）：SQL在执行完之后，自动提交事务；还有一种是手动提交事务，需要关闭自动提交，然后在提交事务的时候，调用commit()方法；或者使用rollback()回滚事务；\n* 事务隔离级别：事务隔离级别用于指定事务中对数据的操作对其他事务的可见性；\n* 事务中出现的问题\n* 脏读：B事务读取A事务中没有commit的数据，如果A回滚，此时B事务的数据就是错误的；\n* 不可重复读：A事务操作时间很长，开始的时候读一条记录，B事务修改了这条记录，A事务再次读取这条记录将得到不同的结果；\n* 幻读：A事务读取一些符合条件的数据，B事务插入了符合条件的若干数据，A通过相同的条件将会读取到B事务插入的数据；\n* 事务的隔离级别\n* TRANSACTION\\_NONE：驱动不支持事务\n* TRANSACTION\\_READ\\_UNCOMMITED：允许其他事务读取当前事务未提交的数据，可能出现脏读，不可重复读，幻读；\n* TRANSACTION\\_READ\\_COMMITED：当前事务未提交对其他事务是不可见的。可以解决脏读，会出现不可重复读，幻读\n* TRANSACTION\\_REPEATABLE\\_READ：保证在一次事务中，相同查询得到结果是一致的，可以解决脏读和不可重复读，不能解决幻读；\n* TRANSACTION\\_SERIALIZABLE：串行事务；可以解决上面三个问题，但是并发效率极低；\n* 默认的事务隔离级别是由数据库驱动实现的。也可以通过connecton.setTransactionSolation()设置；\n* 事务的保存点\n* 保存点通过在事务中标记一个中间的点，一旦标记，事务可以回滚到标记点，而不影响保存点之前的操作；\n* 驱动是否支持保存点，可以通过DataBaseMetaData中的supportSavePoints()来指定；\n* 设置保存点：connection.setSavePoint()\n* 回滚到保存点：setSavePoint会返回一个SavePoint对象，rollback的时候可以把这个对象传进去；\n\n**“磨刀不误砍柴工”这句话说得好，掌握了JDBC，在学习框架源码，往上靠就行了**\n\n# 3 Mybatis常用工具类\n介绍mybatis中一些比较实用的工具类，例如：SQL，ScriptRunner，SqlRunner和MetaObject等；\n\n## 3.1 SQL类\n用于在Java代码中动态构建SQL语句；\n\n```java\n    @Test\n    public void testSelectSQL() {\n        String orgSql = \"SELECT P.ID, P.USERNAME, P.PASSWORD, P.FULL_NAME, P.LAST_NAME, P.CREATED_ON, P.UPDATED_ON\\n\" +\n                        \"FROM PERSON P, ACCOUNT A\\n\" +\n                        \"INNER JOIN DEPARTMENT D on D.ID = P.DEPARTMENT_ID\\n\" +\n                        \"INNER JOIN COMPANY C on D.COMPANY_ID = C.ID\\n\" +\n                        \"WHERE (P.ID = A.ID AND P.FIRST_NAME like ?) \\n\" +\n                        \"OR (P.LAST_NAME like ?)\\n\" +\n                        \"GROUP BY P.ID\\n\" +\n                        \"HAVING (P.LAST_NAME like ?) \\n\" +\n                        \"OR (P.FIRST_NAME like ?)\\n\" +\n                        \"ORDER BY P.ID, P.FULL_NAME\";\n\n        String newSql =  new SQL() {{\n                    SELECT(\"P.ID, P.USERNAME, P.PASSWORD, P.FULL_NAME\");\n                    SELECT(\"P.LAST_NAME, P.CREATED_ON, P.UPDATED_ON\");\n                    FROM(\"PERSON P\");\n                    FROM(\"ACCOUNT A\");\n                    INNER_JOIN(\"DEPARTMENT D on D.ID = P.DEPARTMENT_ID\");\n                    INNER_JOIN(\"COMPANY C on D.COMPANY_ID = C.ID\");\n                    WHERE(\"P.ID = A.ID\");\n                    WHERE(\"P.FIRST_NAME like ?\");\n                    OR();\n                    WHERE(\"P.LAST_NAME like ?\");\n                    GROUP_BY(\"P.ID\");\n                    HAVING(\"P.LAST_NAME like ?\");\n                    OR();\n                    HAVING(\"P.FIRST_NAME like ?\");\n                    ORDER_BY(\"P.ID\");\n                    ORDER_BY(\"P.FULL_NAME\");\n                }}.toString();\n\n        assertEquals(orgSql, newSql);\n    }\n```\n## 3.2 SqlRunner和ScriptRunner\n在Mybatis源码测试用例中出现的频率比较高；用于执行SQL语句和SQL脚本；\n\n```java\n @Test\n    public void testSelectOne() throws SQLException {\n        SqlRunner sqlRunner = new SqlRunner(connection);\n        String qryUserSql = new SQL() {{\n            SELECT(\"*\");\n            FROM(\"user\");\n            WHERE(\"id = ?\");\n        }}.toString();\n        Map<String, Object> resultMap = sqlRunner.selectOne(qryUserSql, Integer.valueOf(1));\n        System.out.println(JSON.toJSONString(resultMap));\n    }\n```\n```java\n @Test\n    public void testScriptRunner() {\n        try {\n            Connection connection = DriverManager.getConnection(\"jdbc:hsqldb:mem:mybatis\",\n                    \"sa\", \"\");\n            ScriptRunner scriptRunner = new ScriptRunner(connection);\n            scriptRunner.runScript(Resources.getResourceAsReader(\"create-table.sql\"));\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n```\n## 3.3 MetaObject和MetaClass\n是Mybatis中的反射工具类，分别封装了对对象和类的反射访问；\n\n```java\n    @Test\n    public void testMetaObject() {\n        List<Order> orders = new ArrayList() {\n            {\n                add(new Order(\"order20171024010246\", \"《Mybatis源码深度解析》图书\"));\n                add(new Order(\"order20171024010248\", \"《AngularJS入门与进阶》图书\"));\n            }\n        };\n        User user = new User(orders, \"江荣波\", 3);\n        MetaObject metaObject = SystemMetaObject.forObject(user);\n        // 获取第一笔订单的商品名称\n        System.out.println(metaObject.getValue(\"orders[0].goodsName\"));\n        // 获取第二笔订单的商品名称\n        System.out.println(metaObject.getValue(\"orders[1].goodsName\"));\n        // 为属性设置值\n        metaObject.setValue(\"orders[1].orderNo\",\"order20181113010139\");\n        // 判断User对象是否有orderNo属性\n        System.out.println(\"是否有orderNo属性且orderNo属性有对应的Getter方法：\" + metaObject.hasGetter(\"orderNo\"));\n        // 判断User对象是否有name属性\n        System.out.println(\"是否有name属性且name属性有对应的Getter方法：\" + metaObject.hasGetter(\"name\"));\n\n    }\n```\n```java\n    @Test\n    public void testMetaClass() {\n        MetaClass metaClass = MetaClass.forClass(Order.class, new DefaultReflectorFactory());\n        // 获取所有有Getter方法的属性名\n        String[] getterNames = metaClass.getGetterNames();\n        System.out.println(JSON.toJSONString(getterNames));\n        // 是否有默认构造方法\n        System.out.println(\"是否有默认构造方法：\" + metaClass.hasDefaultConstructor());\n        // 某属性是否有对应的Getter/Setter方法\n        System.out.println(\"orderNo属性是否有对应的Getter方法：\" + metaClass.hasGetter(\"orderNo\"));\n        System.out.println(\"orderNo属性是否有对应的Setter方法：\" + metaClass.hasSetter(\"orderNo\"));\n\n        System.out.println(\"orderNo属性类型：\" + metaClass.getGetterType(\"orderNo\"));\n\n        // 获取属性Getter方法\n        Invoker invoker = metaClass.getGetInvoker(\"orderNo\");\n        try {\n            // 通过Invoker对象调用Getter方法获取属性值\n            Object orderNo = invoker.invoke(new Order(\"order20171024010248\",\"《Mybatis源码深度解析》图书\"), null);\n            System.out.println(orderNo);\n        } catch (IllegalAccessException e) {\n            e.printStackTrace();\n        } catch (InvocationTargetException e) {\n            e.printStackTrace();\n        }\n\n    }\n```\n## 3.4 ObjectFactory和ProxyFactory\n是对象创建的工具类，前者用于创建Mapper映射实体对象，后者用于创建Mapper映射实体对象对应的代理对象，通过动态代理实现Mybatis中懒加载机制。\n\n```java\n    @Test\n    public void testObjectFactory() {\n        ObjectFactory objectFactory = new DefaultObjectFactory();\n        List<Integer> list = objectFactory.create(List.class);\n        Map<String,String> map = objectFactory.create(Map.class);\n        list.addAll(Arrays.asList(1, 2, 3));\n        map.put(\"test\", \"test\");\n        System.out.println(list);\n        System.out.println(map);\n    }\n```\n```java\n    @Test\n    public void testProxyFactory() {\n        // 创建ProxyFactory对象\n        ProxyFactory proxyFactory = new JavassistProxyFactory();\n        Order order = new Order(\"gn20170123\",\"《Mybatis源码深度解析》图书\");\n        ObjectFactory objectFactory = new DefaultObjectFactory();\n        // 调用ProxyFactory对象的createProxy（）方法创建代理对象\n        Object proxyOrder = proxyFactory.createProxy(order\n                ,mock(ResultLoaderMap.class)\n                ,mock(Configuration.class)\n                ,objectFactory\n                ,Arrays.asList(String.class,String.class)\n                ,Arrays.asList(order.getOrderNo(),order.getGoodsName())\n        );\n        System.out.println(proxyOrder.getClass());\n        System.out.println(((Order)proxyOrder).getGoodsName());\n    }\n```\n# 4 Mybatis核心组件介绍\n## 4.1 使用Mybatis操作数据库\n* 编写mybatis的主配置文件：mybatis-config.xml\n* 新增Java实体与数据库记录建立映射\n* 定义用于执行SQL的Mapper\n* 通过Mybatis提供的API执行我们定义的Mapper\n\n## 4.2 Mybatis核心组件\n<img src=\"【mybatis】Mybatis从入门到入土.assets/-TKD7IgXtBFz_zTiV8VgeJKYL2twPUzV38ZrFg6CzPs.png\" alt=\"image\" style=\"zoom:67%;\" />\n\n### Configuration\nMybatis的配置信息有两种，一种是用来描述Mybatis的主配置信息，一种是用来配置执行SQL语句的Mapper配置文件。\n\nConfiguration是用来描述Mybatis的主配置信息的，其他组件需要获取配置信息的时候，直接通过Configuration对象获取。除此之外，MyBatis在应用启动时，将Mapper配置信息，类型别名，TypeHandler等注册到Configuration组件中，其他组件需要这些信息时，也可以从Configuration对象中获取。\n\n三大作用：\n\n* 用于描述MyBatis配置信息，项目启动时，所有的配置信息都会转换为configuration对象；\n* 作为中间这简化MyBatis各个组件之间的交互，属于**中介者模式**的应用；\n* 作为Executor, ParameterHandler, ResultSetHandler, StatementHandler组件的工厂，便于创建这些组件的实例\n\n### MappedStatement\n用来描述Mapper中的SQL配置信息，是对XML配置文件中///等标签或者@Select/@Update等注解配置信息的封装。\n\n### SqlSession\n是Mybatis提供的面向用户的API，表示和数据库交互时的会话对象，用于完成数据库的CRUD功能，SqlSession是Executor组件的外观，目的是对外提供易于理解和使用的数据库操作接口；\n\n### Executor\n是Mybatis的SQL执行器，在Mybatis中对数据库所有的增删改查操作都是由Executor组件完成的。\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/3MheQjlJAuwOpC0tNFJavPgWUJVQfKCxgQUX0X-ezWQ.png\" alt=\"image\" style=\"zoom: 150%;\" />\n\nSimpleExecutor：能够完成基本的增删改查操作\n\nReuseExecutor：对JDBC的Statement做了缓存，当执行同样的SQL时，直接从缓存中取Statement，避免了频繁创建和销毁，从而提升系统性能；\n\nBatchExecutor：会对调用同一个Mapper执行的update，insert和delete操作，调用Statement对象的批量处理功能。\n\nCachingExecutor：我们知道Mybatis支持一二级缓存，当开启了二级缓存时，会使用CachingExecutor对上面三个进行装饰，为查询增加二级缓存功能。用到了**装饰者模式**的设计模式\n\nExecutor是在SqlSessionFactory.openSession的时候创建的。\n\n### StatementHandler\n封装了对JDBC Statement对象的使用，比如为Statement对象设置参数，调用Statement接口与数据库交互等等。\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/2-ndfSVFWd93s2tMI-R9AYlw7zsz8AQp_Gs-lkLKYa0.png\" alt=\"image\" style=\"zoom:80%;\" />\n\nBaseStatementHandler：抽象类，封装了通用逻辑和方法执行流程，使用了模板方法模式；\n\nSImpleStatementHandler：封装了对JDBC中Statement对象的操作；\n\nPreparedStatementHandler：封装了对JDBC中PreparedStatement对象的操作；\n\nCallableStatementHandler：封装了对JDBC中CallableStatement对象的操作；\n\nRoutingStatementHandler：会更具Mapper配置中的statementType属性（取值为STATEMENT，PREPARED或CALLABLE）创建对应的StatementHandler的实现\n\n### ParameterHandler\n当Mybatis框架使用的Statement类型是PreparedStatement和CallableStatement时，ParameterHandler用于为Statement对象参数占位符设置值。\n\n比较简单，就一个实现：拿到sql中所有的参数，遍历所有的参数，如果需要设值，就获取TypeHandler，然后通过TypeHandler对这个占位符设值。\n\n### ResultSetHandler\n封装了对JDBC中ResultSet的使用，当MyBatis执行的SQL类型是SELECT语句时，ResultSetHandler用于将查询结果转换成Java对象。\n\n一样比较简单，只有一个默认的实现。\n\n### TypeHandler\n是Mybatis中的类型处理器，用于处理Java类型与JDBC类型之间的映射。它的作用主要体现在能够根据Java类型调用PreparedStatement（JDBC的）或CallableStatement（JDBC的）对象对应的SetXXX()方法为Statement对象设置值，而且能够根据Java类型调用ResultSet（JDBC的）对象赌赢的getXXX()获取SQL执行结果。\n\n简单的说：及时jdbc类型和Java类型处理映射的关系；\n\nMybatis通过TypeHandlerRegistry建立JDBC类型，Java类型与Typehandler之间的映射关系；这个类的逻辑比较简单，就是在维护了三个map，分别是jdbc类型与TypeHander的关系，Java类型与Jdbc类型与TypeHander的关系，TypeHander的class与TypeHander之间的关系；这三个map会在TypeHandlerRegistry的无参构造方法中进行register，而这个无参构造方法是在Configuration中被调用的。\n\n### 总结\nMybatis通过sqlSession操作数据库，sqlSession是用户层面的API。\n\n实际上sqlSession是Executor组件的外观（外观模式），目的是为用户提供更友好的方式操作数据库。\n\n真正执行sql操作的是Executor组件，Executor组件可以理解为SQL的执行器，它会使用StatementHandler组件对JDBC的Statement对象进行操作。\n\n当Statement类型是CallableStatement和PreparedStatement时，会通过ParameterHandler组件为参数占位符赋值。\n\nParameterHandler组件中会根据Java类型找到对应的TypeHandler对象；\n\nTypeHandler中会通过Statement提供的setXXX方法为Statement对象中的参数占位符进行设值。\n\nStatementHandler组件使用JDBC中的Statement对象与数据库完成交互后，当SQL语句类型是SELECT时，Mybatis通过ResultSetHandler组件从Statement对象中获取ResultSet对象，然后将ResultSet对象转换为Java对象。\n\n# 5 SqlSession的创建过程\nsqlSession是Mybatis的最顶层API，来学习一下它的创建过程，从三个部分入手\n\n* Configuration实例的创建过程\n* SqlSessionFactory实例的创建过程\n* SqlSession实例化的过程\n\n## 5.1 Configuration实例的创建过程\nConfiguration是Mybatis中比较重要的组件，我们知道mybatis有两种配置文件，一种是描述sql的mapper.xml配置文件，一种是描述mybatis配置的文件；Configuration的创建和xml文件密切相关，但是怎么从xml文件到Configuration的呢？\n\n* 涉及到xml文件的解析，mybatis采用的**XPath**解析xml文件，将配置信息转换成Configuration对象的；（不细说）\n* MyBatis封装了**XPathParser**工具类，简化了XPath的操作，可以方便的获取节点属性，子节点信息等\n\nConfiguration有三个作用：\n\n* 用于描述MyBatis配置信息，例如标签配置的参数信息\n* 作为容器注册Mybatis的其他组件,例如TypeHandler，MappedStatement等\n* 提供工厂方法，创建ResultSethandler，StatementHandler，Executor，Parameterhandler等\n\nConfiguration的创建流程\n\n* 入口：MyBatis通过XMLConfigBuilder来创建Configuration对象的；XMLConfigBuilder接收一个xml配置文件的输入流，调用parse()方法，返回一个Configuration对象。\n* XMLConfigBuilder的parse()方法，会先解析configuration标签，然后通过parseConfiguration()这个方法，解析mybatis的所有标签，每一个标签的解析都有一个单独的方法。\n* 当所有的方法都解析完成之后，就得到了我们的configuration对象。\n\n```java\n        Reader reader = Resources.getResourceAsReader(\"mybatis-config.xml\");\n        // 创建XMLConfigBuilder实例\n        XMLConfigBuilder builder = new XMLConfigBuilder(reader);\n        // 调用XMLConfigBuilder.parse（）方法，解析XML创建Configuration对象\n        Configuration conf = builder.parse();\n```\n## 5.2 SqlSessionFactory实例的创建过程\nmybatis中的sqlSession是使用工厂模式创建的。所以在创建sqlSession之前，需要先创建SqlSessionFactory对象；然后调用工厂的openSession()方法，得到一个sqlSession对象。\n\nsqlSession是mybatis面向用户的最顶层的API；\n\n```java\n        // 获取Mybatis配置文件输入流\n        Reader reader = Resources.getResourceAsReader(\"mybatis-config.xml\");\n        // 通过SqlSessionFactoryBuilder创建SqlSessionFactory实例\n        SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(reader);\n        // 调用SqlSessionFactory的openSession（）方法，创建SqlSession实例\n        SqlSession session = sqlSessionFactory.openSession();\n```\n为了创建sqlSessionFactory对象，首先创建了一个SqlSessionFactoryBuilder对象，然后调用它的build方法，返回一个SqlSessionFactory对象；\n\n那么他的build方法都做了什么事情呢？\n\n```java\n  public SqlSessionFactory build(Reader reader, String environment, Properties properties) {\n    try {\n      XMLConfigBuilder parser = new XMLConfigBuilder(reader, environment, properties);\n      return build(parser.parse());\n    }\n    ....\n  }\n```\nbuild方法调用重载的build方法，创建了一个XMLConfigBuilder对象，通过XMLConfigBuilder的parse方法得到一个configuration对象，然后再次调用build方法，就可以得到一个SqlSessionFactory对象。\n\n这里的build方法又做了什么呢？\n\n```java\n  public SqlSessionFactory build(Configuration config) {\n    return new DefaultSqlSessionFactory(config);\n  }\n```\n就是仅仅的一个new而已。这样就**得到了SqlSessionFactory对象**。接下来看**怎么获取sqlSession**呢？\n\n## 5.3 SqlSession实例化的过程\n**得到了SqlSessionFactory对象**。接下来看**怎么获取sqlSession**呢？\n\n拿到sqlSessionFactory后，通过factory的openSession可以得到一个sqlSession对象；openSession都干了什么呢？\n\n```java\n  private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) {\n    Transaction tx = null;\n    try {\n      //获取mybatis主配置文件的环境信息\n      final Environment environment = configuration.getEnvironment();\n      //创建事务管理器工厂\n      final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment);\n      //创建事务管理器\n      tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit);\n      //根据mybatis主配置文件的executorType的类型创建对应的executor实例（我们知道mybatis执行sql使用过Executor执行的）\n      final Executor executor = configuration.newExecutor(tx, execType);\n      //创建defaultSqlSession实例\n      return new DefaultSqlSession(configuration, executor, autoCommit);\n    } catch (Exception e) {\n      // may have fetched a connection so lets call close()\n      closeTransaction(tx);\n      throw ExceptionFactory.wrapException(\"Error opening session.  Cause: \" + e, e);\n    } finally {\n      ErrorContext.instance().reset();\n    }\n  }\n```\n获取mybatis主配置文件的环境信息，然后通过环境信息获取事务管理器工厂，通过事务管理器工厂获取事务管理器；\n\n然后获取mybatis主配置文件的executorType类型，和事务管理器一起创建Executor实例；\n\n最后获取sqlSession对象。sqlSession对象中持有executor对象的引用，真正执行SQL操作的是Executor对象。\n\n# 6 SqlSession执行Mapper的过程\nMyBatis的Mapper是由两部分组成：\n\n* 一个是Mapper接口：XXXMapper.java\n\n```java\npublic interface UserMapper {\n\n    List<UserEntity> listAllUser();\n\n    @Select(\"select * from user where id=#{userId,jdbcType=INTEGER}\")\n    UserEntity getUserById(@Param(\"userId\") String userId);\n\n    List<UserEntity> getUserByEntity( UserEntity user);\n}\n```\n* 一个是通过注解或者是XML配置的：XXXMapper.xml\n\n```java\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\">\n<mapper namespace=\"com.blog4java.mybatis.example.mapper.UserMapper\">\n    <sql id=\"userAllField\">\n      id,create_time, name, password, phone, nick_name\n    </sql>\n\n    <select id=\"listAllUser\"  resultType=\"com.blog4java.mybatis.example.entity.UserEntity\" >\n        select\n        <include refid=\"userAllField\"/>\n        from user\n    </select>\n\n</mapper>\n\n```\n## 6.1 Mapper 接口的注册和获取（动态代理）过程\n既然是说注册和获取过程，肯定是两个部分，注册和获取；我们先从获取来看，因为简单。\n\nMapper接口用来定义执行SQL语句相关的方法。\n\n```java\n @Test\n    public  void testMybatis () throws IOException {\n        // ...\n        // 获取SqlSession实例\n        SqlSession sqlSession = sqlSessionFactory.openSession();\n        // 获取UserMapper代理对象\n        UserMapper userMapper = sqlSession.getMapper(UserMapper.class);\n        // 执行Mapper方法，获取执行结果\n        List<UserEntity> userList = userMapper.listAllUser();\n\n        System.out.println(JSON.toJSONString(userList));\n    }\n```\nsqlSession.getMapper()获取了一个UserMapper的引用，返回的这个引用到底是什么呢？\n\n我们知道接口中定义的方法一定要通过某个类实现，然后创建这个实现类的实例，才能调用方法。\n\n那么sqlSession.getMapper()方法返回的一定是某个类的实例，是哪个类呢？\n\n其实返回的是一个动态代理对象；下面从两个部分来解析这个部分：\n\n* 简单而标准的JDK动态代理是什么样子的？\n* Mybatis的动态代理怎么实现的？\n\n**简单而标准的动态代理**，代码可以参考mybatis-chapter06\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/Ir1XxRhQgNjnKG39bM_HfWNuek7zmX99GosqiHhoA9k.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n**Mybatis的动态代理怎么实现的？**\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/sA8c4Sv6GSkiDxHGuXIig0uq8wvE5pflypKz3yN7x6Q.png\" alt=\"image\" style=\"zoom:80%;\" />\n\nsqlSession.getmapper做了什么事情呢？\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/MZOwlUoFHpxp37EI-6X3gZ5mdx2RIUh-C_GFME_RWkU.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n简单的描述就是：\n\n1、SqlSession.getMapper()调用的是DefaultSqlSession中的getMapper();\n\n2、DefaultSqlSession中的getMapper()是从configuration中getMapper中获取；\n\n3、configuration中getMapper调用的是MapperRegistry的getMapper;\n\n4、MapperRegistry的getMapper是从自己的一个属性中获取：knownMappers.get()，获取代理工厂MapperProxyfactory；\n\n5、然后代理工厂MapperProxyfactory通过newInstance方法，调用Proxy(JDK)生成动态代理对象。\n\n> 在这里，有一个问题，对比标准的代理方式，我们发现mybatis的代理方式，好像没有持有被代理对象的引用？那么这个动态代理，代理的是什么呢？？ ----> [按住ctrl,点我查看答案](#6.6.1%20%20Mybatis%E7%9A%84%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E4%BB%A3%E7%90%86%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F)\n\n在第4步中，从knownMappers中后获取代理工厂，拿到代理工厂，就可以创建代理对象了，那么，knownMappers是什么时候注册进去的呢？这里就说了我们的**Mapper接口的注册过程**；\n\nknownMappers是一个Map对象，它里面维护的是：Mapper接口对应的class对象 和 代理工厂对象的关系；\n\nknownMappers是类MapperRegistry的一个属性，在MapperRegister中提供一个方法addMappers，通过这个方法可以注册：Mapper接口对应的class对象 和 代理工厂对象的关系；（同时MapperRegistry还有getMapper方法，可以用来获取）\n\n那么这个addMapper是什么时候调用的呢？？这里先简单的说一下，后面还会再提到。\n\n在Configuration初始化的时候，我们知道Configuration的初始化，是从过XMLConfigBuilder中的parse方法初始化的，在parse方法中初始化了很多的属性，其中就包括mapper的初始化。\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/2w3wQEXMBA-jF6EoD1Kycz-7tU6ENXtkCFjA_qYDmK4.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n在mapper初始化的方法中mapperElement中，调用了configuration的addMapper方法，然后调用了mapperRegistry.addMappers;最终会调用到我们期望调用的方法；\n\n## 6.2 Mapper XML配置信息的解析和注册过程\n前面说过，MyBatis通过MappedStatement描述Mapper的SQL配置信息，SQL配置有两种方式：\n\n* 一种是通过XML文件配置\n* 一种是通过Java注解（其实本质就是一种轻量级的配置信息）\n\n流程图搞了几天，都弄不出来，简单点，用语言描述一下\n\n1、我们知道解析xml，都要解析之后保存在Configuration中，那么找到configuration的解析入口：\n\n```java\nXMLConfigBuilder#parseConfiguration\n```\n2、因为这里说的是解析mapper，我们找到专门解析mapper标签的方法；\n\n```java\norg.apache.ibatis.builder.xml.XMLConfigBuilder#mapperElement\n```\n```java\n  private void mapperElement(XNode parent) throws Exception {\n    if (parent != null) {\n      for (XNode child : parent.getChildren()) {\n        if (\"package\".equals(child.getName())) {\n          String mapperPackage = child.getStringAttribute(\"name\");\n          //这里就是将mapper，找到对应的mapper文件和mapper class，保存在mapperRegistry中\n          configuration.addMappers(mapperPackage);\n        } else {\n\n          if (resource != null && url == null && mapperClass == null) {\n            //通过 resource 属性指定XML文件路径\n            ErrorContext.instance().resource(resource);\n            try(InputStream inputStream = Resources.getResourceAsStream(resource)) {\n              XMLMapperBuilder mapperParser = new XMLMapperBuilder(inputStream, configuration, resource, configuration.getSqlFragments());\n              mapperParser.parse();\n            }\n          }\n        }\n      }\n    }\n  }\n```\n3、在上面的代码中，进入到\n\n```java\nmapperParser.parse();\n```\n在这个方法里面解析所有的sql，并将sql生成MappedStatement，保存在configuration中；\n\n```java\n  public void parse() {\n    if (!configuration.isResourceLoaded(resource)) {\n      //调用XPathParser的evalNode()方法获取根节点对应的XNode对象，\n      //然后开始解析所有的sql\n      configurationElement(parser.evalNode(\"/mapper\"));\n      //将资源路径添加到Configuration中\n      configuration.addLoadedResource(resource);\n      //\n      bindMapperForNamespace();\n    }\n\n    //之前已经解析过一遍了，如果出了异常，继续解析之前出现异常的ResultMap对象\n    parsePendingResultMaps();\n    //之前已经解析过一遍了，如果出了异常，继续解析之前出现异常的CacheRef对象\n    parsePendingCacheRefs();\n    //之前已经解析过一遍了，如果出了异常，继续解析之前出现异常的select|insert|update|delete对象\n    parsePendingStatements();\n  }\n```\n4、进入到主要的解析方法中\n\n```java\norg.apache.ibatis.builder.xml.XMLMapperBuilder#configurationElement\n```\n,在这个方法里面，解析所有的具体的sql语句；\n\n```java\n  private void configurationElement(XNode context) {\n    try {\n      //获取命名空间，命名空间不能为空\n      String namespace = context.getStringAttribute(\"namespace\");\n      if (namespace == null || namespace.isEmpty()) {\n        throw new BuilderException(\"Mapper's namespace cannot be empty\");\n      }\n      //设置当前正在解析的mapper的名称空间\n      builderAssistant.setCurrentNamespace(namespace);\n      cacheRefElement(context.evalNode(\"cache-ref\"));\n      cacheElement(context.evalNode(\"cache\"));\n      parameterMapElement(context.evalNodes(\"/mapper/parameterMap\"));\n      resultMapElements(context.evalNodes(\"/mapper/resultMap\"));\n      sqlElement(context.evalNodes(\"/mapper/sql\"));\n      buildStatementFromContext(context.evalNodes(\"select|insert|update|delete\"));\n    } catch (Exception e) {\n      throw new BuilderException(\"Error parsing Mapper XML. The XML location is '\" + resource + \"'. Cause: \" + e, e);\n    }\n  }\n```\n5、这个方法里面解析了很多，我们以解析crud的sql为例，进入到\n\n```Plain Text\nbuildStatementFromContext(context.evalNodes(\"select|insert|update|delete\"));\n```\n，然后在进入到\n\n```Plain Text\norg.apache.ibatis.builder.xml.XMLMapperBuilder#buildStatementFromContext(java.util.List<org.apache.ibatis.parsing.XNode>, java.lang.String)\n```\n到这里面就比较清晰了。\n\n6、使用XMLStatementBuilder进行解析，\n\n```Plain Text\nstatementParser.parseStatementNode();\n```\n解析的方法很长，就不一一列举了，当解析完成的时候，会调用builderAssistant的方法，这是一个辅助类。\n\n```Plain Text\n  public void parseStatementNode() {\n\n    //解析select|insert|update|delete标签\n\n    //将<include>标签替换成<sql>中的内容\n\n    //获取languageDriver对象\n\n    // Parse selectKey after includes and remove them.\n\n    // Parse the SQL (pre: <selectKey> and <include> were parsed and removed)\n\n    //通过languageDriver解析SQL内容，生成SqlSource对象\n\n    //默认的Statement类型是PreparedStatement\n\n    builderAssistant.addMappedStatement(id, sqlSource, statementType, sqlCommandType,\n        fetchSize, timeout, parameterMap, parameterTypeClass, resultMap, resultTypeClass,\n        resultSetTypeEnum, flushCache, useCache, resultOrdered,\n        keyGenerator, keyProperty, keyColumn, databaseId, langDriver, resultSets);\n  }\n```\n7、在`builderAssistant.addMappedStatement`中，会调用`configuration.addMappedStatement(statement);`将生成的MappedStatement放在configuration中。\n\n## 6.3 Mapper 接口中的方法调用过程\n我们知道，mybatis的方法调用的过程，其实是调用动态代理对象的invoke方法，所以，想知道接口方法的调用过程，就看看invoke方法的执行过程。\n\n```Plain Text\n // 从SqlSession中获取UserMapper代理对象\n UserMapper userMapper = sqlSession.getMapper(UserMapper.class);\n // 执行Mapper方法，获取执行结果\n List<UserEntity> userList = userMapper.listAllUser();\n```\n上面代码通过获取到的userMapper调用listAlluser的时候，其实userMapper对象已经是一个动态代理对象了。当这个方法执行的时候，其实执行的是MapperProxy的invoke方法；\n\nMapperProxy的invoke方法的代码很简单：\n\n```Plain Text\n  @Override\n  public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n    try {\n      if (Object.class.equals(method.getDeclaringClass())) {\n        //如果是继承自Object的类，就直接执行，不处理\n        return method.invoke(this, args);\n      } else {\n        //通过cachedInvoker获取MapperMethodInvoker\n        return cachedInvoker(method).invoke(proxy, method, args, sqlSession);\n      }\n    } catch (Throwable t) {\n      throw ExceptionUtil.unwrapThrowable(t);\n    }\n  }\n```\n1、继承自Object的就不说了，看else里面的。\n\n```Plain Text\ncachedInvoker(method)\n```\n这个方法是mybatis新版本里面的，之前不是这样的。\n\n2、这个方法主要作用是根据动态代理对象调用的方法，判断是否是默认方法（jdk8提供的接口也可以有方法实现体）\n\n3、如果是默认方法，就获取MethodHandle，然后封装成DefaultMethodInvoker；这里不说，不是我们关注的重点。\n\n4、如果不是默认方法，就是调用的mapper接口中的方法，就获取MapperMethod，然后封装成PlainMethodInvoker；这个是重点。\n\n5、获取到MapperMethodInvoker之后，调用invoke方法，重点关注PlainMethodInvoker的invoke方法内容。在这个方法内部，调用了MapperMethod的execute方法。\n\n6、execute方法中的内容，我们等会再说，先回过头看看MapperMethod是个什么东西？\n\n7、MapperMethod中包含SqlCommand（封装了SQL语句的类型和Mapper的ID）和MethodSignature（方法签名，返回值类型，分页信息和参数信息）；分别来看这两个内部类。\n\n8、SqlCommand：\n\n```Plain Text\n    public SqlCommand(Configuration configuration, Class<?> mapperInterface, Method method) {\n      final String methodName = method.getName();\n      // 获取声明该方法的类或接口的Class对象，目的是为了获取MappedStatement\n      final Class<?> declaringClass = method.getDeclaringClass();\n      // 获取描述 insert,update 等标签的MappedStatement对象\n      MappedStatement ms = resolveMappedStatement(mapperInterface, methodName, declaringClass,\n          configuration);\n\n    // 目的就是为了从MappedStatement中获取id和type\n        name = ms.getId();\n        type = ms.getSqlCommandType();\n\n    }\n```\n9、MethodSignature：\n\n```Plain Text\n    public MethodSignature(Configuration configuration, Class<?> mapperInterface, Method method) {\n      //获取方法的返回值类型\n      Type resolvedReturnType = TypeParameterResolver.resolveReturnType(method, mapperInterface);\n      .....\n      //RowBounds参数位置索引,用于处理后续的分页查询\n      this.rowBoundsIndex = getUniqueParamIndex(method, RowBounds.class);\n      //ResultHander参数位置索引，用于处理数据库中检索的每一行数据\n      this.resultHandlerIndex = getUniqueParamIndex(method, ResultHandler.class);\n      //ParamNameResolver用于解析Mapper方法参数\n      this.paramNameResolver = new ParamNameResolver(configuration, method);\n    }\n```\n其中new ParamNameResolver(configuration, method)需要说一说，这个类是用来描述sql的参数信息。\n\n* 将@params注解的参数获取到；\n* 如果没有注解，检查参数useActualParamName，使用参数名称作为sql参数的名字。\n* 获取到所有的参数之后，保存在map中，放在不可变的names中。\n\n10、说完了SqlCommand和MethodSignature，在回过头看MapperMethod的execute方法；\n\n* 首先根据SqlCommand对象获取sql语句的类型；\n* 然后根据sql语句的类型调用sqlSession对象对应的方法；（sqlSession是mybatis提供的用户层面的API，方便操作，mybatis中真正执行的是Executor组件）\n\n好了，结束了，在往下看，就是sqlSession怎么执行一个sql的了。\n\n## 6.4 SqlSession执行Mapper的过程\nmybatis生成动态代理之后，调用mapperProxy的invoke方法，在invoke方法中，最终会调用sqlSession的方法，以查询为例子，会调用\n\n```Plain Text\nsqlSession.selectList(command.getName(), param);\n```\n,我们知道sqlSession是mybatis提供的用户层面的API，方便用户查询操作的，所以我们看下他具体是怎么执行查询的呢？\n\n1、selectList会不停的调用重载方法，最终调用到：\n\n```Plain Text\n  private <E> List<E> selectList(String statement, Object parameter, RowBounds rowBounds, ResultHandler handler) {\n    try {\n      //MappedStatement是在mybatis启动的时候就解析xml加载进来了，这里只是根据sql的ID，拿到具体sql的MappedStatement\n      MappedStatement ms = configuration.getMappedStatement(statement);\n      // sqlSession是面向用户的API，真正执行查询的是executor执行器，它是哪里来的？\n      // 是当前sqlSession自带的，sqlSession哪里来的？是在最开始的时候openSession获取到的，然后session.getMapper将this传进来的。\n      // executor是在openSession时候赋值的\n      return executor.query(ms, wrapCollection(parameter), rowBounds, handler);\n    } catch (Exception e) {\n      throw ExceptionFactory.wrapException(\"Error querying database.  Cause: \" + e, e);\n    } finally {\n      ErrorContext.instance().reset();\n    }\n  }\n```\n2、然后看executor.query方法\n\n```Plain Text\n\n  @Override\n  public <E> List<E> query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException {\n    //获取BoundSql对象，BoundSql是对动态SQL解析生成的SQL语句和参数映射信息的封装\n    BoundSql boundSql = ms.getBoundSql(parameter);\n    //创建cacheKey，用于缓存\n    CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql);\n    //调用重载的query方法\n    return query(ms, parameter, rowBounds, resultHandler, key, boundSql);\n  }\n\n\n\n//重载的query方法\n  public <E> List<E> query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException {\n    try {\n      queryStack++;\n      //从缓存中获取结果\n      list = resultHandler == null ? (List<E>) localCache.getObject(key) : null;\n      if (list != null) {\n        handleLocallyCachedOutputParameters(ms, key, parameter, boundSql);\n      } else {\n        //缓存中获取不到，去数据库里面查询\n        list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql);\n      }\n\n    return list;\n  }\n```\n3、可以看到，最终的查询，会走到\n\n```Plain Text\nqueryFromDatabase\n```\n这个方法中，这个方法的入参是MappedStatement（sql所有信息），parameter（参数信息），rowBounds（分页信息），resultHandler（处理结果集的），key（用户缓存），boundSql（sql和参数信息）；\n\n4、\n\n```Plain Text\nqueryFromDatabase\n```\n会调用\n\n```Plain Text\ndoQuery\n```\n,它有很多实现，这里SimpleExecutor中的实现为例子:\n\n```Plain Text\n  @Override\n  public <E> List<E> doQuery(.....) throws SQLException {\n    Statement stmt = null;\n    try {\n      Configuration configuration = ms.getConfiguration();\n      //获取StatementHandler(Mybatis)\n      StatementHandler handler = configuration.newStatementHandler(wrapper, ms, parameter, rowBounds, resultHandler, boundSql);\n      //创建statement对象，并通过JDBC进行参数设置(JDBC)\n      stmt = prepareStatement(handler, ms.getStatementLog());\n      //执行查询(Mybatis)\n      return handler.query(stmt, resultHandler);\n    } finally {\n      closeStatement(stmt);\n    }\n  }\n\n\n  //执行查询(Mybatis)\n  @Override\n  public <E> List<E> query(Statement statement, ResultHandler resultHandler) throws SQLException {\n    //获取sql语句\n    String sql = boundSql.getSql();\n    //JDBC查询\n    statement.execute(sql);\n    //处理结果集\n    return resultSetHandler.handleResultSets(statement);\n  }\n```\n5、上面步骤中的参数处理，通过获取connection对象，创建statement，执行查询，获取结果，都是mybatis调用JDBC操作的。\n\n## 6.5 处理结果集\n6、现在来看一下处理结果集\n\n```Plain Text\n  //\n  @Override\n  public List<Object> handleResultSets(Statement stmt) throws SQLException {\n    ErrorContext.instance().activity(\"handling results\").object(mappedStatement.getId());\n\n    final List<Object> multipleResults = new ArrayList<>();\n\n    int resultSetCount = 0;\n    //获取resultSet（JDBC）对象，将ResuleSet对象包装成ResultSetWrapper（MyBatis）\n    ResultSetWrapper rsw = getFirstResultSet(stmt);\n\n    //获取resultMap信息，一般只有一个：在xml中配置的resultMap\n    List<ResultMap> resultMaps = mappedStatement.getResultMaps();\n    int resultMapCount = resultMaps.size();\n    validateResultMapsCount(rsw, resultMapCount);\n    while (rsw != null && resultMapCount > resultSetCount) {\n      ResultMap resultMap = resultMaps.get(resultSetCount);\n      //处理结果集\n      handleResultSet(rsw, resultMap, multipleResults, null);\n      rsw = getNextResultSet(stmt);\n      cleanUpAfterHandlingResultSet();\n      resultSetCount++;\n    }\n\n    //这里一样，获取resultSet，也是xml中配置的\n    String[] resultSets = mappedStatement.getResultSets();\n    if (resultSets != null) {\n      while (rsw != null && resultSetCount < resultSets.length) {\n        ResultMapping parentMapping = nextResultMaps.get(resultSets[resultSetCount]);\n        if (parentMapping != null) {\n          String nestedResultMapId = parentMapping.getNestedResultMapId();\n          ResultMap resultMap = configuration.getResultMap(nestedResultMapId);\n          //处理结果集\n          handleResultSet(rsw, resultMap, null, parentMapping);\n        }\n        rsw = getNextResultSet(stmt);\n        cleanUpAfterHandlingResultSet();\n        resultSetCount++;\n      }\n    }\n\n    //返回结果集\n    return collapseSingleResultList(multipleResults);\n  }\n```\n## 6.56问题解决\n### 6.6.1 Mybatis的动态代理代理的是什么？\nmybatis的动态代理，其实谁都没有代理； 它仅仅只是生成了一个mapper实例，然后利用了动态代理的“切面”功能； 然后统一使用mapperMethod执行sql查询 标准的JDK动态代理，在真正执行的时候会调用method.invoke() Mybatis的动态代理在真正执行的时候根本没用到method\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/75hg7lcYF4FCkeoxJwcroQ0L0z6VKwGWiNERUHXwGeQ.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n# 7 Mybatis缓存\n## 7.1 Mybatis 缓存（一级和二级）的使用\nMybatis缓存分为一级缓存和二级缓存；\n\n* 一级缓存默认就是开启的，不能关闭，但是可以通过localCacheScope这个属性控制级别，这个参数的取值为SESSION、STATEMENT；\n* SESSION：缓存对整个SqlSession有效，只有执行DML语句的时候，才会被清除。\n* STATEMENT：缓存仅对当前执行的语句有效，当语句执行完毕后，缓存就被清空。\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/3RlWYUTSyXpRjSV3kBog5MhiAThQ9ZyFalGFxSr-vWM.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n* 二级缓存\n* mybaits的二级缓存是mapper范围级别（二级缓存是基于namespace级别的，在同一个Mapper下有效），需要的话首先要在mybatis主配置文件中开启缓存<setting name=\"cacheEnabled\"value=\"true\"/>\n* 然后在需要二级缓存的具体mapper中配置cache，配置缓存策略，缓存刷新频率，缓存的容量等\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/HnSrXtmhbeYFG8Rz9vtJCsdEmI-oowTIpynDWmp4yTY.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n* 在配置mapper时，通过useCache属性执行mapper执行的时候是否使用缓存，还可以通过flushCache属性执行Mapper执行后是否刷新缓存。\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/ZP3e4gYDIp9OX8g-f2wsi-FdgZHWyyIQKt-ndvM0qIg.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n## 7.2 Mybatis缓存框架的实现（装饰者模式）\n```Plain Text\npublic interface Cache {\n\n  /**\n   * 获取缓存的id，通常情况下缓存的id是Mapper的命名空间名称\n   */\n  String getId();\n\n  /**\n   * 讲一个Java对象添加到缓存中\n   */\n  void putObject(Object key, Object value);\n\n  Object getObject(Object key);\n\n  Object removeObject(Object key);\n\n  /**\n   * Clears this cache instance.\n   */\n  void clear();\n\n  int getSize();\n\n  /**\n   * 在3.2.6版本后已经不在使用\n   */\n  default ReadWriteLock getReadWriteLock() {\n    return null;\n  }\n\n}\n```\n<img src=\"【mybatis】Mybatis从入门到入土.assets/-gJXouI-iocqoypYF5uhVARHtXyxs7B1vk5GLjeWc10.png\" alt=\"image\" style=\"zoom:80%;\" />\n\nMybatis的缓存设计采用的是装饰者模式，装饰者模式用的最经典的例子就是Java的IO包，Mybatis同样采用了装饰者模式。\n\n* 提供一个接口Cache，提供缓存的基本操作，比如：put，get，remove，clear等\n* 提供一个基本实现：PerpetualCache，仅使用HashMap存放缓存对象。\n* 提供了大量的装饰器，这些装饰器的构造方法入参就是Cache，比如BlockingCache（阻塞的缓存装饰器），FifoCache（先入先出缓存装饰器）等等\n\n```Plain Text\n    @Test\n    public void testCache() {\n        final int N = 100000;\n        Cache cache = new PerpetualCache(\"default\");\n        cache = new LruCache(cache);\n        cache = new FifoCache(cache);\n        cache = new SoftCache(cache);\n        cache = new WeakCache(cache);\n        cache = new ScheduledCache(cache);\n        cache = new SerializedCache(cache);\n        cache = new SynchronizedCache(cache);\n        cache = new TransactionalCache(cache);\n        for (int i = 0; i < N; i++) {\n            cache.putObject(i, i);\n            ((TransactionalCache) cache).commit();\n        }\n        System.out.println(cache.getSize());\n    }\n```\n* 另外，还提用了一个CacheBuilder方法构建缓存对象。\n\n```Plain Text\n    @Test\n    public void testCacheBuilder() {\n        final int N = 100000;\n        Cache cache = new CacheBuilder(\"com.blog4java.mybatis.example.mapper.UserMapper\")\n                .implementation( PerpetualCache.class)\n                .addDecorator(LruCache.class)\n                .clearInterval(10 * 60L)\n                .size(1024)\n                .readWrite(false)\n                .blocking(false)\n                .properties(null)\n                .build();\n        for (int i = 0; i < N; i++) {\n            cache.putObject(i, i);\n        }\n        System.out.println(cache.getSize());\n    }\n```\n## 7.3 一级缓存的具体实现\n* 我们知道sqlSession是面向用户的API，真正执行的操作的是Executor，以查询为例，在Executor中的具体实现类BaseExecutror中的query方法中，我们知道，执行一个select语句的时候，会先从缓存中获取数据，如果缓存中没有，才会去数据库中查，并且从数据库中查到之后，还会放到缓存中。\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/S0vRu1xQJVhbEbyxcpb9KVAkWuZM1sKv1vgo1edueXU.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n* 这里涉及到两个属性，这两个属性存在BaseExecutor中\n* localCache：Mybatis一级缓存对象，用于缓存Mybatis的查询结果\n* localOutputParameterCache：Mybatis存储过程输出参数缓存，用于缓存存储过程调用结果\n* 这里主要探讨一下localCache，看一下一级缓存是怎么实现的。\n* 需要了解一下CacheKey的实现，如果两个查询的CacheKey一样，就认定为是同一个sql；\n* Cache的生成在CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql);\n* Cache的生成和Mapper的id，执行的sql的参数，以及mapper的的namespace有关；\n* key生成之后，就是关于一级缓存的使用了（用来做什么的），还是以查询为例，当查询的时候，会先从缓存中获取数据，如果缓存中没有，才会去数据库中查，并且从数据库中查到之后，还会放到缓存中。\n* 注意：如果一级缓存的范围设置为localCacheScope=STATEMENT，则每次查询操作完成后，都会清空缓存。\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/q6o8_UiLUVbOQ_y7gHV1CoFAp3qvH30l10IBUlz97x4.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n* 注意：在分布式环境下，务必将localCache设置为STATEMENT，避免其他节点执行SQL更新语句之后，本节点缓存得不到刷新而导致数据一致性的问题。\n\n## 7.4 二级缓存的具体实现\n### 7.4.1 二级缓存的两点常识\n了解二级缓存之前，先了解下面两个常识：\n\n* 二级缓存默认是关闭的，需要开启，具体开启方法，参考：[7.1 Mybatis 缓存（一级和二级）的使用](#7.1%20Mybatis%20%E7%BC%93%E5%AD%98%EF%BC%88%E4%B8%80%E7%BA%A7%E5%92%8C%E4%BA%8C%E7%BA%A7%EF%BC%89%E7%9A%84%E4%BD%BF%E7%94%A8)\n* 还是以查询为例，真正执行查询的是Executor，Executor有几种不同的实现（BaseExecutor不是实现，是抽象类）\n* SImpleExecutor\n* BatchExecutor\n* ReuseExecutor\n* CachingExecutor（比较特殊，用到了装饰者模式，在其他几种Executor基础上，增加了二级缓存功能）\n\n### 7.4.2 Mybatis是怎么创建CachingExecutor的呢\n如果我们配置二级缓存为开启状态，那么Mybatis是怎么创建CachingExecutor的呢？涉及到两部分：\n\n* CachingExecutor是怎么创建出来的？\n* Executor采用工厂模式创建，Configuration类提供了一个工厂方法，newExecutor\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/yaqhOGttroVfNjfXcV-oeb65dc82IbyaFQsnCV7jLok.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n* 可以看到CachingExecutor是对其他三个Executor的**装饰**；\n* 然后再继续分析CachingExecutor的具体实现；\n* 二级缓存开启之后，Mybatis怎么知道它开启了（CacheEnabled这个属性是什么时候初始化的）\n* 在XmlConfigBuilder中，parseConfiguration()中的settingsElement()方法中。\n\n```Plain Text\nconfiguration.setCacheEnabled(booleanValueOf(props.getProperty(\"cacheEnabled\"), true));\n```\n### 7.4.3 CachingEexcutor的具体实现\n* CachingEexcutor的具体实现\n* 这个类的代码比较少；看他的属性：\n\n```Plain Text\n//delegate是被装饰的Executor\nprivate final Executor delegate;\n//这个就比较重要了，用于管理所有的二级缓存对象\nprivate final TransactionalCacheManager tcm = new TransactionalCacheManager();\n```\n* TransactionalCacheManager的实现\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/z_EJdtrI8-abJABqiy2E2KUL-Y44WXeD6buYhouYfiE.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n* 接下来，以查询为例，看看二级缓存是怎么工作的\n\n### 7.4.4 以查询为例，看看二级缓存是怎么工作的\n* 执行查询\n\n```Plain Text\n  @Override\n  public <E> List<E> query(....) throws SQLException {\n    Cache cache = ms.getCache();\n    if (cache != null) {\n      flushCacheIfRequired(ms);\n      //如果当前sql开启了二级缓存（就是mapper.xml中的select|update|delete|insert标签中使用了useCache=true这个属性）\n      if (ms.isUseCache() && resultHandler == null) {\n        ensureNoOutParams(ms, boundSql);\n        //先从二级缓存中获取(使用二级缓存管理器TransactionalCacheManager进行获取)\n        @SuppressWarnings(\"unchecked\")\n        List<E> list = (List<E>) tcm.getObject(cache, key);\n        //如果获取不到，就用原来的Executor（也就是被装饰的Executor进行获取）：先从一级缓存拿，拿不到，再从数据库中拿\n        if (list == null) {\n          list = delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);\n          // issue #578 and #116\n          tcm.putObject(cache, key, list);\n        }\n        return list;\n      }\n    }\n    return delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);\n  }\n```\n* 执行更新语句\n\n```Plain Text\n  @Override\n  public int update(MappedStatement ms, Object parameterObject) throws SQLException {\n    flushCacheIfRequired(ms);\n    return delegate.update(ms, parameterObject);\n  }\n\n\n  private void flushCacheIfRequired(MappedStatement ms) {\n    Cache cache = ms.getCache();\n    //判断是否需要刷新缓存，（就是mapper.xml中的select|update|delete|insert标签中使用了flushCache=true这个属性）\n    if (cache != null && ms.isFlushCacheRequired()) {\n      tcm.clear(cache);\n    }\n  }\n```\n### 7.4.5 二级缓存Cache实例的创建\n在7.4.4中我们看到，不管是执行查询，还是执行更新，我们都要获取两个参数\n\n* 一个是Cache对象（从MappedStatement中获取的）：这是二级缓存对象-->对应的是mapper.xml中的cache标签\n* 一个就是CacheKey\n* 获取到这两个参数之后，才可以通过二级缓存管理器TransactionalCacheManager获取对应的缓存\n\nCacheKey的创建我们已经知道了，是和sqlID，sql的参数信息，nameSpace有关的。那么二级缓存对象实例Cache对象是什么时候创建的，并且是什么时候保存到MappedStatement中的呢？\n\n* 在\n\n```Plain Text\norg.apache.ibatis.builder.xml.XMLMapperBuilder#parse\n```\n这个方法中，会解析cache标签；\n\n* cache标签解析之后，会通过builderAssistant辅助工具类创建Cache实例（用到的是Cache的builder模式）；\n\n```Plain Text\npublic Cache useNewCache(Class<? extends Cache> typeClass,\n  Class<? extends Cache> evictionClass,\n  Long flushInterval,\n  Integer size,\n  boolean readWrite,\n  boolean blocking,\n  Properties props) {\nCache cache = new CacheBuilder(currentNamespace)\n    .implementation(valueOrDefault(typeClass, PerpetualCache.class))\n    .addDecorator(valueOrDefault(evictionClass, LruCache.class))\n    .clearInterval(flushInterval)\n    .size(size)\n    .readWrite(readWrite)\n    .blocking(blocking)\n    .properties(props)\n    .build();\nconfiguration.addCache(cache);\ncurrentCache = cache;\nreturn cache;\n}\n```\n* Cache对象创建完成之后，除了保存到configuration中，还会先**暂时保存**在当前这个辅助工具类中\n\n```Plain Text\nMapperBuilderAssistant\n```\n* 然后是什么时候，将cache对象保存到MappedStatement中的呢？\n\n```Plain Text\nprivate void configurationElement(XNode context) {\ntry {\n  ....\n  //解析cache标签，生成Cache对象，保存到configuration中，并且暂存在builderAssistant中\n  cacheElement(context.evalNode(\"cache\"));\n    .....    \n    //在这里，解析完select|insert|update|delete,会调用addMappedStatement这个方法\n  buildStatementFromContext(context.evalNodes(\"select|insert|update|delete\"));\n} catch (Exception e) {\n  throw ...\n}\n}\n```\n* 在上面的代码中，我们知道解析完select|insert|update|delete,会调用\n\n```Plain Text\norg.apache.ibatis.builder.MapperBuilderAssistant#addMappedStatement(...)\n```\n这个方法\n\n```Plain Text\npublic MappedStatement addMappedStatement(....) {\n  ....\n  MappedStatement.Builder statementBuilder = new MappedStatement.Builder(...)\n      .....\n      .useCache(valueOrDefault(useCache, isSelect))\n      //在这里，就将暂存在MapperBuilderAssistant中的cache对象赋值给了MappedStatement\n      .cache(currentCache);\n  return statement;\n}\n```\n## 7.5 问题解决？？？？？\n### 7.5.1 实际项目中，比如东南亚项目中，怎么控制的缓存呢？\n目前还没有找到答案，应该和mybatis-spring有关，因为代码使用的JavaConfig类进行配置的，不是使用xml，没找到具体的配置在哪里。\n\n# 8 Mybatis日志实现\n常见的日志框架\n\n* Log4j：是欧洲一个项目组开发的日志**实现**，现在是apache基金会的一个项目（1996）\n* Log4j2：是apache开发的日志**实现**，Log4j的升级版（2012）\n* Commons Logging（JCL）：Apache的，一套Java日志**接口**，本身只有一个简单的**实现**（2002下）\n* SLF4J：类似于Commons Logging，也是日志**接口**，本身没有实现（2006）\n* Logback：是日志组件的具体**实现**，属于SLF4J的阵营（2006）\n* JUL：JDK1.4之后java提供日志**实现**（2002上）\n\n# 9 动态SQL实现原理\n动态SQL，顾名思义，就是事先无法预知具体的条件，需要在运行的时候根据具体的情况动态的生成SQL语句。\n\n比如下面的SQL，就是一个动态的SQL\n\n```Plain Text\n    <select id=\"getUserByEntity\"  resultType=\"com.blog4java.mybatis.example.entity.UserEntity\">\n        select\n        <include refid=\"userAllField\"/>\n        from user\n        <where>\n            <if test=\"id != null\">\n                AND id = #{id}\n            </if>\n            <if test=\"name != null\">\n                AND name = #{name}\n            </if>\n            <if test=\"phone != null\">\n                AND phone = #{phone}\n            </if>\n        </where>\n    </select>\n```\n我们的入参可能是id，可能是name，可能是phone，也可能是这三个参数中任意两个或者三个的组合。\n\n当我们SQL的参数不固定的时候，生成SQL就需要根据参数动态来增加或者去除关键字，比如添加上WHERE，去掉多余的AND，OR等。\n\n## 9.1 动态SQL的使用\n主要是一些涉及的标签\n\n* <choose|when|otherwise>\n* <trim|set>\n\n## 9.2 SqlSource、BoundSql、LanguageDriver、SqlNode详解和它们之间的关系\n<img src=\"【mybatis】Mybatis从入门到入土.assets/4ApX3_MkEJJYJUp-QisORKINyjbWe6fjhB9iMRuO6V4.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n## 9.3 动态SQL解析过程\nSqlSource有四个实现，分别是ProviderSqlSource,DynamicSqlSource,RawSqlSource,StaticSqlSource; 其中StaticSqlSource比较特殊，它是用来描述通过ProviderSqlSource,SynamicSqlSource,RawSqlSource解析之后得到的静态sql资源。\n\n* 1、通过注解或者xml配置的sql怎么封装成SqlSource对象的？（SqlSource是怎么来的）\n* 是通过LanguageDriver将Sql配置解析成SqlSource的，什么时候解析的呢？解析的逻辑是什么呢？\n* 是在XmlMapperBuilder中解析select|update|delete|insert语句的时候，会获取到LanguageDriver对象，然后将XNode解析成SqlSource对象\n* 解析的逻辑就是，LanguageDriver通过创建XmlScriptBuilder对象，进行解析，具体逻辑后面说；\n* 2、SqlSource又是怎么解析最后转换成StaticSqlSource的？\n* 比如一个动态sql资源，解析之后肯定一个静态的sql资源；\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/pEIcEuUUVLmiZMER0tKvgXf714RH58QFpihw6DPdkwI.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n* 那么就是从DynamicSqlSource转换成StaticSqlSource的过程，是在：\n* DynamicSqlSource中的getBoundSql方法中会调用SqlSourceBuilder的parse方法\n* SqlSourceBuilder的parse方法会返回一个sqlSource（返回的这个就是StaticSqlSource）\n* LanguageDriver通过创建XmlScriptBuilder对象，进行解析，这是解析的具体逻辑（Xnode-->SqlSource的过程）\n\n比如一个动态sql：\n\n```Plain Text\n<select id=\"getUserByEntity\"  resultType=\"com.blog4java.mybatis.example.entity.UserEntity\">\n    select\n    <include refid=\"userAllField\"/>\n    from user\n    <where>\n        <if test=\"id != null\">\n            AND id = #{id}\n        </if>\n        <if test=\"name != null\">\n            AND name = #{name}\n        </if>\n        <if test=\"phone != null\">\n            AND phone = #{phone}\n        </if>\n    </where>\n</select>\n```\n它解析成的SqlSource的格式是下面这样的\n\n```Plain Text\n\n\nDynamicSqlSource\n    MixSqlNode\n        1、StaticTextSqlNode select\n        2、StaticTextSqlNode id,name.phone\n        3、StaticTextSqlNode from user\n        4、WhereSqlNode where\n            MixSqlNode\n                1、StaticTextSqlNode \"空\"\n                2、IfSqlNode if\n                    MixSqlNode\n                        StaticTextSqlNode AND id = #{id}\n                        test id != null\n                3、StaticTextSqlNode \"空\"\n                4、IfSqlNode if\n                    MixSqlNode\n                        StaticTextSqlNode AND name = #{name}\n                        test name != null\n                5、StaticTextSqlNode \"空\"\n                6、IfSqlNode if\n                    MixSqlNode\n                        StaticTextSqlNode AND phone = #{phone}\n                        test phone != null            \n                7、StaticTextSqlNode \"空\"\n         5、StaticTextSqlNode \"空\"\n```\n* 问题：SqlNode中有一个apply，用于解析sql，替换占位符，生成静态sql，这个方法什么时候调用的呢？\n* 在通过sqlSource的getBoundSql方法调用的时候，会调用apply方法\n* 那么sqlSource的getBoundSql什么时候调用的呢？\n* 我们知道Mybatis是通过MappedStatement保存sql各种信息的，那么理所当然，MappedStatement中有一个sqlSource的引用，是在XMLMapperBuilder中通过languageDriver解析获取到SqlSource之后，然后通过MapperBuilderAssistant.addMappedStatement方法，会将sqlSource存在MappedStatement中。\n* 在MappedStatement中也有一个getBoundSql方法，这个方法会在真正执行sql之前调用\n* sqlSource的getBoundSql会解析动态sql，但是我们知道sqlSource中存的不过是SqlNode，而sqlNode中可能包含很多AND，OR等关键字，那么这些关键字在apply方法的时候，是怎么被去掉的呢？\n* 以where为例子，我们现在获取到了select \\* from user 然后下一句是 AND phone = #{phone} 我们就需要把AND去掉，然后加上WHERE，拼接成这个样子 WHERE phone=#{phone}\n* 这一步，是在TrimSqlNode中做的，为什么是在这里呢，因为WhereSqlNode是TrimSqlNode的子类，在apply的时候，会创建FilteredDynamicContext这个类，然后会调用applyAll这个方法，在这个方法里，会判断当前的SqlSource的类型，是WHERE的话，就把AND去掉改成Where，同理TrimSqlNode的另一个子类SetSqlNode也是这样的，将And替换成Set。\n* 其他的替换，都是类似的\n\n## 9.4 分析\\${}和#{}的区别\n\\${}：当使用这个占位符的时候，Mybatis会使用TextSqlNode来进行描述，在解析的时候，会直接将参数进行替换，比如一个sql\n\n```Plain Text\nselect * from user where name = ${name}\n```\n,假设前端给的参数是\n\n```Plain Text\nString name = \"zhangsan\"\n```\n，那么解析之后得到的最终sql就是\n\n```Plain Text\nselect * from user where name = zhangsan\n```\n,这个sql是会报错的。当使用这个占位符的时候，参数应该这么传\n\n```Plain Text\nString name = \"'zhangsan'\"\n```\n,手动加上引号。\n\n#{}：略\n\n# 10 MyBatis插件实现原理\n## 10.1 Mybatis插件的使用\n* 首先有一个已经写好的Mybatis拦截器类，怎么写？可以参考10.4自定义拦截器插件\n* 对拦截器进行配置，在MyBatis的主配置文件中：\n\n```Plain Text\n  <plugins>\n        <plugin interceptor=\"com.blog4java.plugin.pager.PageInterceptor\">\n            <property name=\"databaseType\" value=\"hsqldb\"/>\n        </plugin>\n\n        <plugin interceptor=\"com.blog4java.plugin.slowsql.SlowSqlInterceptor\">\n            <property name=\"limitSecond\" value=\"0\"/>\n        </plugin>\n    </plugins>\n```\n* 这样，这个拦截器就生效了。\n\n## 10.2 Mybatis拦截器的拦截节点（可以拦截哪些方法）\n用户自定义的插件，只能针对MyBatis中的4个组件中的部分方法进行拦截\n\n| 组件             | 方法                   | 备注 |\n| ---------------- | ---------------------- | ---- |\n| Executor         | update                 |      |\n|                  | query                  |      |\n|                  | flushStatements        |      |\n|                  | commit                 |      |\n|                  | rollback               |      |\n|                  | getTransaction         |      |\n|                  | close                  |      |\n|                  | isClosed               |      |\n| ParameterHandler | getparameterObject     |      |\n|                  | setParameters          |      |\n| ResultSetHandler | handleResultSets       |      |\n|                  | handleOutputParameters |      |\n| StatementHandler | prepare                |      |\n|                  | parameterize           |      |\n|                  | batch                  |      |\n|                  | update                 |      |\n|                  | query                  |      |\n\n## 10.3 Mybatis插件的注册和拦截过程（原理：动态代理）\n分为两个部分，注册和执行拦截；\n\n注册\n\n* 在configuration中有一个` interceptorChain `属性，用来保存所有的通过标签配置的拦截器实例；\n\n* 在XMLConfigBuilder中构建configuration的时候，有一个方法专门解析的\n\n```Plain Text\nprivate void pluginElement(XNode parent) throws Exception {\n  if (parent != null) {\n    for (XNode child : parent.getChildren()) {\n      String interceptor = child.getStringAttribute(\"interceptor\");\n      Properties properties = child.getChildrenAsProperties();\n      //通过java的反射机制，创建拦截器实例\n      Interceptor interceptorInstance = (Interceptor) resolveClass(interceptor).getDeclaredConstructor().newInstance();\n      interceptorInstance.setProperties(properties);\n      configuration.addInterceptor(interceptorInstance);\n    }\n  }\n}\n```\n解析完成之后，将生成的拦截器实例，保存在 `interceptorChain `中；执行拦截.\n\n那么拦截逻辑是怎么执行的呢？\n\n之前说过，configuration有三大作用，其中一个作用就是：作为Executor, ParameterHandler, ResultSetHandler, StatementHandler组件的工厂，便于创建这些组件的实例， \\[点我查看configuration的三个作用\\](#Configuration)\n\n统一使用工厂创建者四个类的实例，这样做的好处是什么呢？\n\n* 将创建的类的实例统一起来，可以根据用于配置参数的不同，创建不同的实例：比如用户使用了缓存，就会创建CacheExecutor一样；\n* 可以在工厂方法中，执行拦截逻辑；\n\n```java\n\nstatementHandler = (StatementHandler) interceptorChain.pluginAll(statementHandler);\nresultSetHandler = (ResultSetHandler) interceptorChain.pluginAll(resultSetHandler);\nparameterHandler = (ParameterHandler) interceptorChain.pluginAll(parameterHandler);\nexecutor = (Executor) interceptorChain.pluginAll(executor);\n\n```\n拦截逻辑都是在pluginAll中执行的。至于pluginAll到底做了什么，我们待会在看。\n\n以Executor组件为例（其他3个类同理），看一下Configuration的newExecutor是怎么创建代理对象的。\n\n!\\[image-20210223162433312\\](https://youdao-note-【mybatis】Mybatis从入门到入土.assets.oss-cn-hangzhou.aliyuncs.com/2021-02/20210223162434.png)\n\n当我们系统中配置了Plugin的时候，生成的Executor才是动态代理对象，否则生成的就是Executor对象；因为\n\n```Plain Text\npublic Object pluginAll(Object target) {\n  //没有拦截器配置，就不会执行plugin，也就不会生成动态代理对象\n  for (Interceptor interceptor : interceptors) {\n    target = interceptor.plugin(target);\n  }\n  return target;\n}\n```\n以sqlSession的selectOne为例，被代理的Executor执行query方法的调用链路图；\n\n* 被代理的Executor对象，会先执行到Plugin类的invoke方法中\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/2KSB9I7XUzXCy41tQEljCCAlPax8LvCgTdoSUdio7QQ.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n## 10.4 自定义Mybatis拦截器\n拦截器代码\n\n```Plain Text\n\n/**\n * @author : zhuansun\n * @date : 2021-02-23 16:48\n **/\n@Intercepts( {\n    //指定拦截Executor的query方法，因为query方法有很多个，我们要通过args执行拦截入参是这些的query方法\n    //<E> List<E> query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException;\n    @Signature(type = Executor.class, method = \"query\", args = {MappedStatement.class,\n        Object.class, RowBounds.class, ResultHandler.class})\n})\npublic class ZspcSqlInterceptor implements Interceptor {\n\n    /**\n     * 这个参数可以通过<plugin>标签中的<properties>设值\n     */\n    private String name;\n\n    @Override\n    public Object intercept(Invocation invocation) throws Throwable {\n\n        System.out.println(\"111111\"+name);\n\n        return invocation.proceed();\n    }\n\n    @Override\n    public Object plugin(Object target) {\n        return Plugin.wrap(target, this);\n    }\n\n    @Override\n    public void setProperties(Properties properties) {\n        this.name = (String) properties.get(\"name\");\n    }\n}\n\n```\nMybatis配置拦截器\n\n```Plain Text\n    <plugins>\n\n        <plugin interceptor=\"com.blog4java.plugin.zspc.ZspcSqlInterceptor\">\n            <property name=\"name\" value=\"我就试试\"/>\n        </plugin>\n\n    </plugins>\n```\n调用\n\n```Plain Text\n\n    @Test\n    public void testPageInterceptor() {\n        UserQuery query = new UserQuery();\n        query.setPageSize(5);\n        query.setFull(true);\n        List<UserEntity> users = userMapper.getUserPageable(query);\n        System.out.println(\"总数据量：\" + query.getTotalCount() + \",总页数：\"\n                + query.getTotalPage()+ \"，当前查询数据：\" + JSON.toJSONString(users));\n    }\n\n```\n输出\n\n```Plain Text\n\n111111我就试试\n总数据量：0,总页数：0，当前查询数据：[{\"id\":0,\"name\":\"User1\",\"password\":\"test\",\"phone\":\"18700001111\"},{\"id\":1,\"name\":\"User2\",\"password\":\"test\",\"phone\":\"18700001111\"},{\"id\":2,\"name\":\"User3\",\"password\":\"test\",\"phone\":\"18700001111\"},{\"id\":3,\"name\":\"User4\",\"password\":\"test\",\"phone\":\"18700001111\"},{\"id\":4,\"name\":\"User5\",\"password\":\"test\",\"phone\":\"18700001111\"},{\"id\":5,\"name\":\"User6\",\"password\":\"test\",\"phone\":\"18700001111\"},{\"id\":6,\"name\":\"User7\",\"password\":\"test\",\"phone\":\"18700001111\"},{\"id\":7,\"name\":\"User8\",\"password\":\"test\",\"phone\":\"18700001111\"},{\"id\":8,\"name\":\"User9\",\"password\":\"test\",\"phone\":\"18700001111\"},{\"id\":9,\"name\":\"User10\",\"password\":\"test\",\"phone\":\"18700001111\"},{\"id\":10,\"name\":\"User11\",\"password\":\"test\",\"phone\":\"18700001111\"},{\"id\":11,\"name\":\"User12\",\"password\":\"test\",\"phone\":\"18700001111\"},{\"id\":12,\"name\":\"User13\",\"password\":\"test\",\"phone\":\"18700001111\"},{\"id\":13,\"name\":\"User14\",\"password\":\"test\",\"phone\":\"18700001111\"}]\nDisconnected from the target VM, address: '127.0.0.1:60862', transport: 'socket'\n\n```\n# 11 MyBatis级联映射与懒加载\n## 11.1 级联映射的介绍与使用\n* 知道什么是级联映射\n* 级联映射的配置（一对多配置 collection，一对一配置 association）\n* discriminator 鉴别器\n\n有一个用户表，有一个订单表，订单表存了用户表的id\n\n比如用户表：id,name(用户名),phone(手机号)\n\n比如订单表：id,userId(用户id),orderNo(订单号),amount(订单金额)\n\n现在有一个需求，我们想查出一个用户，同时查出来这个用户下的所有订单；当然我们有几个方法：\n\n* 先查出user，再根据user查出来订单，最后封装返回值；\n* 使用MyBatis的级联映射\n\n```Plain Text\n//使用级联映射，resultMap指定为detailMap\n<select id=\"getUserByIdFull\" resultMap=\"detailMap\">\n  select * from user where id = #{userId}\n</select>\n//这是detialMap的定义，type是User类型的，里面包含一个collection,指定property为User类的orders属性\n//ofType标识orders属性的类是Order类\n//select标识要执行OrderMapper中的listOrderByUserId这个sql查出来的数据填充orders这个属性\n<resultMap id=\"detailMap\" type=\"com.blog4java.mybatis.example.entity.User\">\n   <collection property=\"orders\" ofType=\"com.blog4java.mybatis.example.entity.Order\"\n                  select=\"com.blog4java.mybatis.example.mapper.OrderMapper.listOrdersByUserId\"\n                  javaType=\"java.util.ArrayList\"\n                  column=\"id\">\n   </collection>\n</resultMap>\n```\n上面是比较简单的级联映射的配置；当然难得也有，比如：\n\n一对多级联映射；\n\n一对一级联映射；\n\n如果我们的需求升级了，用户性别是女的才查订单信息，否则就不查询订单信息；这个时候需要使用到Discriminator。这个的作用类似java中的switch\n\n```Plain Text\n    <resultMap id=\"detailMapForDiscriminator\" type=\"com.blog4java.mybatis.example.entity.User\">\n        <discriminator javaType=\"String\" column=\"gender\">\n            <case value=\"female\" resultType=\"com.blog4java.mybatis.example.entity.User\">\n                <collection property=\"orders\" ofType=\"com.blog4java.mybatis.example.entity.Order\"\n                            select=\"com.blog4java.mybatis.example.mapper.OrderMapper.listOrdersByUserId\"\n                            javaType=\"java.util.ArrayList\"\n                            column=\"id\">\n                </collection>\n            </case>\n        </discriminator>\n    </resultMap>\n```\n## 11.2 懒加载机制的介绍与使用\n**注意：懒加载本身就是针对级联查询的，对于普通查询，没有懒加载一说’**\n\n有这样一个需求，当我们查询用户的时候，如果每次都带出订单信息，但是并不是所有使用到的地方都需要使用订单信息，这样的话，每次都带出，就多了一步这么无用的查询。\n\n那么我们可不可以在调用用户的getOrders方法的时候，就是说当我们确定要拿用户的订单信息的时候，再去数据库里面查询出来。这就叫做懒加载。\n\nMyBatis提供了懒加载机制：\n\n* 在MyBatis的主配置文件中，提供了lazaLoadingEnabled和aggressiveLazyLoading两个参数用于控制是否开启懒加载；\n* lazaLoadingEnabled：是否开启懒加载 true开启\n* aggressiveLazyLoading：控制ResultMap默认的加载行为，false表示懒加载，true表示积极加载\n* 标签提供了一个fetchType属性，为\n\n```Plain Text\nlazy\n```\n表示懒加载，为\n\n```Plain Text\neager\n```\n表示积极加载\n\n```Plain Text\n    <settings>\n       ...\n        <!-- 打开延迟加载的开关 -->\n        <setting name=\"lazyLoadingEnabled\" value=\"true\" />\n        <!-- 将积极加载改为懒加载即按需加载 -->\n        <setting name=\"aggressiveLazyLoading\" value=\"false\" />\n        <!-- toString,hashCode等方法不触发懒加载 -->\n        <setting name=\"lazyLoadTriggerMethods\" value=\"\"/>\n        ...\n    </settings>\n```\n## 11.3 级联查询和懒加载的原理\n* 首先是标签转换成ResultMap对象的逻辑，也是在XMLCOnfigBuilder中解析的，看看代码就知道了。\n* 其次是级联查询的实现逻辑：\n* 知道一个sql在查询完成之后，会使用ResultSetHandler对象的handleResultSets方法处理结果集；\n* handleResultSets方法简化了JDBC对ResultSet对象的操作，会在这里将级联查询的结果进行处理与赋值；\n* 再详细的就不说了，注意一点：级联查询的第二个sql是什么时候执行的呢？可以参考[点我跳转查看](#11.4%20%E9%97%AE%E9%A2%98)\n* 懒加载的实现逻辑\n* 在处理级联查询的结果集的时候，也就是在handleResultSets方法中；\n* 在handleResultSets方法中调用handleRowValues；\n* 然后根据是否是否有嵌套的ResultMap，调用handleRowValuesForNestedResultMap或者handleRowValuesForSimpleResultMap，这都不重要；\n* 重要的是最终会调用到getRowValue方法；\n* 然后会调用到createResultObject这个方法，在这个方法内部，如果开启了懒加载，则调用createProxy创建代理对象（CgLib代理或者javassist代理，只有这两种）；\n* 也就是说，对于开启了懒加载的查询，返回的不是User的实例，而是User实例的代理对象；\n* 当我们执行了User实例代理对象的get方法，就会执行代理类的拦截逻辑；\n* 在拦截逻辑中，会调用到lazyLoader的load方法，最后调用到LoadPair的load方法；\n* 然后会创建ResultLoader对象，最后调用到\n\n```Plain Text\nresultLoader.loadResult()\n```\n，至此完成；\n\n* 懒加载最后一步和正常不开启懒加载的的查询，其实调用的是同一个方法。\n\n## 11.4 问题\n* Mybatis的级联查询，是执行一次sql，还是执行多次sql？\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/WU1bC7GCTaqfR93WF6GZyxDzDGb_U3DIwPnscF3awwc.png\" alt=\"image\" style=\"zoom:80%;\" />\n\n* 从测试的结果来看，是执行了多次sql，那么第二次的sql是在什么时候执行的呢？\n\n他娘的，要骂人了。在跑测试用例的时候，一直很好奇，明天控制台打印出了两个sql，为什么断点只能看到一个。而且IDEA还会有这个提示：\n\n<img src=\"【mybatis】Mybatis从入门到入土.assets/rQzgBZ0o9gq2FaPOBMqu3WbmwTx1632pZxk5yAM1zS4.png\" alt=\"image\"  />\n\n一开始没有在意，还以为是自己断点的位置不对，最后针对这个提示，百度了一下，了解到：\n\n* IDEA对于在toString中调用的方法，是不会走到断点的。具体了解的可以自己百度，很多的。\n* 按照百度的设置，我们把相关设置配置好，最后才发现：\n* 我说为什么断点只能进来一次，原来是因为测试用例默认开启了懒加载；\n* 这就导致查询用户的时候的断点可以进来，在查询order的时候，因为是在toString中调用的，IDEA直接跳过了。\n* 修改了相关配置后，第二个sql就可以断点进来啦。\n* 最后我们测试关闭懒加载模式，看看第二个sql是什么时候调用的呢？\n* 实在第一个sql执行完的之后，处理第一个sql的结果集的时候，在getRowValue方法中，调用applyPropertyMappings方法，然后调用getPropertyMappingValue方法，然后调用getNestedQueryMappingValue方法，然后调用\n\n```Plain Text\nvalue = resultLoader.loadResult()\n```\n方法，在这个方法内部，执行了第二个sql的查询。\n\n# 12 MyBatis与Spring整合\n**简介**\n\n* 我们知道MyBatis是使用sqlSession操作数据库的；\n* 我们知道sqlSession是通过sqlSessionFactory.openSession获取的；\n* 在mybatis-spring中，我们使用spring提供的\n\n```Plain Text\nSqlSessionFactoryBean\n```\n来创建sqlSessionFactory对象\n\n* 在mybatis中，使用sqlSessionFactory.openSession每次都会获取一个新的sqlSession对象，然后调用sqlSession的相关方法就可以与数据库交互了。\n* 在mybatis-spring中，spring提供了sqlSessionTemplate用于完成数据库交互，它是单例的。\n\n**原理**\n\n* spring容器在启动的时候，会将bean的配置信息转换成BeanDefinition对象，BeanDefinition是一个接口，有很多实现类，用来描述不同方式配置的Bean信息；\n* BeanDefinition有一个容器，叫做BeanDefinitionRegistry，所有的Bean信息都会注册到里面；另外，spring对BeanDefinitionRegistry提供了扩展机制，允许用户在spring框架启动时，动态的注册bean信息；\n* 现在我们有了Bean信息，有个Bean信息的容器，spring框架在启动的时候，会根据bean信息创建生成bean实例，并保存在BeanFactory中，他们都是单例的。\n* BeanFactoryPostProcessor\n* ImportBeanDefinitionRegister\n* BeanPostProcessor\n* ClassPathBeanDefinitionScanner\n* FactoryBean：可以理解为生成Bean的，当我们获取FactoryBean的时候，其实获取的是FactoryBean对象getObject方法返回的实例，比如配置SqlSessionFactoryBean，其实获取的是SqlSessionFactory","tags":["mybatis"],"categories":["JAVA","SSM三大框架"]},{"title":"mac更新系统后git无法使用","url":"/note/JAVA/GIT/mac更新系统后git无法使用/","content":"\n\n\n# mac更新系统后git无法使用\n\n今天更新了mac系统，然后就踩了这个坑。\n\n 启动Idea 左下角角提示：\n\n```sh\ncan't start git: /usr/bin/git \nprobably the path to git executable is not valid . `fix it`.12\n```\n\n点击fix it后 点击最上边路径地址后边的test提示：\n\n```sh\nerrors while executing git -- version. exitCode=1 \nerrors: xcrun: error : invalid active developer path(/library/developer/commandlinetools),missing xcrun at:\n/library/developer/commandlinetools.usr/bin/xcrun123\n```\n\n找了一圈。。很多开发者都遇到过这问题。据说苹果每个版本的更新都会有这样的问题，原因是每次安装新的更新后，Xcode都被卸载了。。。。不扯别的说解决方案。\n\n\n\n## 方法一\n\n 通过终端重新安装的Xcode命令行工具使用（其实这里安装的是Command Line Tools，Command Line Tools是在Xcode中的一款工具）\n\n```\nxcode-select --install1\n```\n\n## 方法二（本人没试，感兴趣的小伙伴可以尝试下）\n\n如果不安装Xcode可以重安装git(下载地址)然后在Idea中切换到路径/usr/local/git/bin/git。\n\n<img src=\"mac更新系统后git无法使用.assets/image-20220902164505181.png\" alt=\"image-20220902164505181\" style=\"zoom:80%;\" />\n\n## 参考\n\n- https://blog.csdn.net/qq_23089525/article/details/52789005","tags":["git","mac"],"categories":["JAVA","GIT"]},{"title":"git批量删除分支","url":"/note/JAVA/GIT/git批量删除分支/","content":"\n\n\n# git批量删除分支\n\n\n\n批量删除除当前分支外的所有本地分支：\n\n> git branch | xargs git branch -d\n\n<img src=\"git批量删除分支.assets/image-20220913190900166.png\" alt=\"image-20220913190900166\" style=\"zoom:80%;\" />\n\n批量删除指定分支，比如删除所有，分支名字包含zspc的分支\n\n> git branch | grep 'zspc' | xargs git branch -d\n\n<img src=\"git批量删除分支.assets/image-20220913190918209.png\" alt=\"image-20220913190918209\" style=\"zoom:80%;\" />\n\n删除的过程中，如果被删除的分支没有merge，会有提示，git默认不会删除，如果我们不需要，可以使用-D强制删除","tags":["git","删分支"],"categories":["JAVA","GIT"]},{"title":"git清空stash及其他操作","url":"/note/JAVA/GIT/git清空stash及其他操作/","content":"\n\n\n# git清空stash及其他操作\n\n\n\n将当前变更保存到stash\n\n> git stash\n\n将当前变更保存到stash，自定义说明信息\n\n> git stash save '这里是自定义说明信息'\n\n列出所有的stash\n\n> git stash list\n\n弹出stash，也就是将stash中的内容应用到当前分支，并删除这个stash\n\n> git stash pop\n\n弹出指定的stash，也就是将stash中的内容应用到当前分支，并删除这个stash，后面的@{0}可以指定\n\n> git stash pop stash@{0}\n\n删除stash\n\n> git stash drop\n\n删除指定的stash,后面的@{0}可以指定\n\n> git stash drop stash@{0}\n\n清空stash\n\n> git stash clear\n\n展示stash中的内容\n\n> git show stash@{n}","tags":["git","git stash"],"categories":["JAVA","GIT"]},{"title":"gitkraken基本操作","url":"/note/JAVA/GIT/gitkraken基本操作/","content":"\n# gitkraken基本操作基本操作\n\n\n\n## 参考文章\n\n- https://www.cnblogs.com/thousfeet/p/7840932.html\n\n## 设置页面\n\n<img src=\"gitkraken基本操作.assets/image-20220902161937456.png\" alt=\"image-20220902161937456\" style=\"zoom:80%;\" />\n\n## 主页面\n\n<img src=\"gitkraken基本操作.assets/image-20220902161950613.png\" alt=\"image-20220902161950613\" style=\"zoom: 50%;\" />\n\n## 开发完之后，怎么提交代码\n\n<img src=\"gitkraken基本操作.assets/image-20220902162002852.png\" alt=\"image-20220902162002852\" style=\"zoom:50%;\" />\n\n<img src=\"gitkraken基本操作.assets/image-20220902162008472.png\" alt=\"image-20220902162008472\" style=\"zoom:50%;\" />\n\n<img src=\"gitkraken基本操作.assets/image-20220902162013743.png\" alt=\"image-20220902162013743\" style=\"zoom:50%;\" />\n\n\n\n## 分支合并(合并一个分支到另一个分支)\n\n<img src=\"gitkraken基本操作.assets/image-20220902162027350.png\" alt=\"image-20220902162027350\" style=\"zoom:50%;\" />\n\n\n\n## 冲突解决\n\n<img src=\"gitkraken基本操作.assets/image-20220902162038808.png\" alt=\"image-20220902162038808\" style=\"zoom:50%;\" />\n\n<img src=\"gitkraken基本操作.assets/image-20220902162044157.png\" alt=\"image-20220902162044157\" style=\"zoom:50%;\" />\n\n<img src=\"gitkraken基本操作.assets/image-20220902162050142.png\" alt=\"image-20220902162050142\" style=\"zoom:50%;\" />\n\n<img src=\"gitkraken基本操作.assets/image-20220902162055281.png\" alt=\"image-20220902162055281\" style=\"zoom:50%;\" />\n\n\n\n## 怎么清除保存的用户名和密码\n\n<img src=\"gitkraken基本操作.assets/image-20220902162119698.png\" alt=\"image-20220902162119698\" style=\"zoom: 38%;\" />","tags":["git","gitkrake"],"categories":["JAVA","GIT"]},{"title":"ggitignore规则不生效的解决办法","url":"/note/JAVA/GIT/gitignore规则不生效的解决办法/","content":"\n\n\n# gitignore规则不生效的解决办法\n\n\n\n把某些目录或文件加入忽略规则，按照上述方法定义后发现并未生效，原因是.gitignore只能忽略那些原来没有被追踪的文件，如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。那么解决方法就是先把本地缓存删除（改变成未被追踪状态），然后再提交：\n\n```sh\ngit rm -r --cached .\n\ngit add .\n\ngit commit -m 'update .gitignore'\n```\n\n\n\n<img src=\"gitignore规则不生效的解决办法.assets/image-20220902162247958.png\" alt=\"image-20220902162247958\" style=\"zoom:80%;\" />\n\n\n\n<img src=\"gitignore规则不生效的解决办法.assets/image-20220902162255835.png\" alt=\"image-20220902162255835\" style=\"zoom:80%;\" />","tags":["git","gitignore"],"categories":["JAVA","GIT"]},{"title":"DDD踩坑记录","url":"/note/JAVA/DDD/DDD踩坑记录/","content":"\n\n\n# DDD踩坑记录\n\n\n\n\n\n\n\n\n## 关于gateway的定位\n这么一个场景：如果我们有多个领域，比如领域A和领域B，这两个领域都需要调用 同一个远程服务gateway 获取一些数据；那么这个gateway怎么处理？\n\n* 是每一个领域下有一个单独的gateway？\n* 是把gateway抽离出来，由多个领域都可以进行调用（那么这个gateway所在的领域是个什么定位呢）？\n\n个人理解：\n\n正常来说，按照COLA的架构，是每一个domain（每一个领域）有自己单独的gateway；但是从实际的开发角度来看，每一个领域有自己的gateway比较难以维护，因为如果多个领域调用同一个gateway，相当于这个gateway我要复制一份。\n\n所以，综上考虑：还是选择第二种方案；就是抽离一个公共的model层，所有的gateway放在这里，有点类似于通用子域的概念；所有的领域都可以自由的调用model中的gateway；\n\n（但是，从DDD的角度来说，还是每一个领域都有一个单独的gateway更合理一些）\n\n\n\n\n\n\n\n## 查询条件太多，领域层入参怎么设计\n这么一个场景：假设我们有申请单域，用户需求需要根据：申请时间起止+申请人+审批人+审批状态+···完成时间起止等很多的查询条件，查询出申请单列表；那么这个时候domainSerview的入参就得是好多好多。\n\n纠结的点在于：如果我们这些查询条件封装成一个SO，传入domainSerview，但是这个SO和我的申请单域没有任何的关系；如果我把这些参数打平传进domainSerview，又太多了；怎么办呢？\n\n结论先行：\n\n* 封装成SO传入领域层；\n\n为什么这么做？ 因为我们考虑到代码的可读性，不建议方法有大量的入参，那么至于SO和我的领域有没有关系，其实我觉得不重要，因为DDD的基本概念是领域建模，唯一的要求就是：领域内是干净的。不要和其他的领域耦合在一起。基于这个点考虑，领域内部创建一些无意义的值对象（SO可以看做是值对象），我认为是可以的，因为这些值对象方便了领域内的处理。同时领域对外暴露的命令：查询命令；\n\n另外，如果有一些简单的查询，比如查询某个申请人的所有申请单，我们可以针对这个查询，在领域内单独抽出一个方法；\n\n* searchBySO\n* searchByApplier\n\n\n\n\n\n## 一个领域怎么引用另一个领域？\n比如说：domainService的入参可以是其他领域的聚合吗？在domainService内部可以调用另一个领域的服务呢？ \n\n* 不可以；\n* 不可以；\n\n如果一个领域A需要用到另一个领域B的对象的话，就把领域B的对象转成领域A中的对象；\n\n```java\n//根据userid和权限id获取用户权限\nUser user = userDomainService.getUserPointPrivilegeChildrens(cmd.getUserId(), cmd.getPermissionId());\n//将user权限转成指标维度\nDimension dimension = MetricAppFactory.userPrivilegeToDimension(user);\n//调用指标域（因为User是用户域的，不能直接传给指标域，所以做一层转黄）\nMetric metrics = metricDomainService.inTimeOrder(dimension);\n```\n这个转换，需要是有意义的；比如上面的代码，将用户的权限转成了指标维度，这个转换并不是随便转的，因为每一个指标都会有一个维度的概念。\n\n```java\n@Data\npublic class Metric {\n\n    /**\n     * 指标维度\n     */\n    private Dimension dimension;\n\n    /**\n     * 首页指标:  订单量，入网量，签收量，，等等，所以是一个list\n     */\n    private MetricIndex metricIndex;\n\n    /**\n     *  实时数据： 订单量，入网量，等指标vo\n     */\n    private MetricInTime inTime;\n\n    /**\n     * 历史数据：\n     */\n    private MetricHistory history;\n\n}\n```\n那么在domainService内部，可以调用另一个domainService吗？（换言之，一个领域服务怎么调用另一个领域服务）\n\n<img src=\"DDD踩坑记录.assets/image-20220902161603846.png\" alt=\"image-20220902161603846\" style=\"zoom:80%;\" />\n\n\n\n在cola架构中，domain层中是有防腐层的概念的，\n\n* 防腐层的包一般命名为acl\n* 防腐层的类一般命名为XXXGateWay（通过依赖倒置原则，实现在基础设施层）\n\n通过上面的概念，就可以理解，在一个领域中，一个领域服务调用另一个领域服务必须通过防腐层GateWay来调用，那么随之而来的几个问题：\n\n* 1、如果需要调用的领域服务是本项目中其他领域的服务，也需要gateway吗？\n   * 是的。也需要，把本项目中其他领域的服务当做远程服务一样来调用；\n* 2、如果本项目中有两个领域服务，需要调用远程同一个服务，gateway是在这两个领域服务下，每个领域服务中都创建一个gateway吗？即使这两个gateway代码完全一样？\n   * 是的，每个领域服务都需要创建GateWay，即使两个代码完全一样。\n\n\n\n\n\n## DO转PO的时候，是否需要填充所有的参数？\n比如说，数据库中只存了站点id，但是实际的业务逻辑中，需要用到站点类型，在DO中存的是Site这个实体，那么在从PO到DO的过程中，也就是siteId到Site的过程中，是否需要通过SiteId获取Site的所有数据，然后放在DO中，还是说，只需要把siteId放在Site中，就可以了。\n答案：需要获取所有的。1、获取PO对象；2、通过Factory一次性填充DO所有需要的属性。\n\n\n\n\n\n\n\n## 领域服务(Domain Service)的定位\n>   结论先行：领域服务的入参不必限制为必须是当前领域的聚合根；可以是多种多样的。\n\n之前讨论领域服务是领域层很重要的概念，主要用来写业务逻辑，入参只能使用聚合根；目前来看，这种方式是错误的。比如下面的场景：在进出港业务中：是根据各种各样的扫描（分拨发件，到件，派件，签收，退件，转寄等），采集进港属性（应派信息，签收信息，分拨信息等）；那么就有下面两种方式：\n\n* 目前的做法：将扫描放在进港域的聚合根(Inboard)中，进港的聚合根中包含了大量的扫描信息；\n\n这样做的目的是为了保证进港的领域服务(InboardDomainService)的入参只能是进港聚合根(Inboard)，但是为了满足这个目的，缺引入了很多的缺点：\n\n* 聚合根中引入了大量的无用扫描：为什么说是无用，因为进港聚合中只需要应派信息，签收信息，分拨信息，可是现在却不得不保存大量的扫描（退件，转寄，入库入柜等）；\n* 各种各样的扫描进入到领域服务(inboardDomainServie)之后，还要再拆分，根据不同的扫描类型，进行不同的业务逻辑处理；代码庞大难以维护；\n* 瞻仰一下目前的代码逻辑：这仅仅只是一部分而已\n\n```java\n    /**\n     * 设置应派信息：\n     * - 应派时间\n     * - 应派站点\n     *\n     * @param inboard    : 前端传过来的\n     * @param preInboard : 数据库中的\n     */\n    private void fillDispInfo(Inboard inboard, Inboard preInboard) {\n\n        DispInfo dispInfo = null;\n        SignInfo signInfo = null;\n        SendInfo sendInfo = null;\n\n        log.info(\"fillDispInfo inboard:{}, preInboard:{}\", JSONUtil.toJsonStr(inboard),\n            JSONUtil.toJsonStr(preInboard));\n\n        if (Objects.nonNull(inboard.getSignInfo())) {\n            if (!preInboard.hasDispInfo() || DateUtil.dataAfter(inboard.getSignInfo().getScanTime(),\n                preInboard.getDispInfo().getScanTime())) {\n                //取签收站点作为应派站点\n                dispInfo = (!preInboard.hasDispInfo()) ? new DispInfo() : preInboard.getDispInfo();\n                //省略...设置了应派站点，应派站点的扫描事件，应派站点的结算站点 应派日期后面计算\n\n            }\n\n            if (!preInboard.hasSignInfo() || DateUtil.dataAfter(\n                preInboard.getSignInfo().getScanTime(), inboard.getSignInfo().getScanTime())) {\n                //取最早的签收， 最晚签收时间后面在计算, 签收时间是取最早的。\n                signInfo = inboard.getSignInfo();\n            }\n\n        } else if (BooleanUtils.isTrue(preInboard.isHasCenterScan())) {//之前过分拨\n            if (Objects.nonNull(inboard.getSendInfo())) {//当前扫描是发件扫描\n\n                if (inboard.getSendInfo().isEndCenterSend()) {\n                    //分拨发件，并且下一站是站点， 主要是排除分拨发分拨的情况\n                    if (!preInboard.hasDispInfo() || DateUtil.dataAfter(\n                        inboard.getSendInfo().getScanTime(),\n                        preInboard.getDispInfo().getScanTime())) {\n                        //省略...设置应派信息：发件扫描，下一站是站点，就取下一站\n                      \n                    }\n\n                    if (!preInboard.hasSendInfo() \n                        || DateUtil.dataAfter(inboard.getSendInfo().getScanTime()\n                            ,preInboard.getSendInfo().getCenterSendTime())) {\n                        //分拨发件信息，取当前最新的\n                        sendInfo = (!preInboard.hasSendInfo()) ?\n                            new SendInfo() :\n                            inboard.getSendInfo();\n\n                        //分拨发站点：采集分拨\n                        if (Objects.nonNull(inboard.getSendInfo().getCenterSite())) {\n                            //发件站点是分拨\n                        } else {\n                            //发件站点是一级站点\n                            if (Objects.nonNull(preInboard.getSendInfo())\n                                && Objects.nonNull(preInboard.getSendInfo().getCenterSite())) {\n                                //处理一级发二级的情况，分拨还是原来的。\n                            }\n                        }\n                    }\n                }\n\n            } else {//过了分拨，但不是发件扫描，可能是到件，派件，转寄,退件等等,取当前扫描站点\n                if (inboard.hasArrInfo() && inboard.isHasCenterScan()) {\n                    //如果当前是分拨到件扫描，取消应派站点的采集\n                    if (preInboard.hasDispInfo()) {\n                       //....\n                    }\n                } else if (inboard.hasReturnInfo() && BooleanUtils.isTrue(\n                    inboard.getReturnInfo().isHasReturn())) {\n                    //退件也清空\n                    if (preInboard.hasDispInfo()) {\n                        //....\n                    }\n                } else if (!preInboard.hasDispInfo() || (\n                    Objects.nonNull(inboard.getCurrentScanInfo()) && DateUtil.dataAfter(\n                        inboard.getCurrentScanInfo().getScanTime(),\n                        preInboard.getDispInfo().getScanTime()))) {\n                    //过了分拨，但不是发件扫描，可能是到件，派件，转寄等等,取当前扫描站点\n                    //....\n                }\n            }\n        } else if (BooleanUtils.isTrue(inboard.isHasCenterScan())) {//当前过分拨\n            preInboard.setHasCenterScan(true);\n            if (Objects.nonNull(inboard.getSendInfo()) && inboard.getSendInfo()\n                .isEndCenterSend()) {//当前扫描是分拨发件扫描\n                if (!preInboard.hasSendInfo() || DateUtil.dataAfter(\n                    inboard.getSendInfo().getScanTime(),\n                    preInboard.getSendInfo().getCenterSendTime())) {\n                    //分拨发件信息，取当前最新的\n                    //....\n                    //这里不会有丰网一级发二级的情况需要处理\n                    //因为当前过分拨（不会是丰网一级，只有丰网分拨，和大网网点才是当前过分拨）\n                }\n                if (!preInboard.hasDispInfo() || DateUtil.dataAfter(\n                    inboard.getSendInfo().getScanTime(), preInboard.getDispInfo().getScanTime())) {\n                    //派件信息：取下一站\n                    //....\n                }\n            }\n        }\n\n        if (Objects.nonNull(dispInfo)) {\n            //填充丰网应派\n            dispInfo.setFwDisp(dispInfo.getScanSite().isFwSite());\n            preInboard.setDispInfo(dispInfo);\n        }\n        if (Objects.nonNull(sendInfo)) {\n            preInboard.setSendInfo(sendInfo);\n        }\n        if (Objects.nonNull(signInfo)) {\n            preInboard.setSignInfo(signInfo);\n        }\n\n        log.info(\"计算应派日期前的inboard code {} inboard {}\",inboard.getCode(), JSONUtil.toJsonStr(inboard));\n\n        //计算应派日期+最晚签收时间\n        if (Objects.nonNull(preInboard.getSendInfo()) && Objects.nonNull(preInboard.getDispInfo())) {\n               //这里的逻辑都是简化后的\n            String centerSiteCode = preInboard.getSendInfo().getCenterSite().getSfCode();\n            String dispSiteCode = preInboard.getDispInfo().getScanSite().getCode();\n            Date centerSendTime = preInboard.getSendInfo().getCenterSendTime();\n           \n            if (StringUtils.isNoneBlank(centerSiteCode, dispSiteCode) && Objects.nonNull(centerSendTime)) {\n\n                List<SiteShixiaoPO> shixiaoPOList = //获取时效配置\n\n                //应派时间+最晚签收时间\n                if (CollectionUtils.isNotEmpty(shixiaoPOList)) {\n                    fillOutDispAndLatestSignTime(shixiaoPOList, preInboard,centerSendTime);\n                }else {\n                    //没有时效配置，要清空硬派日期和最晚签收日期\n                    preInboard.getDispInfo().setDispTime(null);\n                    if (Objects.nonNull(preInboard.getSignInfo())){\n                        preInboard.getSignInfo().setLatestSignTime(null);\n                    }\n                }\n            }\n        }\n    }\n```\n上面的代码复杂并且难以维护，在重构的过程中，分析出，这种思路（指的是domainService的入参必须是领域的聚合根(Inboard)）应该是错误的；\n\n然后去找理论支撑，在《中台架构与实现》P177看到对领域服务的定义：\n\n```java\n如果一个业务行为由多个实体对象参与完成，我们就将这部分业务逻辑放在领域服务中实现；\n领域服务与实体方法的主要区别是：实体方法完成单一实体自身的业务逻辑，是相对简单的原子业务逻辑；而领域服务则是由多个实体组合的相对复杂的业务逻辑。\n```\n简单的说，我们之前理解的业务逻辑写在领域服务中，是比较片面的，因为业务逻辑的实现，是可以在实体中，和领域服务中；\n\n* 业务逻辑\n   * 在实体中实现\n   * 在领域服务中实现\n\n总结下来：领域服务的入参不必限制为必须是当前领域的聚合根；可以是多种多样的。有一个限制就是：在实体方法和领域服务中，避免直接调用其他聚合的领域服务或者直接应用其他聚合的实体和值对象（不要增加耦合）；但是并不是说不让引用，而是通过正确的方式引用（通过唯一标识，引用其他聚合）。\n\n最后，我采用的比较靠谱的方案是：入参使用“唯一标识\"，这里的唯一标识加了引号，意思是说：不一定是唯一标识，但是一定是简单类型。\n\n```java\n//只有code和入网时间\nInboard inboard = InboardAppFactory.toInboard(msg);\n\n//当前扫描站点，分拨下一站，扫描时间，扫描类型（内部）\nlong scanSiteId = InboardAppFactory.toScanSiteId(msg);\nlong nextSiteId = InboardAppFactory.toNextSiteId(msg);\nDate scanTime = InboardAppFactory.toScanTime(msg);;\nInboardInnerScanTypeEnum inboardInnerScanTypeEnum = InboardAppFactory.toInnerScanType(msg.getScanDto().getScanTypeId());\n\n//创建或者更新进港单(入参使用简单类型)\nInboard afterInboard = inboardDomainService.processScanMsg(inboard,scanSiteId,nextSiteId,scanTime,inboardInnerScanTypeEnum);\n\n```","tags":["DDD","领域驱动"],"categories":["JAVA","DDD"]}]